"""API routes for apartments, documents, and file upload."""
import hashlib
import json
import os
import re
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional

from fastapi import APIRouter, Body, File, Form, UploadFile, HTTPException
from pydantic import BaseModel

from database import get_connection

router = APIRouter(prefix="/api", tags=["api"])

UPLOAD_DIR = Path(__file__).parent.parent / "uploads"
UPLOAD_DIR.mkdir(exist_ok=True)

# Import unified LLM client
from llm_client import call_llm, is_llm_available, get_llm_info, LLM_MODEL, set_llm_mode, get_llm_mode, set_llm_model, get_llm_model, ping_llm, call_vision_llm

# Import RAG index (FAISS + embedding)
import numpy as np
from rag_index import get_embedding, build_index as rag_build_index, search as faiss_search, invalidate_index

@router.get("/debug/llm")
def debug_llm():
    """Debug endpoint to check LLM config."""
    import os
    return {
        "is_llm_available": is_llm_available(),
        "get_llm_info": get_llm_info(),
        "env_LLM_ENABLED": os.getenv("LLM_ENABLED"),
        "env_LLM_PROVIDER": os.getenv("LLM_PROVIDER"),
        "env_LLM_API_KEY_set": bool(os.getenv("LLM_API_KEY"))
    }


@router.get("/llm/mode")
def get_mode():
    """Get current LLM mode (local/remote)."""
    return {"mode": get_llm_mode(), "info": get_llm_info()}


def _fetch_model_list() -> list[dict]:
    """Fetch available models from current Ollama endpoint."""
    import requests as _req
    info = get_llm_info()
    base_url = (info.get("base_url") or "http://127.0.0.1:11434").rstrip("/")
    headers = {}
    api_key = os.getenv("LLM_API_KEY", "")
    if info.get("mode") == "remote" and api_key:
        headers["Authorization"] = f"Bearer {api_key}"
    try:
        resp = _req.get(f"{base_url}/api/tags", headers=headers, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        models = []
        for m in data.get("models", []):
            name = m.get("name", "")
            size_gb = round(m.get("size", 0) / 1e9, 1) if m.get("size") else None
            models.append({"name": name, "size_gb": size_gb})
        models.sort(key=lambda x: x["name"])
        return models
    except Exception:
        return []


@router.post("/llm/mode")
def switch_mode(mode: str = Form(...)):
    """Switch LLM mode at runtime. Returns model list for the new mode."""
    try:
        set_llm_mode(mode)
        models = _fetch_model_list()
        return {"success": True, "mode": get_llm_mode(), "info": get_llm_info(), "models": models}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.get("/llm/model")
def get_model():
    """Get current LLM model."""
    return {"model": get_llm_model(), "info": get_llm_info()}


@router.post("/llm/model")
def switch_model(model: str = Form(...)):
    """Switch LLM model at runtime."""
    set_llm_model(model)
    return {"success": True, "model": get_llm_model(), "info": get_llm_info()}


@router.get("/llm/models")
def list_models():
    """List available models from current Ollama endpoint (local or remote)."""
    info = get_llm_info()
    models = _fetch_model_list()
    return {"models": models, "mode": info.get("mode"), "current": get_llm_model()}


class ApartmentCreate(BaseModel):
    apt_id: str
    name: str


class ApartmentResponse(BaseModel):
    apt_id: str
    name: str
    created_at: str


# ============ APARTMENTS ============

@router.post("/apartments")
def create_apartment(apt_id: str = Form(...), name: str = Form(...)):
    """Create a new apartment (accepts form data)."""
    conn = get_connection()
    cursor = conn.cursor()
    try:
        cursor.execute(
            "INSERT INTO apartments (apt_id, name) VALUES (%s, %s)",
            (apt_id, name)
        )
        conn.commit()
        return {"success": True, "apt_id": apt_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
    finally:
        conn.close()


@router.get("/apartments")
def list_apartments():
    """List all apartments."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT apt_id, name, created_at FROM apartments ORDER BY created_at DESC")
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]


@router.delete("/apartments/{apt_id}")
def delete_apartment(apt_id: str):
    """Delete apartment and all related data (cascading)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT apt_id FROM apartments WHERE apt_id = %s", (apt_id,))
    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Apartment not found")

    cursor.execute("SELECT doc_id FROM documents WHERE apt_id = %s", (apt_id,))
    doc_ids = [r["doc_id"] for r in cursor.fetchall()]
    cursor.execute("SELECT conversation_id FROM conversations WHERE apt_id = %s", (apt_id,))
    conv_ids = [r["conversation_id"] for r in cursor.fetchall()]
    deleted = {}
    if doc_ids:
        ph = ",".join(["%s"] * len(doc_ids))
        for tbl in ["manual_section_revisions", "manual_sections", "qa_issues", "chunks", "api_specs", "doc_chunks"]:
            cursor.execute(f"DELETE FROM {tbl} WHERE doc_id IN ({ph})", doc_ids)
            deleted[tbl] = cursor.rowcount
    if conv_ids:
        ph2 = ",".join(["%s"] * len(conv_ids))
        cursor.execute(f"DELETE FROM messages WHERE conversation_id IN ({ph2})", conv_ids)
        deleted["messages"] = cursor.rowcount
    for tbl in ["conversations", "improve_suggestions", "branch_class_cache", "documents"]:
        cursor.execute(f"DELETE FROM {tbl} WHERE apt_id = %s", (apt_id,))
        deleted[tbl] = cursor.rowcount
    cursor.execute("DELETE FROM apartments WHERE apt_id = %s", (apt_id,))
    deleted["apartments"] = cursor.rowcount
    conn.commit()
    conn.close()
    return {"success": True, "apt_id": apt_id, "deleted": deleted}


@router.delete("/doc/{doc_id}")
def delete_document(doc_id: str):
    """Delete a document and all related data."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT doc_id FROM documents WHERE doc_id = %s", (doc_id,))
    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    deleted = {}
    for tbl in ["manual_section_revisions", "manual_sections", "qa_issues", "chunks", "api_specs", "doc_chunks"]:
        cursor.execute(f"DELETE FROM {tbl} WHERE doc_id = %s", (doc_id,))
        deleted[tbl] = cursor.rowcount
    cursor.execute("DELETE FROM documents WHERE doc_id = %s", (doc_id,))
    deleted["documents"] = cursor.rowcount
    conn.commit()
    conn.close()
    return {"success": True, "doc_id": doc_id, "deleted": deleted}


# ============ UPLOAD ============

@router.post("/upload")
async def upload_document(
    apt_id: str = Form(...),
    file: UploadFile = File(...)
):
    """Upload a document file."""
    content = await file.read()
    content_hash = hashlib.sha256(content).hexdigest()
    
    filename = file.filename or "unknown"
    ext = filename.rsplit(".", 1)[-1].lower() if "." in filename else "txt"
    source_type = ext if ext in ("docx", "pdf", "txt", "md") else "txt"
    
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute(
        "SELECT doc_id, version FROM documents WHERE apt_id = %s AND content_hash = %s AND status != 'ARCHIVED' ORDER BY version DESC LIMIT 1",
        (apt_id, content_hash)
    )
    existing = cursor.fetchone()
    
    if existing:
        new_version = existing["version"] + 1
        cursor.execute(
            "UPDATE documents SET status = 'ARCHIVED', updated_at = %s WHERE apt_id = %s AND content_hash = %s AND status != 'ARCHIVED'",
            (datetime.now().isoformat(), apt_id, content_hash)
        )
    else:
        cursor.execute("SELECT MAX(version) as max_ver FROM documents WHERE apt_id = %s", (apt_id,))
        row = cursor.fetchone()
        new_version = (row["max_ver"] or 0) + 1
    
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    file_path = UPLOAD_DIR / f"{doc_id}.{source_type}"
    with open(file_path, "wb") as f:
        f.write(content)
    
    raw_text = f"[Placeholder: extract text first]"
    
    now = datetime.now().isoformat()
    cursor.execute("""
        INSERT INTO documents (doc_id, apt_id, title, source_filename, source_type, content_hash, raw_text, version, status, created_at, updated_at)
        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, 'DRAFT', %s, %s)
    """, (doc_id, apt_id, filename, filename, source_type, content_hash, raw_text, new_version, now, now))
    
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id, "version": new_version, "content_hash": content_hash, "status": "DRAFT"}


@router.get("/docs")
def list_documents(apt_id: Optional[str] = None):
    """List documents, optionally filtered by apartment."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute(
            "SELECT doc_id, apt_id, title, source_filename, source_type, version, status, created_at FROM documents WHERE apt_id = %s ORDER BY created_at DESC",
            (apt_id,)
        )
    else:
        cursor.execute(
            "SELECT doc_id, apt_id, title, source_filename, source_type, version, status, created_at FROM documents ORDER BY created_at DESC"
        )
    
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]


# ============ STEP 2: EXTRACT TEXT ============

VISION_PROMPT = "ì´ ì´ë¯¸ì§€ë¥¼ í•œêµ­ì–´ë¡œ ìì„¸íˆ ì„¤ëª…í•´ì¤˜. UI í™”ë©´ì´ë¼ë©´ ì–´ë–¤ ê¸°ëŠ¥ì˜ í™”ë©´ì¸ì§€, ë²„íŠ¼/ë©”ë‰´/ì…ë ¥ í•„ë“œ ë“± êµ¬ì„± ìš”ì†Œë¥¼ í¬í•¨í•´ì„œ ì„¤ëª…í•´."
VISION_OCR_PROMPT = "ì´ ì´ë¯¸ì§€ì— ìˆëŠ” í…ìŠ¤íŠ¸ë¥¼ í•œêµ­ì–´ë¡œ ì •í™•íˆ ì½ì–´ì„œ ê·¸ëŒ€ë¡œ ì¶œë ¥í•´ì¤˜. ëª©ì°¨ë¼ë©´ í•­ëª©ê³¼ í˜ì´ì§€ ë²ˆí˜¸ë¥¼ ìœ ì§€í•´ì¤˜. í‘œë¼ë©´ í‘œ êµ¬ì¡°ë¥¼ ìœ ì§€í•´ì¤˜. ê°€ëŠ¥í•œ ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ë³´ì¡´í•´ì„œ ì¶œë ¥í•´."

# Extract progress tracker (in-memory)
_extract_progress = {}  # {doc_id: {"page": 1, "total_pages": 10, "images_done": 2, "images_total": 5, "status": "..."}}


def _is_extract_cancelled(doc_id: str) -> bool:
    return _extract_progress.get(doc_id, {}).get("cancelled", False)


def _describe_image(image_base64: str, doc_id: str = None) -> str:
    """Vision LLMìœ¼ë¡œ ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±. ì‹¤íŒ¨ ì‹œ [ì´ë¯¸ì§€ ì„¤ëª… ì‹¤íŒ¨] ë°˜í™˜."""
    if doc_id and _is_extract_cancelled(doc_id):
        return "[ì´ë¯¸ì§€ ì„¤ëª… ê±´ë„ˆëœ€: ì·¨ì†Œë¨]"
    if doc_id and doc_id in _extract_progress:
        _extract_progress[doc_id]["images_done"] = _extract_progress[doc_id].get("images_done", 0) + 1
        _extract_progress[doc_id]["status"] = f"ì´ë¯¸ì§€ {_extract_progress[doc_id]['images_done']}/{_extract_progress[doc_id].get('images_total', '?')} Vision LLM ë¶„ì„ ì¤‘"
    try:
        desc = call_vision_llm(VISION_PROMPT, image_base64)
        if desc and desc.strip():
            return f"[ì´ë¯¸ì§€ ì„¤ëª…: {desc.strip()}]"
    except Exception as e:
        print(f"[EXTRACT] Vision LLM failed: {e}")
    return "[ì´ë¯¸ì§€ ì„¤ëª… ì‹¤íŒ¨]"


@router.get("/doc/{doc_id}/extract-progress")
def get_extract_progress(doc_id: str):
    """Poll extract progress."""
    return _extract_progress.get(doc_id, {})


@router.post("/doc/{doc_id}/extract-cancel")
def cancel_extract(doc_id: str):
    """Cancel an in-progress extract operation."""
    if doc_id in _extract_progress:
        _extract_progress[doc_id]["cancelled"] = True
        return {"success": True, "detail": "ì·¨ì†Œ ìš”ì²­ë¨"}
    return {"success": False, "detail": "ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì—†ìŒ"}


@router.post("/doc/{doc_id}/extract-text")
def extract_text(doc_id: str, resume_page: int = 0):
    """Extract text from uploaded document (DOCX, PDF, TXT, MD).
    resume_page: 0=ì²˜ìŒë¶€í„°, N=Ní˜ì´ì§€ë¶€í„° ì´ì–´ì„œ (PDF only, ê¸°ì¡´ raw_textì— append)
    """
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT source_filename, source_type, raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    source_type = doc["source_type"]
    file_path = UPLOAD_DIR / f"{doc_id}.{source_type}"

    if not file_path.exists():
        conn.close()
        raise HTTPException(status_code=404, detail="File not found on disk")

    # ì´ì–´í•˜ê¸°: ê¸°ì¡´ í…ìŠ¤íŠ¸ ë³´ì¡´
    existing_text = (doc["raw_text"] or "") if resume_page > 0 else ""
    raw_text = ""
    image_count = 0

    if source_type == "docx":
        try:
            from docx import Document
            import zipfile
            import base64

            _extract_progress[doc_id] = {"page": 0, "total_pages": 0, "images_done": 0, "images_total": 0, "status": "DOCX í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘", "pages": {}}

            docx_doc = Document(str(file_path))
            parts = []

            # ë‹¨ë½
            para_texts = []
            for p in docx_doc.paragraphs:
                if p.text.strip():
                    para_texts.append(p.text)
            if para_texts:
                para_content = "\n".join(para_texts)
                parts.append(para_content)
                _extract_progress[doc_id]["pages"]["ë³¸ë¬¸"] = para_content

            # í…Œì´ë¸”
            _extract_progress[doc_id]["status"] = "í…Œì´ë¸” ì¶”ì¶œ ì¤‘"
            for t_idx, table in enumerate(docx_doc.tables):
                rows_text = []
                for row in table.rows:
                    cells = [cell.text.strip() for cell in row.cells]
                    rows_text.append(" | ".join(cells))
                if rows_text:
                    table_content = "[í‘œ]\n" + "\n".join(rows_text)
                    parts.append(table_content)
                    _extract_progress[doc_id]["pages"][f"í‘œ {t_idx + 1}"] = table_content

            # ì´ë¯¸ì§€ ìˆ˜ ë¯¸ë¦¬ ì¹´ìš´íŠ¸
            with zipfile.ZipFile(str(file_path), 'r') as zf:
                media_files = [n for n in zf.namelist() if n.startswith("word/media/")]
                _extract_progress[doc_id]["images_total"] = len(media_files)
                _extract_progress[doc_id]["status"] = f"ì´ë¯¸ì§€ 0/{len(media_files)} Vision LLM ë¶„ì„ ì¤‘" if media_files else "ì™„ë£Œ ì¤‘"

                for name in media_files:
                    img_bytes = zf.read(name)
                    img_b64 = base64.b64encode(img_bytes).decode()
                    image_count += 1
                    print(f"[EXTRACT] DOCX image {image_count}: {name} ({len(img_bytes)} bytes)")
                    desc = _describe_image(img_b64, doc_id)
                    parts.append(desc)
                    _extract_progress[doc_id]["pages"][f"ì´ë¯¸ì§€ {image_count}"] = desc

            raw_text = "\n".join(parts)
            if _is_extract_cancelled(doc_id):
                _extract_progress[doc_id].pop("pages", None)
                _extract_progress[doc_id]["status"] = "ì·¨ì†Œë¨"
            else:
                _extract_progress.pop(doc_id, None)
        except Exception as e:
            _extract_progress.pop(doc_id, None)
            conn.close()
            raise HTTPException(status_code=500, detail=f"DOCX extraction failed: {str(e)}")

    elif source_type == "pdf":
        try:
            import fitz
            import base64
            fitz.TOOLS.mupdf_display_errors(False)

            pdf_doc = fitz.open(str(file_path))
            total_pages = len(pdf_doc)

            # ì´ë¯¸ì§€ í¬í•¨ í˜ì´ì§€ ìˆ˜ ë¯¸ë¦¬ ì¹´ìš´íŠ¸
            pages_with_images = []
            for pg_idx, pg in enumerate(pdf_doc):
                if pg.get_images(full=True):
                    pages_with_images.append(pg_idx)

            # resume_page: 0-indexed internally (resume_page=3 â†’ skip pages 0,1,2)
            start_page = max(0, resume_page - 1) if resume_page > 0 else 0
            remaining_image_pages = [p for p in pages_with_images if p >= start_page]
            _extract_progress[doc_id] = {"page": start_page, "total_pages": total_pages, "images_done": 0, "images_total": len(remaining_image_pages), "status": f"{start_page}/{total_pages} í˜ì´ì§€", "pages": {}}

            if start_page > 0:
                print(f"[EXTRACT] PDF resume from page {start_page + 1}/{total_pages} for {doc_id}")

            def _is_garbled(text: str) -> bool:
                """í…ìŠ¤íŠ¸ê°€ ê¹¨ì¡ŒëŠ”ì§€ íŒë³„. í•œê¸€ì´ ê¸°ëŒ€ë˜ëŠ” ë¬¸ì„œì—ì„œ í•œê¸€ ë¹„ìœ¨ì´ ê·¹íˆ ë‚®ìœ¼ë©´ ê¹¨ì§„ ê²ƒ."""
                import re
                if not text or len(text) < 10:
                    return False
                # ê³µë°±/ì¤„ë°”ê¿ˆ ì œê±° í›„ ì‹¤ì§ˆ ë¬¸ìë§Œ íŒë³„
                content = re.sub(r'\s+', '', text)
                if not content:
                    return False
                korean = len(re.findall(r'[ê°€-í£]', content))
                alpha_num = len(re.findall(r'[a-zA-Z0-9]', content))
                readable = korean + alpha_num
                ratio = readable / len(content)
                # í•œê¸€+ì˜ìˆ«ìê°€ 40% ë¯¸ë§Œì´ë©´ ê¹¨ì§„ í…ìŠ¤íŠ¸
                return ratio < 0.4

            # Phase 1: í…ìŠ¤íŠ¸ ì¶”ì¶œ + ë Œë”ë§ ëŒ€ìƒ ê²°ì • (ìˆœì°¨, ë¹ ë¦„)
            page_texts = {}       # {page_num: text}
            image_renders = {}    # {page_num: img_b64}
            garbled_pages = set()
            for page_num, page in enumerate(pdf_doc):
                if page_num < start_page:
                    continue
                _extract_progress[doc_id]["page"] = page_num + 1
                _extract_progress[doc_id]["status"] = f"{page_num + 1}/{total_pages} í˜ì´ì§€ í…ìŠ¤íŠ¸ ì¶”ì¶œ"

                page_text = page.get_text().strip()
                if page_text and not _is_garbled(page_text):
                    page_texts[page_num] = page_text
                    # í…ìŠ¤íŠ¸ ì •ìƒ + ì´ë¯¸ì§€ ì—†ëŠ” í˜ì´ì§€ëŠ” ë°”ë¡œ ìŠ¤íŠ¸ë¦¬ë°
                    if page_num not in pages_with_images:
                        _extract_progress[doc_id]["pages"][str(page_num + 1)] = page_text
                elif page_text:
                    # í…ìŠ¤íŠ¸ ê¹¨ì§ â†’ Vision LLMìœ¼ë¡œ ëŒ€ì²´
                    garbled_pages.add(page_num)
                    print(f"[EXTRACT] PDF page {page_num + 1}: garbled text detected, will use Vision LLM")

                # ì´ë¯¸ì§€ í¬í•¨ í˜ì´ì§€ ë˜ëŠ” í…ìŠ¤íŠ¸ ê¹¨ì§„ í˜ì´ì§€ â†’ ë Œë”ë§
                if page_num in remaining_image_pages or page_num in garbled_pages:
                    pix = page.get_pixmap(dpi=150)
                    image_renders[page_num] = base64.b64encode(pix.tobytes("png")).decode()
                    print(f"[EXTRACT] PDF page {page_num + 1} render: {pix.width}x{pix.height}")

            _extract_progress[doc_id]["images_total"] = len(image_renders)

            pdf_doc.close()

            # Phase 2: Vision LLM ìˆœì°¨ í˜¸ì¶œ (Ollama ëŒ€í˜• ëª¨ë¸ì€ ë™ì‹œì²˜ë¦¬ ë¶ˆê°€)
            image_descs = {}  # {page_num: desc}
            vision_total = len(image_renders)

            if image_renders:
                print(f"[EXTRACT] Vision LLM sequential: {vision_total} pages for {doc_id}")
                for vi, (pg_num, img_b64) in enumerate(sorted(image_renders.items())):
                    if _is_extract_cancelled(doc_id):
                        print(f"[EXTRACT] Cancelled at vision {vi + 1}/{vision_total}")
                        break
                    # í…ìŠ¤íŠ¸ ê¹¨ì§„ í˜ì´ì§€ëŠ” OCR í”„ë¡¬í”„íŠ¸, ì´ë¯¸ì§€ í¬í•¨ í˜ì´ì§€ëŠ” ì„¤ëª… í”„ë¡¬í”„íŠ¸
                    prompt = VISION_OCR_PROMPT if pg_num in garbled_pages else VISION_PROMPT
                    label = "OCR" if pg_num in garbled_pages else "ì´ë¯¸ì§€ ì„¤ëª…"
                    _extract_progress[doc_id]["status"] = f"í˜ì´ì§€ {pg_num + 1} {label} ì¤‘ ({vi + 1}/{vision_total})"
                    _extract_progress[doc_id]["images_done"] = vi

                    try:
                        desc = call_vision_llm(prompt, img_b64)
                        if desc and desc.strip():
                            desc = desc.strip() if pg_num in garbled_pages else f"[ì´ë¯¸ì§€ ì„¤ëª…: {desc.strip()}]"
                        else:
                            desc = f"[{label} ì‹¤íŒ¨]"
                    except Exception as e:
                        print(f"[EXTRACT] Vision LLM failed page {pg_num + 1}: {e}")
                        desc = f"[{label} ì‹¤íŒ¨]"

                    image_descs[pg_num] = desc
                    image_count += 1
                    _extract_progress[doc_id]["images_done"] = image_count
                    # í•´ë‹¹ í˜ì´ì§€ í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ í•©ì³ì„œ ìŠ¤íŠ¸ë¦¬ë°
                    combined = page_texts.get(pg_num, "")
                    combined = (combined + "\n" + desc).strip() if combined else desc
                    _extract_progress[doc_id]["pages"][str(pg_num + 1)] = combined

            # Phase 3: í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì¡°í•©
            parts = []
            all_pages = sorted(set(list(page_texts.keys()) + list(image_descs.keys())))
            for pg_num in all_pages:
                page_parts = []
                if pg_num in page_texts:
                    page_parts.append(page_texts[pg_num])
                if pg_num in image_descs:
                    page_parts.append(image_descs[pg_num])
                if page_parts:
                    parts.append("\n".join(page_parts))
            raw_text = "\n".join(parts)
            if _is_extract_cancelled(doc_id):
                # ì·¨ì†Œ ì‹œ: progress ìœ ì§€ (ì´ì–´í•˜ê¸°ìš©), pagesë§Œ ì œê±° (ë©”ëª¨ë¦¬ ì ˆì•½)
                _extract_progress[doc_id].pop("pages", None)
                _extract_progress[doc_id]["status"] = f"ì·¨ì†Œë¨ ({_extract_progress[doc_id].get('page', 0)}/{total_pages} í˜ì´ì§€)"
            else:
                _extract_progress.pop(doc_id, None)
        except Exception as e:
            # ì—ëŸ¬ ì‹œì—ë„ progress ìœ ì§€ (ì´ì–´í•˜ê¸°ìš©)
            if doc_id in _extract_progress:
                _extract_progress[doc_id].pop("pages", None)
                _extract_progress[doc_id]["status"] = f"ì˜¤ë¥˜ ({_extract_progress[doc_id].get('page', 0)}/{_extract_progress[doc_id].get('total_pages', '?')} í˜ì´ì§€)"
            conn.close()
            raise HTTPException(status_code=500, detail=f"PDF extraction failed: {str(e)}")

    elif source_type in ("txt", "md"):
        raw_text = file_path.read_text(encoding="utf-8", errors="ignore")

    else:
        raw_text = f"[ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {source_type}]"

    # ì´ì–´í•˜ê¸°: ê¸°ì¡´ í…ìŠ¤íŠ¸ + ìƒˆ í…ìŠ¤íŠ¸ í•©ì¹¨
    if existing_text and raw_text:
        raw_text = existing_text + "\n" + raw_text
    elif existing_text:
        raw_text = existing_text

    cursor.execute("UPDATE documents SET raw_text = %s, updated_at = %s WHERE doc_id = %s",
                   (raw_text, datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()

    return {
        "success": True,
        "doc_id": doc_id,
        "chars": len(raw_text),
        "resumed_from": resume_page if resume_page > 0 else None,
        "image_count": image_count,
        "preview": raw_text[:300] if raw_text else ""
    }


MANUALIZE_HARD_LIMIT = 30000  # chars â€” force window split above this (128K ctx ê¸°ì¤€)
MANUALIZE_WINDOW_SIZE = 25000  # chars per window (~37K tokens, 128K ctxì˜ ~75%)
MANUALIZE_WINDOW_OVERLAP = 300

# Manualize progress tracker (in-memory, per doc_id)
_manualize_progress = {}  # {doc_id: {"done": 3, "total": 15}}


@router.get("/doc/{doc_id}/char-count")
def get_char_count(doc_id: str):
    """Get raw_text character count and window split info (lightweight pre-check)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    row = cursor.fetchone()
    conn.close()
    if not row:
        raise HTTPException(status_code=404, detail="Document not found")
    raw_text = row["raw_text"] or ""
    chars = len(raw_text)
    mode = get_llm_mode()
    window_count = len(_split_by_headings(raw_text))
    return {"chars": chars, "window_count": window_count, "hard_limit": MANUALIZE_HARD_LIMIT, "mode": mode}


@router.get("/doc/{doc_id}/manualize-progress")
def get_manualize_progress(doc_id: str):
    """Poll manualize progress. Returns {done, total, cancelled, sections}."""
    prog = _manualize_progress.get(doc_id, {"done": 0, "total": 0})
    return {
        "done": prog.get("done", 0),
        "total": prog.get("total", 0),
        "cancelled": prog.get("cancelled", False),
        "sections": prog.get("sections", {}),
    }


@router.post("/doc/{doc_id}/manualize-cancel")
def cancel_manualize(doc_id: str):
    """Cancel an in-progress manualize operation."""
    if doc_id in _manualize_progress:
        _manualize_progress[doc_id]["cancelled"] = True
        return {"success": True, "detail": "ì·¨ì†Œ ìš”ì²­ë¨"}
    return {"success": False, "detail": "ì§„í–‰ ì¤‘ì¸ ì‘ì—… ì—†ìŒ"}


# ============ STEP 2: MANUALIZE ============

MANUALIZE_PROMPT = """ë‹¹ì‹ ì€ ì˜ì—…/ìš´ì˜ ë¬¸ì„œë¥¼ RAG(ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€)ì— ë„£ê¸° ìœ„í•œ
"êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œê¸°"ì…ë‹ˆë‹¤.

âš ï¸ ì´ ì‘ì—…ì€ ìš”ì•½ì´ ì•„ë‹™ë‹ˆë‹¤.
âš ï¸ ë¬¸ì„œë¥¼ ì¤„ì´ê±°ë‚˜ ì••ì¶•í•˜ëŠ” ì‘ì—…ì´ ì•„ë‹™ë‹ˆë‹¤.

[ìµœìš°ì„  ëª©í‘œ]
- ì›ë¬¸(raw_text)ì˜ ì •ë³´ì™€ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ì—¬ JSONìœ¼ë¡œ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
- ì›ë¬¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€/ì¶”ì¸¡/ì°½ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì›ë¬¸ì— ìˆëŠ” ì •ë³´ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ë¬¸ì„œë¥¼ ê°„ëµí™”í•˜ê±°ë‚˜ ì••ì¶•í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ê°€ëŠ¥í•œ í•œ ì›ë¬¸ ì •ë³´ëŸ‰ì„ ìœ ì§€í•©ë‹ˆë‹¤.
- RAG ê²€ìƒ‰ì— ì˜ ê±¸ë¦¬ë„ë¡ êµ¬ì¡°ë§Œ ì •ë¦¬í•©ë‹ˆë‹¤.

ì¦‰:
ìš”ì•½ âŒ
ì¬ì‘ì„± âŒ
êµ¬ì¡°í™”ëœ ì¶”ì¶œ âœ…

[ì…ë ¥]
raw_text: {raw_text}

[ì¶œë ¥ í˜•ì‹]
- RFC8259 ìœ íš¨ JSONë§Œ ì¶œë ¥
- ì½”ë“œë¸”ë¡, ì„¤ëª…, ì£¼ì„, ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€
- ì˜¤ì§ JSON í…ìŠ¤íŠ¸ë§Œ
- ëª¨ë“  ë¬¸ìì—´ì€ í°ë”°ì˜´í‘œ ì‚¬ìš©
- trailing comma ê¸ˆì§€
- ëª¨ë“  í•„ë“œëŠ” ë°˜ë“œì‹œ í¬í•¨ (ì—†ìœ¼ë©´ "" ë˜ëŠ” [])

{{
  "doc_title": "",
  "doc_type": "POLICY|PROCESS|FAQ|NOTICE|MIXED",
  "summary": "",
  "sections": [
    {{
      "section_id": "",
      "name": "",
      "tags": [],
      "content": [
        {{
          "rule_id": "",
          "title": "",
          "bullets": [],
          "structured": {{
            "target": "",
            "condition": "",
            "procedure": [],
            "exceptions": [],
            "owner": "",
            "channel": ""
          }},
          "source_quotes": [],
          "issues": []
        }}
      ]
    }}
  ],
  "clarification_questions": [],
  "pii_handling": {{
    "pii_found": false,
    "pii_types": [],
    "masking_policy": []
  }},
  "change_summary": ""
}}

[ğŸš¨ ì ˆëŒ€ ê·œì¹™ (ìœ„ë°˜ ê¸ˆì§€)]

1) ì •ë³´ ì‚­ì œ ê¸ˆì§€
- ì›ë¬¸ì— ìˆëŠ” í•­ëª©/ë¦¬ìŠ¤íŠ¸/ë¬¸ì¥ì„ ì„ì˜ë¡œ ì œê±°í•˜ì§€ ë§ˆì„¸ìš”.

2) ì •ë³´ ì••ì¶• ê¸ˆì§€
- ì—¬ëŸ¬ í•­ëª©ì„ í•˜ë‚˜ë¡œ í•©ì³ ì¤„ì´ì§€ ë§ˆì„¸ìš”.

3) ìš”ì•½ ê¸ˆì§€
- ì˜ë¯¸ë§Œ ë‚¨ê¸°ê³  ì¶•ì•½í•˜ì§€ ë§ˆì„¸ìš”.

4) ì¼ë°˜í™” ê¸ˆì§€
- "ë“±", "ê¸°íƒ€", "í¬í•¨"ìœ¼ë¡œ ë­‰ê°œì§€ ë§ˆì„¸ìš”.

5) ì¬êµ¬ì„± ìµœì†Œí™”
- êµ¬ì¡° ì •ë¦¬ ì™¸ ì˜ë¯¸ ë³€ê²½ ê¸ˆì§€.

[ì„¹ì…˜ ê·œì¹™ - ìµœìš°ì„ ]

- sectionsëŠ” ì›ë¬¸ í—¤ë”© êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë”°ë¦…ë‹ˆë‹¤.
- ì„¹ì…˜ ìˆ˜ë¥¼ ì„ì˜ë¡œ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì´ì§€ ë§ˆì„¸ìš”.
- ë³‘í•©/ë¶„ë¦¬ ê¸ˆì§€.
- ì›ë¬¸ í—¤ë”©ì´ ì—†ìœ¼ë©´ sectionsëŠ” 1ê°œ("general").

section_id:
- ì„¹ì…˜ëª… slug ì‚¬ìš©
- ì—†ìœ¼ë©´ "general"

[rule ìƒì„± ê·œì¹™]

- ì›ë¬¸ ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ëŠ” ê°œìˆ˜ì™€ í•­ëª©ì„ ê·¸ëŒ€ë¡œ ë³´ì¡´.
- í•œ ë¶ˆë¦¿ = í•œ ì •ë³´ ë‹¨ìœ„.
- ì‚­ì œ/ë³‘í•© ê¸ˆì§€.
- "í˜„ì¬ ë²„ì „", "ì›¹ì‚¬ì´íŠ¸" ê°™ì€ ì •ë³´ë„ ê°ê° ë³„ë„ rule ê°€ëŠ¥.

rule_id:
S1-R1, S1-R2â€¦ ìˆœì„œ ê³ ì •

[bullets ì‘ì„± ê·œì¹™]

- ì›ë¬¸ ë¬¸ì¥ì„ ìµœëŒ€í•œ ìœ ì§€
- ê³¼ë„í•œ ì¬ì‘ì„± ê¸ˆì§€
- ì •ë³´ ì¶”ê°€ ê¸ˆì§€

[source_quotes]

- ê° ruleë§ˆë‹¤ 0~2ê°œ
- ì›ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬
- í•œêµ­ì–´ 20~70ì
- ì—†ìœ¼ë©´ []

[structured]

- ì›ë¬¸ì— ëª…ì‹œëœ ê²ƒë§Œ ì±„ì›€
- ì—†ìœ¼ë©´ ì „ë¶€ ë¹ˆê°’

[issues]

- ë¬¸ì œ ì—†ìœ¼ë©´ []

[PII]

ìˆìœ¼ë©´:
- ë§ˆìŠ¤í‚¹
- issuesì— PII_RISK
- pii_found=true

ì—†ìœ¼ë©´:
- pii_found=false
- ë‚˜ë¨¸ì§€ []

[doc_type íŒë‹¨]

POLICY: ê·œì •/ê¸°ì¤€
PROCESS: ì ˆì°¨
FAQ: Q/A
NOTICE: ì•ˆë‚´/ì†Œê°œ
MIXED: í˜¼í•©

[ë§ˆì§€ë§‰ ì²´í¬ (ìŠ¤ìŠ¤ë¡œ ê²€ì¦)]

JSON ì¶œë ¥ ì „ ë°˜ë“œì‹œ í™•ì¸:
- ì›ë¬¸ ì •ë³´ê°€ ë¹ ì§€ì§€ ì•Šì•˜ëŠ”ê°€?
- ë¶ˆë¦¿ ê°œìˆ˜ê°€ ì¤„ì§€ ì•Šì•˜ëŠ”ê°€?
- ìš”ì•½í•˜ì§€ ì•Šì•˜ëŠ”ê°€?

í•˜ë‚˜ë¼ë„ ìœ„ë°˜ì´ë©´ ë‹¤ì‹œ ìƒì„±í•˜ì„¸ìš”.

[ê¸ˆì§€]
- ì¶”ì¸¡
- ì¼ë°˜ ìƒì‹ ë³´ì™„
- ìƒˆ ì •ë³´ ì¶”ê°€
- JSON ì™¸ ì¶œë ¥"""


@router.post("/doc/{doc_id}/manualize")
def manualize(doc_id: str, force: bool = False):
    """Convert raw text to structured manual sections.
    Default: single LLM call with full raw_text + MANUALIZE_PROMPT.
    Exception: if len(raw_text) > HARD_LIMIT, uses window-based fallback internally."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    raw_text = doc["raw_text"]
    if not raw_text or raw_text.strip() == "" or raw_text.startswith("["):
        conn.close()
        error_msg = "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € 'Extract'ë¥¼ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”."
        if raw_text and raw_text.startswith("["):
            error_msg = f"í…ìŠ¤íŠ¸ ì¶”ì¶œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤: {raw_text}"
        raise HTTPException(status_code=400, detail=error_msg)

    # Check if manual sections already exist (return cached if not forced)
    if not force:
        cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
        existing = cursor.fetchall()
        if existing:
            sections = {row["section_name"]: row["section_text"] for row in existing}
            conn.close()
            return {
                "success": True,
                "doc_id": doc_id,
                "sections": list(sections.keys()),
                "section_details": sections,
                "llm_used": False,
                "cached": True,
                "todo_questions": []
            }

    # LLM is required
    if not is_llm_available():
        conn.close()
        raise HTTPException(status_code=503, detail="LLMì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

    # Quick connectivity check before starting long operation
    llm_ok, llm_detail = ping_llm(timeout=5.0)
    if not llm_ok:
        conn.close()
        raise HTTPException(status_code=503, detail=llm_detail)

    raw_text_chars = len(raw_text)
    mode = get_llm_mode()
    window_count = len(_split_by_headings(raw_text))

    # ì§„í–‰ë¥  ì´ˆê¸°í™” (sections: ì™„ë£Œëœ ì„¹ì…˜ì„ ì¦‰ì‹œ í”„ë¡ íŠ¸ì— ìŠ¤íŠ¸ë¦¬ë°)
    _manualize_progress[doc_id] = {"done": 0, "total": window_count, "sections": {}}

    try:
        if mode == "remote":
            # Remote: í•­ìƒ ì„¹ì…˜ ë¶„í•  â†’ ìˆœì°¨ ì²˜ë¦¬ (ë‚´ìš© ë³´ì¡´)
            print(f"[MANUALIZE] Remote sequential: {raw_text_chars} chars, {window_count} sections for {doc_id}")
            sections_map = _manualize_with_window(raw_text, doc_id)
        elif raw_text_chars > MANUALIZE_HARD_LIMIT:
            # Local + ëŒ€í˜• ë¬¸ì„œ: ìœˆë„ìš° ë³‘ë ¬ ì²˜ë¦¬
            print(f"[MANUALIZE] Local parallel: {raw_text_chars} chars > {MANUALIZE_HARD_LIMIT}")
            sections_map = _manualize_with_window(raw_text, doc_id)
        else:
            # Local + ì†Œí˜• ë¬¸ì„œ: ë‹¨ì¼ í˜¸ì¶œ
            print(f"[MANUALIZE] Local single: {raw_text_chars} chars for {doc_id}")
            sections_map = _manualize_single(raw_text, doc_id)
            _manualize_progress[doc_id] = {"done": 1, "total": 1, "sections": dict(sections_map)}

    except HTTPException:
        raise
    except Exception as e:
        conn.close()
        raise HTTPException(status_code=502, detail=f"Manualize ì‹¤íŒ¨: {str(e)}")

    # Check if cancelled
    if _manualize_progress.get(doc_id, {}).get("cancelled"):
        _manualize_progress.pop(doc_id, None)
        conn.close()
        raise HTTPException(status_code=499, detail="ì‚¬ìš©ìê°€ Manualizeë¥¼ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")

    # Save sections (gate_status stays NULL â€” user triggers Gate manually)
    cursor.execute("DELETE FROM manual_sections WHERE doc_id = %s", (doc_id,))
    cursor.execute("UPDATE documents SET completed_phases = NULL WHERE doc_id = %s", (doc_id,))
    sec_counter = 0
    for section_name, section_text in sections_map.items():
        sec_counter += 1
        if not section_name or not section_name.strip():
            section_name = f"## ê¸°íƒ€ ({sec_counter})"
        section_id = f"sec_{uuid.uuid4().hex[:8]}"
        text_val = section_text if section_text else "ì •ë³´ ì—†ìŒ"
        cursor.execute(
            """INSERT INTO manual_sections
               (section_id, doc_id, section_name, section_text, ai_text,
                gate_status, gate_score, gate_reasons_json, gate_stale)
               VALUES (%s, %s, %s, %s, %s, NULL, NULL, NULL, 0)""",
            (section_id, doc_id, section_name, text_val, text_val)
        )

    cursor.execute("UPDATE documents SET updated_at = %s WHERE doc_id = %s",
                   (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()

    return {
        "success": True,
        "doc_id": doc_id,
        "sections": list(sections_map.keys()),
        "section_details": sections_map,
        "todo_questions": [],
        "change_summary": "",
        "pii_handling": {},
        "gate_results": {},
        "llm_used": True,
        "raw_text_chars": raw_text_chars,
        "window_count": window_count,
    }


def _manualize_single(raw_text: str, doc_id: str) -> dict:
    """Single LLM call with full raw_text. Returns {section_name: section_text}."""
    content = call_llm(MANUALIZE_PROMPT.format(raw_text=raw_text), temperature=0.3)
    if not content:
        raise Exception("LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    json_match = re.search(r'\{[\s\S]*\}', content)
    if not json_match:
        raise Exception("LLM ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    parsed = _clean_llm_json(json_match.group())
    return _flatten_manualize_json(parsed)


def _split_by_headings(raw_text: str) -> list:
    """Split raw_text into sections by heading patterns.
    Returns list of (start_pos, section_text) tuples."""
    # Match common heading patterns: numbered (1. 2.1 ì œ3ì¡° etc.), markdown (#), or ALL-CAPS lines
    heading_pattern = re.compile(
        r'^(?:'
        r'#{1,4}\s+'                          # Markdown headings
        r'|ì œ?\s*\d+[ì¡°í•­ì¥ì ˆí¸]\s*'           # ë²•ë¥ /ê·œì • ìŠ¤íƒ€ì¼ (ì œ1ì¡°, ì œ2ì¥ ë“±)
        r'|\d+(?:\.\d+)*[\.\)]\s+'            # Numbered (1. 2.1. 3.2.1)
        r'|[ê°€-í£]{1,2}[\.\)]\s+'             # Korean bullets (ê°€. ë‚˜.)
        r'|[IVX]+[\.\)]\s+'                   # Roman numerals
        r'|[A-Z][A-Z\s]{4,}$'                # ALL-CAPS lines (5+ chars)
        r')',
        re.MULTILINE
    )

    # Find all heading positions
    positions = [m.start() for m in heading_pattern.finditer(raw_text)]

    if not positions:
        # No headings found â†’ return whole text as one section
        return [(0, raw_text)]

    # Ensure we start from 0
    if positions[0] != 0:
        positions.insert(0, 0)

    # Build sections
    sections = []
    for i, pos in enumerate(positions):
        end = positions[i + 1] if i + 1 < len(positions) else len(raw_text)
        sections.append((pos, raw_text[pos:end]))

    return sections


def _group_sections_into_windows(sections: list, window_size: int) -> list:
    """Group heading-based sections into windows that fit within window_size.
    Each window is a string. If a single section exceeds window_size, it is sent alone."""
    windows = []
    current_window = ""

    for _pos, section_text in sections:
        # If adding this section would exceed limit
        if current_window and len(current_window) + len(section_text) > window_size:
            windows.append(current_window)
            current_window = section_text
        else:
            current_window += section_text

    if current_window:
        windows.append(current_window)

    return windows


def _process_one_window(window_text: str, idx: int, total: int) -> dict:
    """Process a single window. Returns {section_name: section_text} or empty dict."""
    print(f"[MANUALIZE] Processing window {idx + 1}/{total} ({len(window_text)} chars)...")
    try:
        content = call_llm(MANUALIZE_PROMPT.format(raw_text=window_text), temperature=0.3)
        if not content:
            return {}
        json_match = re.search(r'\{[\s\S]*\}', content)
        if not json_match:
            return {}
        parsed = _clean_llm_json(json_match.group())
        return _flatten_manualize_json(parsed)
    except Exception as e:
        print(f"[MANUALIZE] Window {idx + 1} failed: {e}")
        return {}


def _merge_sections(all_sections: dict, new_sections: dict, idx: int) -> None:
    """Merge new_sections into all_sections. Dedup by exact match, index on conflict."""
    for name, text in new_sections.items():
        if name in all_sections:
            if all_sections[name].strip() == text.strip():
                continue
            name = f"{name} ({idx + 1})"
        all_sections[name] = text


def _manualize_with_window(raw_text: str, doc_id: str) -> dict:
    """Large document processing. Mode-aware:
    - Local: group into windows â†’ parallel (concurrent threads)
    - Remote: heading sections â†’ sequential (ì‘ë‹µ ë°›ê³  ë‹¤ìŒ ì „ì†¡)"""
    sections = _split_by_headings(raw_text)
    print(f"[MANUALIZE] Found {len(sections)} heading sections for {doc_id}")

    mode = get_llm_mode()

    if mode == "remote":
        # Remote: ì„¹ì…˜ë³„ ìˆœì°¨ ì²˜ë¦¬ (ì‘ì€ ì…ë ¥ â†’ ë‚´ìš© ë³´ì¡´ ìš°ìˆ˜)
        total = len(sections)
        print(f"[MANUALIZE] Remote sequential: {total} sections for {doc_id}")
        _manualize_progress[doc_id] = {"done": 0, "total": total, "sections": {}}
        all_sections = {}
        for i, (_pos, section_text) in enumerate(sections):
            # Check cancellation before each section
            if _manualize_progress.get(doc_id, {}).get("cancelled"):
                print(f"[MANUALIZE] Cancelled at section {i}/{total} for {doc_id}")
                break
            if not section_text.strip():
                _manualize_progress[doc_id]["done"] = i + 1
                continue
            result = _process_one_window(section_text, i, total)
            _merge_sections(all_sections, result, i)
            _manualize_progress[doc_id]["done"] = i + 1
            _manualize_progress[doc_id]["sections"] = dict(all_sections)
    else:
        # Local: ìœˆë„ìš° ë¬¶ì–´ì„œ ë³‘ë ¬ ì²˜ë¦¬ (ì§„í–‰ë¥ ì€ ì„¹ì…˜ ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ í‘œì‹œ)
        from concurrent.futures import ThreadPoolExecutor, as_completed
        windows = _group_sections_into_windows(sections, MANUALIZE_WINDOW_SIZE)
        total_windows = len(windows)
        total_sections = len(sections)
        print(f"[MANUALIZE] Local parallel: {total_windows} windows ({total_sections} sections) for {doc_id}")
        _manualize_progress[doc_id] = {"done": 0, "total": total_sections, "sections": {}}

        # Compute how many sections each window covers (mirrors _group_sections_into_windows logic)
        _sections_per_window = []
        _cur_len = 0
        _cur_cnt = 0
        for _pos, _st in sections:
            if _cur_len > 0 and _cur_len + len(_st) > MANUALIZE_WINDOW_SIZE:
                _sections_per_window.append(_cur_cnt)
                _cur_cnt = 1
                _cur_len = len(_st)
            else:
                _cur_cnt += 1
                _cur_len += len(_st)
        if _cur_cnt > 0:
            _sections_per_window.append(_cur_cnt)

        all_sections = {}
        if total_windows == 1:
            result = _process_one_window(windows[0], 0, 1)
            _merge_sections(all_sections, result, 0)
            _manualize_progress[doc_id]["done"] = total_sections
            _manualize_progress[doc_id]["sections"] = dict(all_sections)
        else:
            with ThreadPoolExecutor(max_workers=total_windows) as executor:
                futures = {
                    executor.submit(_process_one_window, w, i, total_windows): i
                    for i, w in enumerate(windows)
                }
                for future in as_completed(futures):
                    i = futures[future]
                    result = future.result()
                    _merge_sections(all_sections, result, i)
                    done_inc = _sections_per_window[i] if i < len(_sections_per_window) else 1
                    _manualize_progress[doc_id]["done"] = _manualize_progress[doc_id]["done"] + done_inc
                    _manualize_progress[doc_id]["sections"] = dict(all_sections)

    # ì™„ë£Œ í›„ ì •ë¦¬
    _manualize_progress.pop(doc_id, None)

    if not all_sections:
        raise Exception("ëª¨ë“  ì²˜ë¦¬ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    return all_sections


# ============ STEP 2a: SPLIT (progressive) ============

@router.post("/doc/{doc_id}/split")
def split_document(doc_id: str, force: bool = False):
    """Split document into chunks (rule-based). Returns chunk list for progressive manualize.
    If not force and chunks already exist, returns cached chunks."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    raw_text = doc["raw_text"]
    if not raw_text or raw_text.strip() == "" or raw_text.startswith("["):
        conn.close()
        raise HTTPException(status_code=400, detail="ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € Extractë¥¼ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”.")

    # Return cached chunks if not force
    if not force:
        cursor.execute("SELECT chunk_id, chunk_index, split_basis, notes, raw_chunk FROM doc_chunks WHERE doc_id = %s ORDER BY chunk_index", (doc_id,))
        existing = cursor.fetchall()
        if existing:
            chunks = []
            chunk_dicts = []
            for row in existing:
                preview = (row["raw_chunk"] or "")[:200]
                heading = row["notes"] or ""
                if not heading and row["raw_chunk"]:
                    first_line = row["raw_chunk"].split("\n", 1)[0].strip().lstrip("#").strip()
                    heading = first_line[:60] if first_line else f"Chunk {row['chunk_index'] + 1}"
                chunks.append({
                    "chunk_id": row["chunk_id"],
                    "chunk_index": row["chunk_index"],
                    "heading": heading,
                    "preview": preview,
                    "char_count": len(row["raw_chunk"] or ""),
                })
                chunk_dicts.append({
                    "chunk_id": row["chunk_id"],
                    "chunk_index": row["chunk_index"],
                    "notes": row["notes"] or "",
                    "raw_chunk": row["raw_chunk"] or "",
                })
            batches = _build_batches(chunk_dicts, doc_id)
            batch_info = [{"batch_id": b["batch_id"], "chunk_ids": b["chunk_ids"],
                           "oversize": [o["item_id"] for o in b.get("oversize_sections", [])]}
                          for b in batches]
            conn.close()
            return {"success": True, "doc_id": doc_id, "chunks": chunks, "batches": batch_info, "cached": True}

    # Split
    doc_chunks = _split_document(raw_text, doc_id, cursor)
    conn.commit()

    chunks = []
    for chunk in doc_chunks:
        raw_chunk = chunk.get("raw_chunk", "")
        heading = chunk.get("notes", "") or ""
        if not heading and raw_chunk:
            first_line = raw_chunk.split("\n", 1)[0].strip().lstrip("#").strip()
            heading = first_line[:60] if first_line else f"Chunk {chunk.get('chunk_index', 0) + 1}"
        chunks.append({
            "chunk_id": chunk["chunk_id"],
            "chunk_index": chunk.get("chunk_index", 0),
            "heading": heading,
            "preview": raw_chunk[:200],
            "char_count": len(raw_chunk),
        })

    batches = _build_batches(doc_chunks, doc_id)
    batch_info = [{"batch_id": b["batch_id"], "chunk_ids": b["chunk_ids"],
                   "oversize": [o["item_id"] for o in b.get("oversize_sections", [])]}
                  for b in batches]

    conn.close()
    return {"success": True, "doc_id": doc_id, "chunks": chunks, "batches": batch_info, "cached": False}


# ============ STEP 2b: MANUALIZE-CHUNK (progressive) ============

@router.post("/doc/{doc_id}/manualize-chunk/{chunk_id}")
def manualize_single_chunk(doc_id: str, chunk_id: str):
    """Manualize a single chunk. Returns section data immediately.
    Saves to manual_sections (appends, does not delete existing)."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT chunk_id, chunk_index, raw_chunk, notes FROM doc_chunks WHERE chunk_id = %s AND doc_id = %s", (chunk_id, doc_id))
    chunk_row = cursor.fetchone()
    if not chunk_row:
        conn.close()
        raise HTTPException(status_code=404, detail="Chunk not found")

    if not is_llm_available():
        conn.close()
        raise HTTPException(status_code=503, detail="LLM ë¹„í™œì„±")

    chunk = {
        "chunk_id": chunk_row["chunk_id"],
        "chunk_index": chunk_row["chunk_index"],
        "raw_chunk": chunk_row["raw_chunk"] or "",
        "notes": chunk_row["notes"] or "",
    }

    result = _manualize_chunk(chunk, doc_id)

    section_name = result["section_name"]
    if not section_name or not section_name.strip():
        section_name = f"## ê¸°íƒ€ (chunk-{chunk_id[:8]})"
    section_text = result["section_text"] or "ì •ë³´ ì—†ìŒ"
    evidence = result.get("evidence_spans", [])

    # Save section (check if already exists for this chunk)
    cursor.execute("SELECT section_id FROM manual_sections WHERE doc_id = %s AND source_chunk_id = %s", (doc_id, chunk_id))
    existing = cursor.fetchone()
    if existing:
        cursor.execute(
            """UPDATE manual_sections SET section_name = %s, section_text = %s,
               evidence_json = %s, updated_at = %s WHERE section_id = %s""",
            (section_name, section_text,
             json.dumps(evidence, ensure_ascii=False),
             datetime.now().isoformat(), existing["section_id"])
        )
        section_id = existing["section_id"]
    else:
        section_id = f"sec_{uuid.uuid4().hex[:8]}"
        cursor.execute(
            """INSERT INTO manual_sections
               (section_id, doc_id, section_name, section_text,
                source_chunk_id, evidence_json, merge_status)
               VALUES (%s, %s, %s, %s, %s, %s, 'BODY')""",
            (section_id, doc_id, section_name, section_text,
             chunk_id, json.dumps(evidence, ensure_ascii=False))
        )

    cursor.execute("UPDATE documents SET updated_at = %s WHERE doc_id = %s",
                   (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()

    evidence_matched = sum(1 for s in evidence if s.get("char_start", -1) >= 0)

    return {
        "success": result.get("success", True),
        "chunk_id": chunk_id,
        "section_id": section_id,
        "section_name": section_name,
        "section_text": section_text,
        "evidence_spans": evidence,
        "evidence_matched": evidence_matched,
        "evidence_total": len(evidence),
    }


# ============ STEP 2c: MANUALIZE-BATCH (progressive) ============

@router.post("/doc/{doc_id}/manualize-batch/{batch_id}")
def manualize_batch_endpoint(doc_id: str, batch_id: str):
    """Manualize a batch of chunks in a single LLM call.
    Returns section data for all items in the batch."""
    conn = get_connection()
    cursor = conn.cursor()

    # Load chunks for this batch
    cursor.execute("SELECT chunk_id, chunk_index, notes, raw_chunk FROM doc_chunks WHERE doc_id = %s ORDER BY chunk_index", (doc_id,))
    all_chunks = [dict(row) for row in cursor.fetchall()]
    if not all_chunks:
        conn.close()
        raise HTTPException(status_code=404, detail="No chunks found")

    # Rebuild batches to find the requested one
    batches = _build_batches(all_chunks, doc_id)
    target_batch = None
    for b in batches:
        if b["batch_id"] == batch_id:
            target_batch = b
            break
    if not target_batch:
        conn.close()
        raise HTTPException(status_code=404, detail=f"Batch {batch_id} not found")

    if not is_llm_available():
        conn.close()
        raise HTTPException(status_code=503, detail="LLM ë¹„í™œì„±")

    # Call LLM with batch prompt
    results = _manualize_batch(target_batch, doc_id)

    # Build chunk_id â†’ raw_chunk map for evidence indexing
    chunk_map = {c["chunk_id"]: c["raw_chunk"] for c in all_chunks}

    # Save each result to manual_sections
    saved = []
    for item in target_batch["items"]:
        chunk_id = item["item_id"]
        # Find matching result
        result = None
        for r in results:
            if r.get("item_id") == chunk_id:
                result = r
                break
        if not result:
            # Fallback: use first unmatched result or raw text
            if results:
                result = results.pop(0)
            else:
                result = {
                    "section_name": item["section_title"] or f"Chunk {chunk_id}",
                    "section_text": item["section_text"][:3000],
                    "evidence_spans": [],
                }

        section_name = result.get("section_name", item["section_title"] or f"Chunk {chunk_id}")
        if not section_name or not section_name.strip():
            section_name = f"## ê¸°íƒ€ (chunk-{chunk_id[:8]})"
        section_text = result.get("section_text", "") or "ì •ë³´ ì—†ìŒ"
        evidence_spans = result.get("evidence_spans", [])

        # Index evidence spans
        raw_chunk = chunk_map.get(chunk_id, "")
        indexed_spans = _index_evidence_spans(evidence_spans, raw_chunk)

        # Save/update
        cursor.execute("SELECT section_id FROM manual_sections WHERE doc_id = %s AND source_chunk_id = %s", (doc_id, chunk_id))
        existing = cursor.fetchone()
        if existing:
            cursor.execute(
                """UPDATE manual_sections SET section_name = %s, section_text = %s,
                   evidence_json = %s, updated_at = %s WHERE section_id = %s""",
                (section_name, section_text,
                 json.dumps(indexed_spans, ensure_ascii=False),
                 datetime.now().isoformat(), existing["section_id"]))
            section_id = existing["section_id"]
        else:
            section_id = f"sec_{uuid.uuid4().hex[:8]}"
            cursor.execute(
                """INSERT INTO manual_sections
                   (section_id, doc_id, section_name, section_text,
                    source_chunk_id, evidence_json, merge_status)
                   VALUES (%s, %s, %s, %s, %s, %s, 'BODY')""",
                (section_id, doc_id, section_name, section_text,
                 chunk_id, json.dumps(indexed_spans, ensure_ascii=False)))

        evidence_matched = sum(1 for s in indexed_spans if s.get("char_start", -1) >= 0)
        saved.append({
            "chunk_id": chunk_id,
            "section_id": section_id,
            "section_name": section_name,
            "section_text": section_text,
            "evidence_spans": indexed_spans,
            "evidence_matched": evidence_matched,
            "evidence_total": len(indexed_spans),
        })

    # Handle oversize sections
    oversize_errors = []
    for o in target_batch.get("oversize_sections", []):
        oversize_errors.append({
            "chunk_id": o["item_id"],
            "code": "OVERSIZE_SECTION",
            "message": f"ì„¹ì…˜ì´ {o['char_len']}ìë¡œ ë°°ì¹˜ í•œë„({MAX_BATCH_CHARS}ì) ì´ˆê³¼",
        })

    cursor.execute("UPDATE documents SET updated_at = %s WHERE doc_id = %s",
                   (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()

    return {
        "success": True,
        "batch_id": batch_id,
        "sections": saved,
        "errors": oversize_errors,
    }


# ============ STEP 2: SECTION GATE (per-section AI check) ============

GATE_CHECK_PROMPT = """ë‹¹ì‹ ì€ RAG ë°˜ì˜ ì „, ë§¤ë‰´ì–¼ ì„¹ì…˜ í…ìŠ¤íŠ¸(section_text)ì˜ í’ˆì§ˆ/ë¦¬ìŠ¤í¬ë¥¼ íŒì •í•˜ëŠ” QA ê²Œì´íŠ¸ì…ë‹ˆë‹¤.
ì¤‘ìš”: ë‚´ìš©ì„ ìƒˆë¡œ ì‘ì„±í•˜ê±°ë‚˜ ê³ ì¹˜ì§€ ë§ê³ , ì˜¤ì§ 'ê²€ì¦ ê²°ê³¼'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ì…ë ¥]
section_text: {section_text}
raw_text: {raw_text}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[íŒì • ìƒíƒœ]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASS: ë°”ë¡œ RAG ë°˜ì˜ ê°€ëŠ¥
NEED_FIX: ì‚¬ëŒì´ ìˆ˜ì •/ë³´ê°• í›„ ë°˜ì˜ ê¶Œì¥
BLOCK: RAG ë°˜ì˜ ê¸ˆì§€(ë³´ì•ˆ/ì‹¬ê° ì¶©ëŒ/ëŒ€ëŸ‰ ìœ ì‹¤)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ğŸš¨ ìµœìš°ì„  ì›ì¹™]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ì¶”ì¸¡ ê¸ˆì§€.
- ëª¨ë“  ì§€ì ì€ section_text ë˜ëŠ” raw_textì˜ **ì¦ê±° ë¬¸ìì—´ ê¸°ë°˜**ì´ì–´ì•¼ í•¨.
- location_hintì— ì¦ê±°ë¥¼ ì œì‹œí•  ìˆ˜ ì—†ìœ¼ë©´ reasonsì— ë„£ì§€ ë§ˆì„¸ìš”.
- "ê°€ëŠ¥ì„±", "ì¶”ì •", "ì•„ë§ˆ" ê°™ì€ í‘œí˜„ ê¸°ë°˜ ì§€ì  ê¸ˆì§€.
- ê°™ì€ ë‚´ìš©ì„ ë°˜ë³µ/ê³¼ì‰ ì§€ì í•˜ì§€ ë§ˆì„¸ìš”(í•µì‹¬ë§Œ).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ìš´ì˜ ì•ˆì „í•€]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- HIGH severityëŠ” "ëª…ë°±í•˜ê³  ì§ì ‘ì ì¸ ì¦ê±°"ê°€ ìˆì„ ë•Œë§Œ ë¶€ì—¬í•˜ì„¸ìš”.
  ì• ë§¤í•˜ê±°ë‚˜ í•´ì„ ì—¬ì§€ê°€ ìˆìœ¼ë©´ HIGHë¡œ ì˜¬ë¦¬ì§€ ë§ê³  MEDIUMìœ¼ë¡œ ë‘ì„¸ìš”(ê³¼ë„í•œ BLOCK ë°©ì§€).
- HIGHëŠ” ì¼ë°˜ í•´ì„ì´ ì•„ë‹ˆë¼, ì‚¬ëŒì´ ë³´ì•„ë„ ëª…ë°±í•œ ìœ„ë°˜/ì¶©ëŒë¡œ ì¸ì‹ë˜ëŠ” ê²½ìš°ì—ë§Œ ë¶€ì—¬í•˜ì„¸ìš”.
- scoreëŠ” ì •êµí•œ í‰ê°€ê°€ ì•„ë‹ˆë¼ status êµ¬ê°„(PASS/NEED_FIX/BLOCK)ì„ ë°˜ì˜í•˜ëŠ” ëŒ€ëµì  ê°’ì…ë‹ˆë‹¤.
  ìš´ì˜ íŒë‹¨ì€ scoreê°€ ì•„ë‹ˆë¼ statusë¥¼ ìš°ì„ í•˜ì„¸ìš”.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ê²€ì¦ í•­ëª©]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

1) PII_RISK (BLOCK ìš°ì„  â€” ì‹¤ì œ ê°’ì¼ ë•Œë§Œ)
ë‹¤ìŒ **ì‹¤ì œ ê°’**ì´ ë§ˆìŠ¤í‚¹ ì—†ì´ ìˆì„ ë•Œë§Œ ë¬¸ì œ:
- ì£¼ë¯¼ë²ˆí˜¸ íŒ¨í„´
- ê³„ì¢Œ/IDì„± ê¸´ ìˆ«ìì—´(10~16ìë¦¬)

PII_RISKê°€ ì•„ë‹Œ ê²ƒ (ì˜ˆì™¸):
- ê³ ê°ì„¼í„°/ëŒ€í‘œ ì „í™”ë²ˆí˜¸ (02-xxxx-xxxx, 1588-xxxx ë“±): ê³µê°œ ì—°ë½ì²˜ì´ë¯€ë¡œ PII ì•„ë‹˜
- ì„œë¹„ìŠ¤ ì•ˆë‚´ìš© ì´ë©”ì¼/URL: ê³µê°œ ì •ë³´ì´ë¯€ë¡œ PII ì•„ë‹˜
- ê°œì¸ íœ´ëŒ€í°(010-xxxx-xxxx)ë„ ë§¤ë‰´ì–¼ì— ì˜ë„ì ìœ¼ë¡œ ê¸°ì¬ëœ ê²ƒì´ë©´ PII ì•„ë‹˜
- "ì „í™”ë²ˆí˜¸/ì´ë©”ì¼/ì£¼ì†Œ" ê°™ì€ í•„ë“œëª…ì€ PII ì•„ë‹˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

2) CONFLICT
ë™ì¼ ì¡°ê±´/ëŒ€ìƒ/ìƒí™©ì—ì„œ **ì§ì ‘ ì¶©ëŒ**í•˜ëŠ” ê·œì¹™ë§Œ í•´ë‹¹.
(ê°€ëŠ¥ vs ë¶ˆê°€, í—ˆìš© vs ê¸ˆì§€ ë“±)
location_hint: ì¶©ëŒí•˜ëŠ” ë¬¸ì¥ ì¤‘ í•˜ë‚˜ë¥¼ section_textì—ì„œ **ì›ë¬¸ ê·¸ëŒ€ë¡œ** ë³µì‚¬ (10~60ì)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

3) MISSING / AMBIGUOUS
ì‹¤í–‰ ê·œì¹™ì¸ë° í•„ìˆ˜ê°’(ê¸°í•œ/ê¸ˆì•¡/ì¡°ê±´/ì±„ë„/ë‹´ë‹¹) ì—†ìœ¼ë©´ MISSING.
ëª¨í˜¸ í‘œí˜„ ë°˜ë³µ ì‹œ AMBIGUOUS:
(ì ë‹¹íˆ/ê°€ëŠ¥í•˜ë©´/ìƒí™©ì— ë”°ë¼/í˜‘ì˜ í›„/í•„ìš”ì‹œ)
ì†Œê°œ/ê°œìš” ì„¹ì…˜ì—ëŠ” ê³¼ë„ ì ìš© ê¸ˆì§€.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

4) HALLUCINATION_RISK
section_textì— ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì˜ë¬´ ê·œì¹™ì´ ìˆëŠ”ë°,
raw_textì—ì„œ **ë™ì¼ ì£¼ì œ ê´€ë ¨ í‘œí˜„ ì „ë°˜**ì„ ì°¾ê¸° ì–´ë ¤ìš¸ ë•Œë§Œ í•´ë‹¹.
- ë‹¨ì¼ í‚¤ì›Œë“œ ë¶€ì¬ë¡œ íŒë‹¨ ê¸ˆì§€.
- section_textì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 1~3ê°œë¥¼ ë¨¼ì € ë½‘ì•„ ë¹„êµ.

ì˜ˆì™¸ (HALLUCINATION_RISKë¡œ íŒì •í•˜ì§€ ë§ ê²ƒ):
- ì—°ë½ì²˜(ì „í™”ë²ˆí˜¸), ì´ë©”ì¼, URL, ì›¹ì‚¬ì´íŠ¸ ì£¼ì†Œ: ì‚¬ìš©ìê°€ ì˜ë„ì ìœ¼ë¡œ ì¶”ê°€í–ˆì„ ìˆ˜ ìˆìŒ
- "ì§€ì›", "ë¬¸ì˜", "ì•ˆë‚´" ë“± ì„œë¹„ìŠ¤ ì•ˆë‚´ ì„¹ì…˜ì˜ ì—°ë½ ì •ë³´ëŠ” ì‹ ë¢°
- [ì‚¬ìš©ì ì§ì ‘ ìˆ˜ì • êµ¬ê°„]ìœ¼ë¡œ í‘œì‹œëœ ë‚´ìš©

location_hint:
- section_textì—ì„œ ë¬¸ì œ ë˜ëŠ” ë¬¸ì¥/ë¶ˆë¦¿ì„ **ì›ë¬¸ ê·¸ëŒ€ë¡œ** ë³µì‚¬ (10~60ì)

severity:
- ê¸°ë³¸ MEDIUM
- HIGHëŠ” ì„œë¡œ ë‹¤ë¥¸ êµ¬ì²´ ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì˜ë¬´ ê·œì •ì´ 2ê°œ ì´ìƒ ì¶”ê°€ëœ ê²½ìš°ë§Œ
  (ê·¸ë¦¬ê³  ê·¸ ì¶”ê°€ê°€ ëª…ë°±íˆ í™•ì¸ë  ë•Œë§Œ)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

5) FORMAT
FORMATì€ ì•„ë˜ì¼ ë•Œë§Œ:
- section_textê°€ "##"ë¡œ ì‹œì‘í•˜ì§€ ì•ŠìŒ
- ### í•­ëª©ì´ 0ê°œ AND '- ' bulletë„ 3ê°œ ë¯¸ë§Œ

ì˜ˆì™¸:
### 0ê°œë¼ë„ '- ' bullet â‰¥ 3ì´ë©´ FORMAT ì•„ë‹˜.

FORMAT ë‹¨ë… BLOCK ê¸ˆì§€.
(í˜•ì‹ ë¬¸ì œëŠ” ìµœëŒ€ NEED_FIXê¹Œì§€ë§Œ)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

6) OMISSION (ì •ë³´ ìœ ì‹¤) â€” ë°©í–¥: raw_text â†’ section_textë§Œ í•´ë‹¹
raw_textì˜ **ì‹¤í–‰ ì •ë³´**ê°€ section_textì— ì—†ì„ ë•Œë§Œ OMISSION.
ì ìš© ëŒ€ìƒ:
- ì ˆì°¨ / ì¡°ê±´ / ì˜ˆì™¸ / ê¸ˆì•¡ / ê¸°í•œ / ë‹´ë‹¹ / ì±„ë„
ì ìš© ì œì™¸:
- ë°°ê²½ ì„¤ëª… / í™ë³´ ë¬¸êµ¬ / ì˜ˆì‹œ/ë¶€ì—°

âš ï¸ ë°©í–¥ ì£¼ì˜:
- OMISSION = raw_textì— ìˆëŠ”ë° section_textì— ì—†ëŠ” ê²ƒ
- section_textì— ìˆëŠ”ë° raw_textì— ì—†ëŠ” ê²ƒì€ OMISSIONì´ ì•„ë‹˜ (â†’ HALLUCINATION_RISK ê²€í†  ëŒ€ìƒ)
- section_textì—ë§Œ ì¡´ì¬í•˜ëŠ” ì—°ë½ì²˜/URL/ì´ë©”ì¼ ë“±ì„ OMISSIONìœ¼ë¡œ ë¶„ë¥˜í•˜ì§€ ë§ˆì„¸ìš”.
- [ì‚¬ìš©ìê°€ ì§ì ‘ ì‚­ì œí•œ ë‚´ìš©]ìœ¼ë¡œ í‘œì‹œëœ ë‚´ìš©ì€ OMISSIONìœ¼ë¡œ íŒì •í•˜ì§€ ë§ˆì„¸ìš”.

íŒì •(ì¦ê±° í•„ìˆ˜):
- raw_textì—ì„œ ëˆ„ë½ ì˜ˆì‹œ 2ê°œ(ì§§ì€ ë¬¸ì¥/ë¶ˆë¦¿) ì œì‹œ
- section_textì—ì„œ í•´ë‹¹ ë‚´ìš©ì´ ì—†ìŒì„ ëª…ì‹œ

location_hint: ëˆ„ë½ëœ ë‚´ìš©ì´ ì¶”ê°€ë˜ì–´ì•¼ í•  section_textì˜ **ì¸ì ‘ ì¤„**ì„ ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µì‚¬ (10~60ì).
(ì˜ˆ: raw_textì— 'ë½ì»¤ ì¢…ë¥˜: ì†Œí˜• 40x40cm'ê°€ ëˆ„ë½ â†’ section_textì˜ 'ë½ì»¤(ì‚¬ë¬¼í•¨)ì„¤ì •' ì¤„ì„ ë³µì‚¬)

ëŒ€ëŸ‰ ëˆ„ë½(í•µì‹¬ ì‹¤í–‰ì •ë³´ ë‹¤ìˆ˜ ëˆ„ë½) ì‹œë§Œ BLOCK.
- "ëŒ€ëŸ‰ ëˆ„ë½"ì€ ì‹¤í–‰ ì •ë³´ ìœ í˜•(ì ˆì°¨/ì¡°ê±´/ì˜ˆì™¸/ê¸ˆì•¡/ê¸°í•œ/ë‹´ë‹¹/ì±„ë„) ì¤‘ **2ê°€ì§€ ì´ìƒ ìœ í˜•ì— ëŒ€í•´**
  raw_textì— ëª…ì‹œëœ ì‹¤í–‰ ì •ë³´ê°€ section_textì—ì„œ **ë°˜ë³µì ìœ¼ë¡œ ëˆ„ë½**ë˜ëŠ” ê²½ìš°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
  (ë‹¨, ë‹¨ì¼ í•­ëª© ìˆ˜ì¤€ì˜ ê²½ë¯¸ ëˆ„ë½ì€ HIGHë¡œ ì˜¬ë¦¬ì§€ ë§ê³  MEDIUMìœ¼ë¡œ ë‘ì„¸ìš”.)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ê²€ì¦ ë¶ˆì¶©ë¶„]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASS ê¸ˆì§€ â†’ NEED_FIX (INSUFFICIENT_EVIDENCE)

í•´ë‹¹ ì¡°ê±´(ë°˜ë“œì‹œ ê·¼ê±°ë¡œ ì„¤ëª…):
- raw_text < 800ì AND (OMISSION/CONFLICT/HALLUCINATION_RISK) ëŒ€ì¡°ì— í•„ìš”í•œ ê·¼ê±°ê°€ ë¶€ì¡±í•¨
- section_text í•µì‹¬ í‚¤ì›Œë“œ(1~3ê°œ)ë¥¼ raw_textì—ì„œ ì°¾ê¸° ì–´ë ¤ì›Œ ê·¼ê±° ëŒ€ì¡°ê°€ ì‚¬ì‹¤ìƒ ë¶ˆê°€ëŠ¥í•¨
- ê·¼ê±° ë¶€ì¡±ìœ¼ë¡œ íŒë‹¨ ë¶ˆê°€

location_hint: section_textì—ì„œ ê·¼ê±° ë¶€ì¡±í•œ ë¬¸ì¥ì„ **ì›ë¬¸ ê·¸ëŒ€ë¡œ** ë³µì‚¬ (10~60ì)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ì¶œë ¥ í˜•ì‹]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RFC8259 JSONë§Œ ì¶œë ¥.

ì œì•½:
- reasonsëŠ” ìµœëŒ€ 6ê°œê¹Œì§€ë§Œ ì¶œë ¥
- ê°™ì€ typeì€ ìµœëŒ€ 2ê°œê¹Œì§€ë§Œ ì¶œë ¥

{{
  "status": "PASS|NEED_FIX|BLOCK",
  "score": 0,
  "reasons": [
    {{
      "type": "PII_RISK|CONFLICT|MISSING|AMBIGUOUS|HALLUCINATION_RISK|FORMAT|OMISSION|INSUFFICIENT_EVIDENCE",
      "severity": "LOW|MEDIUM|HIGH",
      "message": "ë¬¸ì œ ì„¤ëª…",
      "location_hint": "section_textì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” ë¶€ë¶„ì„ **ê·¸ëŒ€ë¡œ ë³µì‚¬**í•˜ì„¸ìš” (10~60ì). ì›ë¬¸ê³¼ í•œ ê¸€ìë¼ë„ ë‹¤ë¥´ë©´ í•˜ì´ë¼ì´íŠ¸ê°€ ì•ˆ ë©ë‹ˆë‹¤. ì„¤ëª…/ì‚¬ìœ ëŠ” ì—¬ê¸°ì— ì“°ì§€ ë§ˆì„¸ìš”.",
      "fix_suggestion": "ìˆ˜ì • ë°©ë²•"
    }}
  ],
  "required_actions": []
}}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Score ê¸°ì¤€]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PASS: 85~100
NEED_FIX: 50~84
BLOCK: 0~49

ê°ì  ê¸°ì¤€(100ì—ì„œ ê°ì ):
HIGH = -40
MEDIUM = -20
LOW = -10

- scoreëŠ” 0~100 ì •ìˆ˜ë¡œ í´ë¨í”„(ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ 0 ë˜ëŠ” 100ìœ¼ë¡œ ë³´ì •)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[Status ê²°ì • ê·œì¹™]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ì•„ë˜ ì¡°ê±´ì´ reasonsì— ì¡´ì¬í•˜ë©´ statusëŠ” ë°˜ë“œì‹œ ê·¸ë ‡ê²Œ ê²°ì •í•´ì•¼ í•©ë‹ˆë‹¤(ê°•ì œ).

- PII_RISK(HIGH) â†’ BLOCK (ë¬´ì¡°ê±´)
- CONFLICT(HIGH) â†’ BLOCK
- OMISSION(HIGH) â†’ BLOCK
- HALLUCINATION_RISK(HIGH) â†’ BLOCK

ì¶”ê°€ ê·œì¹™:
- HIGH 1ê°œ(ìœ„ 4ê°œ ìœ í˜• ì™¸ í¬í•¨) â†’ ìµœì†Œ NEED_FIX
- MEDIUM 2ê°œ ì´ìƒ â†’ NEED_FIX
- LOWë§Œ ì¡´ì¬ â†’ PASS ê°€ëŠ¥
- FORMATì€ BLOCK ê¸ˆì§€ ìœ ì§€

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[PASS ìµœì¢… ì¡°ê±´]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
reasonsê°€ ë¹„ì–´ ìˆì–´ë„ ì•„ë˜ ëª¨ë‘ ë§Œì¡±:
- PII ì—†ìŒ
- FORMAT í†µê³¼
- ê²€ì¦ ì¶©ë¶„(INSUFFICIENT_EVIDENCE ì•„ë‹˜)
- ëª…ë°±í•œ ëˆ„ë½/í™˜ê° ì—†ìŒ
"""


def _gate_section_internal(doc_id: str, section_name: str, cursor, raw_text_fallback: str = None):
    """Core gate logic reusable by gate_section() and gate-all.
    cursor must be from an open connection. Does NOT commit/close.
    raw_text_fallback: optional pre-fetched raw_text to avoid redundant query."""
    import time as _time
    t_start = _time.time()

    cursor.execute("SELECT section_id, section_text, source_chunk_id, ai_text, gate_reasons_json FROM manual_sections WHERE doc_id = %s AND section_name = %s",
                   (doc_id, section_name))
    sec = cursor.fetchone()
    if not sec:
        return {"success": False, "section_name": section_name, "error": "Section not found"}

    # Load previously dismissed reasons
    prev_dismissed = []
    try:
        prev_reasons = json.loads(sec["gate_reasons_json"] or "[]")
        prev_dismissed = [r for r in prev_reasons if r.get("dismissed")]
    except Exception:
        pass

    # Determine raw reference: prefer raw_chunk from doc_chunks if available
    raw_reference = ""
    if sec["source_chunk_id"]:
        cursor.execute("SELECT raw_chunk FROM doc_chunks WHERE chunk_id = %s", (sec["source_chunk_id"],))
        chunk_row = cursor.fetchone()
        if chunk_row and chunk_row["raw_chunk"]:
            raw_reference = chunk_row["raw_chunk"][:4000]

    if not raw_reference:
        if raw_text_fallback is not None:
            raw_reference = raw_text_fallback[:4000]
        else:
            cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
            doc = cursor.fetchone()
            raw_reference = (doc["raw_text"] or "")[:4000] if doc else ""

    # Detect user edits: diff section_text vs ai_text
    user_edits_note = ""
    ai_text = sec["ai_text"] or ""
    section_text = sec["section_text"] or ""
    if ai_text and section_text != ai_text:
        ai_lines = set(ai_text.strip().splitlines())
        sec_lines = set(section_text.strip().splitlines())
        sec_lines_list = section_text.strip().splitlines()
        # ì‚¬ìš©ìê°€ ì¶”ê°€í•œ ë¼ì¸ (ai_textì— ì—†ê³  section_textì— ìˆìŒ)
        added = [l for l in sec_lines_list if l.strip() and l not in ai_lines]
        # ì‚¬ìš©ìê°€ ì‚­ì œí•œ ë¼ì¸ (ai_textì— ìˆê³  section_textì— ì—†ìŒ)
        removed = [l for l in ai_text.strip().splitlines() if l.strip() and l not in sec_lines]
        parts = []
        if added:
            added_preview = "\n".join(added[:10])
            parts.append(f"[ì‚¬ìš©ìê°€ ì§ì ‘ ì¶”ê°€í•œ ë‚´ìš© â€” HALLUCINATION_RISK ì œì™¸ ëŒ€ìƒ]\n{added_preview}\n(ìœ„ ë‚´ìš©ì€ ì‚¬ìš©ìê°€ ì˜ë„ì ìœ¼ë¡œ ì¶”ê°€í•œ ê²ƒì´ë¯€ë¡œ raw_textì— ì—†ë”ë¼ë„ HALLUCINATION_RISKë¡œ íŒì •í•˜ì§€ ë§ˆì„¸ìš”.)")
        if removed:
            removed_preview = "\n".join(removed[:10])
            parts.append(f"[ì‚¬ìš©ìê°€ ì§ì ‘ ì‚­ì œí•œ ë‚´ìš© â€” OMISSION ì œì™¸ ëŒ€ìƒ]\n{removed_preview}\n(ìœ„ ë‚´ìš©ì€ ì‚¬ìš©ìê°€ ì˜ë„ì ìœ¼ë¡œ ì‚­ì œí•œ ê²ƒì´ë¯€ë¡œ section_textì— ì—†ë”ë¼ë„ OMISSIONìœ¼ë¡œ íŒì •í•˜ì§€ ë§ˆì„¸ìš”.)")
        if parts:
            user_edits_note = "\n\n" + "\n\n".join(parts)

    # Build dismissed note for LLM prompt
    dismissed_note = ""
    if prev_dismissed:
        dismissed_items = []
        for d in prev_dismissed:
            dismissed_items.append(f"- [{d.get('type','?')}] {d.get('description','')[:100]}")
        dismissed_note = "\n\n[ì´ì „ ê²€ì¦ì—ì„œ ì‚¬ìš©ìê°€ ë¬´ì‹œ(dismiss)í•œ ì´ìŠˆ - ë™ì¼ ì´ìŠˆë¥¼ ë‹¤ì‹œ ë³´ê³ í•˜ì§€ ë§ˆì„¸ìš”]\n" + "\n".join(dismissed_items)

    gate_result = {"status": "PASS", "score": 100, "reasons": [], "required_actions": []}

    if is_llm_available():
        try:
            prompt_text = GATE_CHECK_PROMPT.format(
                section_text=section_text[:3000],
                raw_text=raw_reference
            ) + user_edits_note + dismissed_note
            print(f"[GATE_SECTION] calling LLM for '{section_name}' (user_edits_note: {len(user_edits_note)} chars, dismissed: {len(prev_dismissed)})")
            content = call_llm(prompt_text, temperature=0.3)
            print(f"[GATE_SECTION] LLM response for '{section_name}': {content[:300] if content else 'EMPTY'}")
            if content:
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    gate_result = _clean_llm_json(json_match.group())
                    print(f"[GATE_SECTION] result for '{section_name}': status={gate_result.get('status')}, reasons={len(gate_result.get('reasons', []))}")
        except Exception as e:
            print(f"[GATE_SECTION] LLM error: {e}")
    else:
        print(f"[GATE_SECTION] LLM not available, returning default PASS")

    # Post-process: restore dismissed flags for matching reasons
    new_reasons = gate_result.get("reasons", [])
    if prev_dismissed:
        dismissed_keys = {(d.get("type", ""), d.get("description", "")[:80]) for d in prev_dismissed}
        for r in new_reasons:
            key = (r.get("type", ""), r.get("description", "")[:80])
            if key in dismissed_keys:
                r["dismissed"] = True

    # Append previously dismissed reasons that LLM didn't re-report (keep them visible as dismissed)
    existing_keys = {(r.get("type", ""), r.get("description", "")[:80]) for r in new_reasons}
    for d in prev_dismissed:
        key = (d.get("type", ""), d.get("description", "")[:80])
        if key not in existing_keys:
            d["dismissed"] = True
            new_reasons.append(d)

    gate_result["reasons"] = new_reasons

    # Recalculate status from active (non-dismissed) reasons only
    active = [r for r in new_reasons if not r.get("dismissed")]
    if not active:
        gate_result["status"] = "PASS"
        gate_result["score"] = max(gate_result.get("score", 100), 85)
    else:
        has_high = any(r.get("severity") == "HIGH" for r in active)
        high_types = {r.get("type") for r in active if r.get("severity") == "HIGH"}
        block_types = {"PII_RISK", "CONFLICT", "OMISSION", "HALLUCINATION_RISK"}
        if has_high and high_types & block_types:
            gate_result["status"] = "BLOCK"
        elif has_high or sum(1 for r in active if r.get("severity") == "MEDIUM") >= 2:
            gate_result["status"] = "NEED_FIX"
        else:
            gate_result["status"] = "PASS"

    # Save gate result to manual_sections
    cursor.execute(
        "UPDATE manual_sections SET gate_status = %s, gate_score = %s, gate_reasons_json = %s, gate_stale = 0, updated_at = %s WHERE section_id = %s",
        (gate_result["status"], gate_result.get("score", 100),
         json.dumps(gate_result["reasons"], ensure_ascii=False),
         datetime.now().isoformat(), sec["section_id"])
    )

    gate_result["time_s"] = round(_time.time() - t_start, 2)
    return {"success": True, "section_name": section_name, **gate_result}


@router.post("/doc/{doc_id}/section/{section_name}/gate")
def gate_section(doc_id: str, section_name: str):
    """Run AI gate check on a single section.
    If source_chunk_id exists, use raw_chunk instead of full raw_text (chunk-based)."""
    conn = get_connection()
    cursor = conn.cursor()
    result = _gate_section_internal(doc_id, section_name, cursor)
    if not result.get("success"):
        conn.close()
        raise HTTPException(status_code=404, detail=result.get("error", "Section not found"))
    conn.commit()
    conn.close()
    return result


@router.post("/doc/{doc_id}/gate-all")
def gate_all(doc_id: str):
    """Run AI gate check on ALL sections of a document."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT section_name FROM manual_sections WHERE doc_id = %s", (doc_id,))
    sections = cursor.fetchall()
    if not sections:
        conn.close()
        raise HTTPException(status_code=400, detail="No sections found")

    # Pre-fetch raw_text once for fallback
    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    raw_text_fallback = (doc["raw_text"] or "") if doc else ""

    results = []
    pass_count = need_fix_count = block_count = 0
    for sec in sections:
        r = _gate_section_internal(doc_id, sec["section_name"], cursor, raw_text_fallback=raw_text_fallback)
        results.append(r)
        status = r.get("status", "PASS")
        if status == "PASS":
            pass_count += 1
        elif status == "NEED_FIX":
            need_fix_count += 1
        elif status == "BLOCK":
            block_count += 1

    conn.commit()
    conn.close()
    return {
        "success": True, "pass_count": pass_count,
        "need_fix_count": need_fix_count, "block_count": block_count,
        "results": results
    }


@router.post("/doc/{doc_id}/fill-all")
def fill_all(doc_id: str):
    """ë§¥ë½ ë³´ê°•(Fill)ë§Œ ì „ ì„¹ì…˜ì— ì‹¤í–‰."""
    if not is_llm_available():
        raise HTTPException(status_code=503, detail="LLM ì‚¬ìš© ë¶ˆê°€")

    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    raw_text = doc["raw_text"] or ""
    raw_text_safe = raw_text[:4000] if raw_text else "(ì›ë³¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”.)"

    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
    all_sections = cursor.fetchall()
    if not all_sections:
        conn.close()
        raise HTTPException(status_code=400, detail="No sections found")

    total = len(all_sections)
    done_count = 0
    failed_sections = []

    for sec in all_sections:
        section_name = sec["section_name"]
        try:
            qa_policy_text = "Q&AëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ê¸°ì¡´ Q&Aë§Œ ìœ ì§€/ì •ë¦¬í•˜ì„¸ìš”."
            prompt = FILL_SECTION_TEXT_PROMPT_V3.format(
                section_text=sec["section_text"],
                raw_text=raw_text_safe,
                qa_policy_text=qa_policy_text
            )
            result = call_llm(prompt, temperature=0.3)
            if result and result.strip():
                cursor.execute(
                    "UPDATE manual_sections SET section_text = %s, updated_at = %s WHERE doc_id = %s AND section_name = %s",
                    (result.strip(), datetime.now().isoformat(), doc_id, section_name)
                )
                done_count += 1
        except Exception as e:
            print(f"[FILL_ALL] error for {section_name}: {e}")
            failed_sections.append({"section_name": section_name, "reason": str(e)})

    conn.commit()
    conn.close()
    return {
        "success": True, "total": total,
        "done_count": done_count, "failed_count": len(failed_sections),
        "failed_sections": failed_sections
    }


@router.post("/doc/{doc_id}/refine-all")
def refine_all(doc_id: str):
    """RAG ìµœì í™”ë§Œ ì „ ì„¹ì…˜ì— ì‹¤í–‰."""
    if not is_llm_available():
        raise HTTPException(status_code=503, detail="LLM ì‚¬ìš© ë¶ˆê°€")

    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    raw_text = doc["raw_text"] or ""
    raw_text_safe = raw_text[:4000] if raw_text else "(ì›ë³¸ ì—†ìŒ)"

    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
    all_sections = cursor.fetchall()
    if not all_sections:
        conn.close()
        raise HTTPException(status_code=400, detail="No sections found")

    total = len(all_sections)
    done_count = 0
    failed_sections = []

    for sec in all_sections:
        section_name = sec["section_name"]
        try:
            prompt = FINALIZE_SECTION_TEXT_PROMPT_V1.format(
                section_text=sec["section_text"],
                raw_text=raw_text_safe
            )
            result = call_llm(prompt, temperature=0.3)
            if result and result.strip():
                cursor.execute(
                    "UPDATE manual_sections SET section_text = %s, updated_at = %s WHERE doc_id = %s AND section_name = %s",
                    (result.strip(), datetime.now().isoformat(), doc_id, section_name)
                )
                done_count += 1
        except Exception as e:
            print(f"[REFINE_ALL] error for {section_name}: {e}")
            failed_sections.append({"section_name": section_name, "reason": str(e)})

    conn.commit()
    conn.close()
    return {
        "success": True, "total": total,
        "done_count": done_count, "failed_count": len(failed_sections),
        "failed_sections": failed_sections
    }


class PhaseUpdate(BaseModel):
    phase: str  # "fill", "refine", "gate"


@router.post("/doc/{doc_id}/completed-phase")
def save_completed_phase(doc_id: str, req: PhaseUpdate):
    """Add a completed phase to the document's completed_phases field."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT completed_phases FROM documents WHERE doc_id = %s", (doc_id,))
    row = cursor.fetchone()
    if not row:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    existing = row["completed_phases"] or ""
    phases = [p for p in existing.split(",") if p]
    if req.phase not in phases:
        phases.append(req.phase)
    cursor.execute("UPDATE documents SET completed_phases = %s WHERE doc_id = %s",
                   (",".join(phases), doc_id))
    conn.commit()
    conn.close()
    return {"success": True, "completed_phases": ",".join(phases)}


@router.post("/doc/{doc_id}/snapshot-ai-text")
def snapshot_ai_text(doc_id: str):
    """Copy current section_text to ai_text for all sections (after AI operation)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE manual_sections SET ai_text = section_text WHERE doc_id = %s",
        (doc_id,)
    )
    updated = cursor.rowcount
    conn.commit()
    conn.close()
    return {"success": True, "updated": updated}


class SnapshotSectionAiText(BaseModel):
    section_name: str


@router.post("/doc/{doc_id}/snapshot-ai-text-section")
def snapshot_ai_text_section(doc_id: str, req: SnapshotSectionAiText):
    """Copy current section_text to ai_text for a single section."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE manual_sections SET ai_text = section_text WHERE doc_id = %s AND section_name = %s",
        (doc_id, req.section_name)
    )
    conn.commit()
    conn.close()
    return {"success": True}


@router.post("/doc/{doc_id}/section/{section_name}/gate-dismiss")
def gate_dismiss(doc_id: str, section_name: str, body: dict = Body(...)):
    """Toggle dismissed flag on a gate reason. Recalculate status from active reasons."""
    reason_index = body.get("reason_index")
    if reason_index is None:
        raise HTTPException(status_code=400, detail="reason_index required")
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        "SELECT gate_reasons_json, gate_status FROM manual_sections WHERE doc_id = %s AND section_name = %s",
        (doc_id, section_name)
    )
    sec = cursor.fetchone()
    if not sec:
        conn.close()
        raise HTTPException(status_code=404, detail="Section not found")
    reasons = []
    try:
        reasons = json.loads(sec["gate_reasons_json"] or "[]")
    except Exception:
        pass
    if not (0 <= reason_index < len(reasons)):
        conn.close()
        raise HTTPException(status_code=400, detail="Invalid reason_index")
    # Toggle dismissed flag
    reasons[reason_index]["dismissed"] = not reasons[reason_index].get("dismissed", False)
    # Recalculate status from active (non-dismissed) reasons only
    active = [r for r in reasons if not r.get("dismissed")]
    if not active:
        new_status = "PASS"
    else:
        has_high = any(r.get("severity") == "HIGH" for r in active)
        high_types = {r.get("type") for r in active if r.get("severity") == "HIGH"}
        block_types = {"PII_RISK", "CONFLICT", "OMISSION", "HALLUCINATION_RISK"}
        if has_high and high_types & block_types:
            new_status = "BLOCK"
        else:
            new_status = "NEED_FIX"
    cursor.execute(
        "UPDATE manual_sections SET gate_reasons_json = %s, gate_status = %s, gate_stale = 0, updated_at = %s WHERE doc_id = %s AND section_name = %s",
        (json.dumps(reasons, ensure_ascii=False), new_status, datetime.now().isoformat(), doc_id, section_name)
    )
    conn.commit()
    conn.close()
    return {"success": True, "status": new_status, "reasons": reasons}


@router.post("/doc/{doc_id}/section/{section_name}/gate-stale")
def set_gate_stale(doc_id: str, section_name: str):
    """Mark section as gate_stale (saved without gate re-check)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE manual_sections SET gate_stale = 1, updated_at = %s WHERE doc_id = %s AND section_name = %s",
        (datetime.now().isoformat(), doc_id, section_name)
    )
    conn.commit()
    conn.close()
    return {"success": True}


# ============ STEP 2: QUALITY GATE (document-level) ============

QUALITY_GATE_PROMPT = """ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ ìš´ì˜ ë§¤ë‰´ì–¼ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë§¤ë‰´ì–¼ ì„¹ì…˜ë“¤ì„ ê²€í† í•˜ê³  ì´ìŠˆë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì´ìŠˆ íƒ€ì…:
- MISSING (RED): í™˜ë¶ˆ/ì˜ˆì•½/ìš´ì˜ì‹œê°„/ê¶Œí•œ ì¤‘ í•µì‹¬ ì •ë³´ê°€ ì™„ì „íˆ ì—†ìŒ
- AMBIGUOUS (YELLOW): "ìƒí™©ì— ë”°ë¼", "ê°€ëŠ¥í•˜ë©´", "ì ë‹¹íˆ", "í˜‘ì˜ í›„" ë“± ëª¨í˜¸í•œ í‘œí˜„
- CONFLICT (RED): ê°™ì€ ì£¼ì œì—ì„œ ìƒë°˜ëœ ê·œì¹™ ë°œê²¬
- PII_RISK (RED): ì£¼ë¯¼ë²ˆí˜¸/ì „í™”ë²ˆí˜¸ ë“± ê°œì¸ì •ë³´ íŒ¨í„´
- API_NEEDED (YELLOW): "ì˜ˆì•½ ìƒì„±", "ë¬¸ì ë°œì†¡", "ê°•ì¢Œ ì¶”ê°€" ë“± ì‹œìŠ¤í…œ ì—°ë™ í•„ìš”

ê° ì´ìŠˆ í˜•ì‹:
{{"severity": "RED|YELLOW", "issue_type": "íƒ€ì…", "message": "ì„¤ëª…", "suggestion": "í•´ê²°ë°©ì•ˆ"}}

ë§¤ë‰´ì–¼ ë‚´ìš©:
{sections_text}

JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""


@router.post("/doc/{doc_id}/quality-gate")
def quality_gate(doc_id: str):
    """Run quality checks on manual sections."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
    sections = cursor.fetchall()
    if not sections:
        conn.close()
        raise HTTPException(status_code=400, detail="Manualize first")
    
    sections_text = "\n\n".join([f"[{s['section_name']}]\n{s['section_text']}" for s in sections])
    
    issues = []
    
    # Rule-based checks first
    # PII check
    pii_patterns = [
        (r'\d{6}-\d{7}', 'ì£¼ë¯¼ë²ˆí˜¸'),
        (r'010-?\d{4}-?\d{4}', 'ì „í™”ë²ˆí˜¸'),
    ]
    for pattern, pii_type in pii_patterns:
        if re.search(pattern, sections_text):
            issues.append({
                "severity": "RED",
                "issue_type": "PII_RISK",
                "message": f"{pii_type} íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤",
                "suggestion": "ê°œì¸ì •ë³´ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ë§ˆìŠ¤í‚¹í•˜ì„¸ìš”"
            })
    
    # Ambiguous phrases check
    ambiguous_phrases = ["ìƒí™©ì— ë”°ë¼", "ê°€ëŠ¥í•˜ë©´", "ì ë‹¹íˆ", "í˜‘ì˜ í›„", "ê²½ìš°ì— ë”°ë¼", "í•„ìš”ì‹œ"]
    for phrase in ambiguous_phrases:
        if phrase in sections_text:
            issues.append({
                "severity": "YELLOW",
                "issue_type": "AMBIGUOUS",
                "message": f"ëª¨í˜¸í•œ í‘œí˜„ ë°œê²¬: '{phrase}'",
                "suggestion": "êµ¬ì²´ì ì¸ ê¸°ì¤€ì´ë‚˜ ì¡°ê±´ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”"
            })
    
    # API needed check
    api_phrases = ["ì˜ˆì•½ ìƒì„±", "ì˜ˆì•½ ì·¨ì†Œ", "ë¬¸ì ë°œì†¡", "SMS", "ê°•ì¢Œ ì¶”ê°€", "ê°•ì¢Œ ì‚­ì œ", "íšŒì› ë“±ë¡"]
    for phrase in api_phrases:
        if phrase in sections_text:
            issues.append({
                "severity": "YELLOW",
                "issue_type": "API_NEEDED",
                "message": f"ì‹œìŠ¤í…œ ì—°ë™ í•„ìš”: '{phrase}'",
                "suggestion": "í•´ë‹¹ ê¸°ëŠ¥ì˜ API ìŠ¤í™ì„ ì •ì˜í•˜ì„¸ìš”"
            })
    
    # Missing check for critical sections
    for s in sections:
        if s["section_text"] == "ì •ë³´ ì—†ìŒ" and s["section_name"] in ["í™˜ë¶ˆ/ìœ„ì•½/ì •ì‚°", "ì˜ˆì•½/ì·¨ì†Œ/ë³€ê²½", "ìš´ì˜ì‹œê°„/íœ´ë¬´"]:
            issues.append({
                "severity": "RED",
                "issue_type": "MISSING",
                "message": f"í•„ìˆ˜ ì„¹ì…˜ '{s['section_name']}'ì˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤",
                "suggestion": "í•´ë‹¹ ê·œì •ì„ ì¶”ê°€í•˜ì„¸ìš”"
            })
    
    # LLM-based additional checks if available
    llm_error_msg = None
    if is_llm_available() and len(issues) < 5:
        try:
            content = call_llm(QUALITY_GATE_PROMPT.format(sections_text=sections_text[:6000]), temperature=0.3)
            if content:
                json_match = re.search(r'\[[\s\S]*\]', content)
                if json_match:
                    llm_issues = _clean_llm_json(json_match.group())
                    issues.extend(llm_issues[:5])  # Limit LLM issues
            else:
                llm_error_msg = "LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°€ëŠ¥ì„±)"
        except Exception as e:
            llm_error_msg = f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            print(f"[QUALITY_GATE] LLM error: {e}")
    
    # Clear old issues and save new
    cursor.execute("DELETE FROM qa_issues WHERE doc_id = %s", (doc_id,))
    for issue in issues:
        issue_id = f"issue_{uuid.uuid4().hex[:8]}"
        cursor.execute(
            "INSERT INTO qa_issues (issue_id, doc_id, severity, issue_type, message, suggestion, status) VALUES (%s, %s, %s, %s, %s, %s, 'OPEN')",
            (issue_id, doc_id, issue.get("severity", "YELLOW"), issue.get("issue_type", "OTHER"), 
             issue.get("message", ""), issue.get("suggestion", ""))
        )
    
    conn.commit()
    conn.close()
    
    red_count = len([i for i in issues if i.get("severity") == "RED"])
    yellow_count = len([i for i in issues if i.get("severity") == "YELLOW"])
    
    return {
        "success": True, 
        "doc_id": doc_id, 
        "red_count": red_count, 
        "yellow_count": yellow_count, 
        "issues": issues,
        "llm_error": llm_error_msg,
        "api_specs": extract_api_spec(doc_id).get("specs", [])
    }


# ============ STEP 2: UPDATE SECTIONS ============

class SectionsUpdate(BaseModel):
    sections: dict # {section_name: text}

@router.put("/doc/{doc_id}/sections")
def update_sections(doc_id: str, req: SectionsUpdate):
    """Update manual sections manually."""
    conn = get_connection()
    cursor = conn.cursor()

    for name, text in req.sections.items():
        cursor.execute(
            "UPDATE manual_sections SET section_text = %s WHERE doc_id = %s AND section_name = %s",
            (text, doc_id, name)
        )
    
    cursor.execute("UPDATE documents SET updated_at = %s WHERE doc_id = %s", (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id}


# ============ STEP 2: AI HELPER (Fill/Refine) ============

class RefineRequest(BaseModel):
    text: str
    task: str  # "refine", "fill", "recommend"
    context: Optional[str] = None  # Section name or issue message
    allow_qa: Optional[str] = None  # "true" or "false" (for fill task)


def _clean_llm_json(raw: str):
    """Clean and parse JSON from LLM response. Handles common issues.
    Returns dict or list depending on input."""
    raw = re.sub(r',\s*([}\]])', r'\1', raw)       # trailing comma
    raw = raw.replace('\n', ' ').replace('\r', '')   # newlines
    raw = re.sub(r'[\x00-\x1f]', ' ', raw)          # control chars
    # Attempt 1: direct parse
    try:
        return json.loads(raw)
    except json.JSONDecodeError:
        pass
    # Attempt 2: single â†’ double quotes
    try:
        return json.loads(raw.replace("'", '"'))
    except json.JSONDecodeError:
        pass
    # Attempt 3: fix unescaped quotes inside string values
    # e.g. "message": "ë¬¸ì œëŠ” "ì´ê²ƒ" ì…ë‹ˆë‹¤" â†’ "message": "ë¬¸ì œëŠ” \"ì´ê²ƒ\" ì…ë‹ˆë‹¤"
    try:
        fixed = re.sub(
            r'(?<=: )"((?:[^"\\]|\\.)*?)"(?=\s*[,}\]])',
            lambda m: '"' + m.group(1) + '"',
            raw
        )
        return json.loads(fixed)
    except json.JSONDecodeError:
        pass
    # Attempt 4: try closing at each } or ] from the end (handles trailing garbage)
    close_positions = [i for i, c in enumerate(raw) if c in ('}', ']')]
    for pos in reversed(close_positions):
        try:
            return json.loads(raw[:pos + 1])
        except json.JSONDecodeError:
            continue
    raise json.JSONDecodeError("_clean_llm_json: all attempts failed", raw, 0)


def _flatten_manualize_json(parsed: dict) -> dict:
    """Convert MANUALIZE_PROMPT JSON output to flat {section_name: section_text} dict.
    Formats each section as structured markdown (## / ### / - bullets)."""
    sections_map = {}
    for sec in parsed.get("sections", []):
        name = sec.get("name", sec.get("section_id", "general"))
        lines = [f"## {name}"]
        for rule in sec.get("content", []):
            title = rule.get("title", "")
            if title:
                lines.append(f"### {title}")
            for bullet in rule.get("bullets", []):
                lines.append(f"- {bullet}")
            # structured fields
            st = rule.get("structured", {})
            for key in ("target", "condition", "owner", "channel"):
                val = st.get(key, "")
                if val:
                    lines.append(f"- {key}: {val}")
            for proc_item in st.get("procedure", []):
                lines.append(f"- {proc_item}")
            for exc_item in st.get("exceptions", []):
                lines.append(f"- ì˜ˆì™¸: {exc_item}")
        section_text = "\n".join(lines)
        sections_map[name] = section_text
    # Fallback: if no sections parsed, try summary
    if not sections_map:
        summary = parsed.get("summary", "")
        if summary:
            sections_map["general"] = f"## ìš”ì•½\n- {summary}"
        else:
            sections_map["general"] = "ì •ë³´ ì—†ìŒ"
    return sections_map


def _to_bool_allow_qa(val) -> bool:
    """Normalize allow_qa to bool. Accepts bool, str, int, None."""
    if val is None:
        return False
    if isinstance(val, bool):
        return val
    if isinstance(val, str):
        return val.strip().lower() in ("true", "1", "yes")
    if isinstance(val, (int, float)):
        return bool(val)
    return False


# ============ CHUNK-BASED PIPELINE: PROMPTS ============

SPLIT_PROMPT = """ë‹¹ì‹ ì€ ê¸´ ë¬¸ì„œë¥¼ RAG ì²˜ë¦¬ì— ì í•©í•œ chunk(ì²­í¬)ë¡œ ë¶„í• í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì…ë ¥]
raw_text: {raw_text}

[ê·œì¹™]
1) í—¤ë”©(#, ##, ìˆ«ì. ë“±) ê²½ê³„ë¥¼ ìš°ì„ ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
2) ê° chunkëŠ” 3,000~8,000ì ëª©í‘œ, ìµœëŒ€ 12,000ìë¥¼ ë„˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
3) 3,000ì ë¯¸ë§Œì˜ ì§§ì€ ì„¹ì…˜ì€ ì¸ì ‘ ì„¹ì…˜ê³¼ í•©ì¹©ë‹ˆë‹¤.
4) start_anchor, end_anchorëŠ” ì›ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬í•œ ë¬¸ìì—´(20~60ì)ì…ë‹ˆë‹¤.
   - start_anchor: í•´ë‹¹ chunkê°€ ì‹œì‘í•˜ëŠ” ì›ë¬¸ ë¬¸ì¥/í—¤ë”©ì˜ ì²« ë¶€ë¶„
   - end_anchor: í•´ë‹¹ chunkê°€ ëë‚˜ëŠ” ì›ë¬¸ ë¬¸ì¥ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„
5) split_basis: ë¶„í•  ê·¼ê±° (ì˜ˆ: "heading", "length", "topic_shift")
6) chunk_idëŠ” "C1", "C2"... ìˆœì„œì…ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
RFC8259 JSON ë°°ì—´ë§Œ ì¶œë ¥. ì½”ë“œë¸”ë¡/ì„¤ëª…/ì£¼ì„ ê¸ˆì§€.

[
  {{
    "chunk_id": "C1",
    "start_anchor": "ì›ë¬¸ ì‹œì‘ ë¶€ë¶„ í…ìŠ¤íŠ¸",
    "end_anchor": "ì›ë¬¸ ë ë¶€ë¶„ í…ìŠ¤íŠ¸",
    "split_basis": "heading",
    "notes": ""
  }}
]

[ì œì•½]
- anchorëŠ” ë°˜ë“œì‹œ ì›ë¬¸(raw_text)ì— ì¡´ì¬í•˜ëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
- chunk ê°„ ê²¹ì¹¨ ì—†ì´ ì „ì²´ ë¬¸ì„œë¥¼ ë¹ ì§ì—†ì´ ì»¤ë²„í•´ì•¼ í•©ë‹ˆë‹¤.
- JSON ë°°ì—´ ì™¸ ì–´ë–¤ ì¶œë ¥ë„ ê¸ˆì§€í•©ë‹ˆë‹¤."""

MANUALIZE_CHUNK_PROMPT = """ë‹¹ì‹ ì€ ì˜ì—…/ìš´ì˜ ë¬¸ì„œì˜ ì¼ë¶€(chunk)ë¥¼ RAGì— ë„£ê¸° ìœ„í•œ
"êµ¬ì¡°í™”ëœ ì •ë³´ ì¶”ì¶œê¸°"ì…ë‹ˆë‹¤.

âš ï¸ ì´ ì‘ì—…ì€ ìš”ì•½ì´ ì•„ë‹™ë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì¤„ì´ê±°ë‚˜ ì••ì¶•í•˜ëŠ” ì‘ì—…ì´ ì•„ë‹™ë‹ˆë‹¤.

[ìµœìš°ì„  ëª©í‘œ]
- ì…ë ¥ëœ chunk(raw_chunk)ì˜ ì •ë³´ì™€ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ì—¬ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
- ì›ë¬¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€/ì¶”ì¸¡/ì°½ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì›ë¬¸ì— ìˆëŠ” ì •ë³´ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ê°œì¸ì •ë³´(ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ê³„ì¢Œë²ˆí˜¸ ë“±)ëŠ” ***ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.

[ì…ë ¥]
raw_chunk: {raw_chunk}

[ì¶œë ¥ í˜•ì‹]
RFC8259 ìœ íš¨ JSONë§Œ ì¶œë ¥. ì½”ë“œë¸”ë¡, ì„¤ëª…, ì£¼ì„, ë§ˆí¬ë‹¤ìš´ ê¸ˆì§€.

{{
  "section_name": "ì´ chunkì˜ ì£¼ì œë¥¼ ëŒ€í‘œí•˜ëŠ” ì„¹ì…˜ëª…",
  "section_text": "êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ (## í—¤ë”©, ### í•­ëª©, - ë¶ˆë¦¿ í˜•ì‹)",
  "evidence_spans": [
    {{
      "span_text": "raw_chunkì—ì„œ ë³µì‚¬í•œ ê·¼ê±° ë¬¸ì¥ (20~80ì)",
      "maps_to": "section_text ë‚´ ëŒ€ì‘ ìœ„ì¹˜ ì„¤ëª… (í•­ëª©ëª… ë˜ëŠ” ë¶ˆë¦¿ ìš”ì•½)",
      "is_pii": false
    }}
  ]
}}

[evidence_spans ê·œì¹™]
- ìµœì†Œ 2ê°œ ì´ìƒ ì‘ì„± (ê°€ëŠ¥í•˜ë©´ í•µì‹¬ ì •ë³´ë§ˆë‹¤ 1ê°œ)
- span_textëŠ” raw_chunk ì›ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬ (verbatim)
- is_pii=trueì¸ spanì€ ë§ˆìŠ¤í‚¹ëœ í˜•íƒœë¡œ ì‘ì„±
- maps_toëŠ” section_textì˜ ì–´ëŠ ë¶€ë¶„ì— ëŒ€ì‘í•˜ëŠ”ì§€ ê°„ë‹¨íˆ í‘œì‹œ

[section_text ì‘ì„± ê·œì¹™]
- "## ì„¹ì…˜ëª…"ìœ¼ë¡œ ì‹œì‘
- "### í•­ëª©ì œëª©"ìœ¼ë¡œ í•˜ìœ„ êµ¬ë¶„
- "- " ë¶ˆë¦¿ìœ¼ë¡œ ë³¸ë¬¸
- ì›ë¬¸ ì •ë³´ ì‚­ì œ/ìš”ì•½/ì••ì¶• ê¸ˆì§€
- ì›ë¬¸ ë¶ˆë¦¿ ê°œìˆ˜ì™€ í•­ëª© ê·¸ëŒ€ë¡œ ë³´ì¡´

[ê¸ˆì§€]
- ì¶”ì¸¡, ì¼ë°˜ ìƒì‹ ë³´ì™„, ìƒˆ ì •ë³´ ì¶”ê°€
- JSON ì™¸ ì¶œë ¥"""


# ============ CHUNK-BASED PIPELINE: HELPERS ============

def _resolve_anchors(raw_text: str, chunks_raw: list) -> list:
    """Resolve start_anchor/end_anchor to char_start/char_end positions in raw_text."""
    resolved = []
    search_from = 0
    for i, chunk in enumerate(chunks_raw):
        start_anchor = chunk.get("start_anchor", "")
        end_anchor = chunk.get("end_anchor", "")

        # Find start position
        char_start = raw_text.find(start_anchor, search_from) if start_anchor else search_from
        if char_start < 0:
            # Fallback: try from beginning
            char_start = raw_text.find(start_anchor) if start_anchor else search_from
        if char_start < 0:
            char_start = search_from

        # Find end position
        if end_anchor:
            end_search = max(char_start, search_from)
            end_pos = raw_text.find(end_anchor, end_search)
            if end_pos >= 0:
                char_end = end_pos + len(end_anchor)
            else:
                # Fallback: next chunk's start or end of text
                char_end = len(raw_text) if i == len(chunks_raw) - 1 else None
        else:
            char_end = len(raw_text) if i == len(chunks_raw) - 1 else None

        # Deferred end for non-last chunks
        if char_end is None:
            char_end = len(raw_text)  # will be adjusted by next chunk

        resolved.append({
            **chunk,
            "char_start": char_start,
            "char_end": char_end,
        })
        search_from = char_start + 1

    # Adjust: ensure no gaps/overlaps between consecutive chunks
    for i in range(len(resolved) - 1):
        if resolved[i + 1]["char_start"] < resolved[i]["char_end"]:
            resolved[i]["char_end"] = resolved[i + 1]["char_start"]

    # Extract raw_chunk text
    for r in resolved:
        r["raw_chunk"] = raw_text[r["char_start"]:r["char_end"]]

    return resolved


def _save_doc_chunks(doc_id: str, chunks: list, cursor) -> list:
    """Save resolved chunks to doc_chunks table. Returns list with chunk_ids."""
    cursor.execute("DELETE FROM doc_chunks WHERE doc_id = %s", (doc_id,))
    saved = []
    for i, chunk in enumerate(chunks):
        chunk_id = f"dchunk_{uuid.uuid4().hex[:8]}"
        cursor.execute("""
            INSERT INTO doc_chunks (chunk_id, doc_id, chunk_index, start_anchor, end_anchor,
                                    char_start, char_end, split_basis, notes, raw_chunk)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (chunk_id, doc_id, i,
              chunk.get("start_anchor", ""), chunk.get("end_anchor", ""),
              chunk.get("char_start", 0), chunk.get("char_end", 0),
              chunk.get("split_basis", ""), chunk.get("notes", ""),
              chunk.get("raw_chunk", "")))
        saved.append({**chunk, "chunk_id": chunk_id, "chunk_index": i})
    return saved


def _index_evidence_spans(spans: list, raw_chunk: str) -> list:
    """Index span_text positions within raw_chunk. Sets char_start/char_end or -1 if not found."""
    indexed = []
    for span in spans:
        span_text = span.get("span_text", "")
        is_pii = span.get("is_pii", False)

        if is_pii or not span_text:
            indexed.append({**span, "char_start": -1, "char_end": -1})
            continue

        pos = raw_chunk.find(span_text)
        if pos < 0:
            # Fallback: case-insensitive
            pos = raw_chunk.lower().find(span_text.lower())
        if pos >= 0:
            indexed.append({**span, "char_start": pos, "char_end": pos + len(span_text)})
        else:
            indexed.append({**span, "char_start": -1, "char_end": -1})
    return indexed


def _conservative_dedup(sections: list) -> list:
    """Remove exact-duplicate sections by section_name. Keep first occurrence."""
    seen = set()
    result = []
    for sec in sections:
        name = sec.get("section_name", "")
        if name not in seen:
            seen.add(name)
            result.append(sec)
    return result


# ============ BATCH MANUALIZE ============

MAX_BATCH_CHARS = 12000

MANUALIZE_BATCH_PROMPT = """ë‹¹ì‹ ì€ 'Manualize ë°°ì¹˜ ì‹¤í–‰ê¸°'ì…ë‹ˆë‹¤.
ì¤‘ìš”: ì•„ë˜ [MANUALIZE_RULES]ì˜ ê·œì¹™ì„ **ê·¸ëŒ€ë¡œ ì¤€ìˆ˜**í•˜ì—¬ ì‘ì—…í•˜ì„¸ìš”.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[MANUALIZE_RULES - SOURCE OF TRUTH]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- ì›ë¬¸(section_text)ì˜ ì •ë³´ì™€ êµ¬ì¡°ë¥¼ ìµœëŒ€í•œ ê·¸ëŒ€ë¡œ ë³´ì¡´í•˜ì—¬ êµ¬ì¡°í™”í•©ë‹ˆë‹¤.
- ì›ë¬¸ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€/ì¶”ì¸¡/ì°½ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ì›ë¬¸ì— ìˆëŠ” ì •ë³´ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ìƒëµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
- ìš”ì•½/ì••ì¶•/ì¼ë°˜í™” ê¸ˆì§€.
- ê°œì¸ì •ë³´(ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ê³„ì¢Œë²ˆí˜¸ ë“±)ëŠ” ***ë¡œ ë§ˆìŠ¤í‚¹í•©ë‹ˆë‹¤.
- section_text ë‚´ë¶€ì˜ í—¤ë”©/ë²ˆí˜¸/êµ¬ë¶„ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ë§¤í•‘í•˜ì„¸ìš”.
- êµ¬ë¶„ì´ ì—†ìœ¼ë©´ sections=1ê°œ(general)ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”.
- ì¶œë ¥ í˜•ì‹: "## ì„¹ì…˜ëª…", "### í•­ëª©ì œëª©", "- " ë¶ˆë¦¿.
- evidence_spans: ìµœì†Œ 2ê°œ, span_textëŠ” ì›ë¬¸ì—ì„œ ê·¸ëŒ€ë¡œ ë³µì‚¬(verbatim).

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ì…ë ¥]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{batches_json}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ì‘ì—… ê·œì¹™ - ë°°ì¹˜ ì „ìš©]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1) item(=ì„¹ì…˜) ê°„ ì •ë³´ ì„ê¸° ê¸ˆì§€. ê° ì„¹ì…˜ì€ ë…ë¦½ì ìœ¼ë¡œ Manualize.
2) ì„¹ì…˜ êµ¬ì¡° ë³´ì¡´. ì„¹ì…˜ ìˆ˜ë¥¼ ì„ì˜ ì¡°ì • ê¸ˆì§€.
3) oversize_sectionsì— ìˆëŠ” í•­ëª©ì€ Manualize í•˜ì§€ ë§ê³  errorsì— ê¸°ë¡ë§Œ.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
[ì¶œë ¥ í˜•ì‹]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RFC8259 ìœ íš¨ JSONë§Œ ì¶œë ¥. ì½”ë“œë¸”ë¡/ì„¤ëª…/ì£¼ì„ ê¸ˆì§€.

{{
  "batch_id": "<ì…ë ¥ batch_id>",
  "results": [
    {{
      "item_id": "S001",
      "section_name": "ì„¹ì…˜ ì£¼ì œë¥¼ ëŒ€í‘œí•˜ëŠ” ì´ë¦„",
      "section_text": "êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ (## / ### / - í˜•ì‹)",
      "evidence_spans": [
        {{
          "span_text": "ì›ë¬¸ì—ì„œ ë³µì‚¬í•œ ê·¼ê±° ë¬¸ì¥ (20~80ì)",
          "maps_to": "section_text ë‚´ ëŒ€ì‘ ìœ„ì¹˜",
          "is_pii": false
        }}
      ]
    }}
  ],
  "errors": []
}}"""


def _build_batches(doc_chunks: list, doc_id: str, max_chars: int = MAX_BATCH_CHARS) -> list:
    """Group doc_chunks into batches. Section boundary = batch boundary.
    A section never splits across batches. If adding a section exceeds max_chars,
    it goes to the next batch."""
    batches = []
    current_items = []
    current_chars = 0
    oversize = []
    batch_index = 0

    for chunk in doc_chunks:
        raw_chunk = chunk.get("raw_chunk", "")
        char_len = len(raw_chunk)
        heading = chunk.get("notes", "") or ""
        if not heading and raw_chunk:
            first_line = raw_chunk.split("\n", 1)[0].strip().lstrip("#").strip()
            heading = first_line[:60] if first_line else f"Chunk {chunk.get('chunk_index', 0) + 1}"

        item = {
            "item_id": chunk["chunk_id"],
            "section_title": heading,
            "section_text": raw_chunk,
            "char_len": char_len,
        }

        # Oversize: single section exceeds max_chars
        if char_len > max_chars:
            oversize.append({
                "item_id": chunk["chunk_id"],
                "section_title": heading,
                "char_len": char_len,
                "reason": "section_text exceeds max_batch_chars"
            })
            continue

        # Would exceed limit â†’ close current batch, start new one
        if current_items and current_chars + char_len > max_chars:
            batch_index += 1
            batches.append({
                "doc_id": doc_id,
                "batch_id": f"B{batch_index:03d}",
                "batch_index": batch_index - 1,
                "max_batch_chars": max_chars,
                "items": current_items,
                "oversize_sections": [],
                "chunk_ids": [it["item_id"] for it in current_items],
            })
            current_items = []
            current_chars = 0

        current_items.append(item)
        current_chars += char_len

    # Last batch
    if current_items:
        batch_index += 1
        batches.append({
            "doc_id": doc_id,
            "batch_id": f"B{batch_index:03d}",
            "batch_index": batch_index - 1,
            "max_batch_chars": max_chars,
            "items": current_items,
            "oversize_sections": [],
            "chunk_ids": [it["item_id"] for it in current_items],
        })

    # Attach oversize to first batch (or create one)
    if oversize:
        if batches:
            batches[0]["oversize_sections"] = oversize
        else:
            batches.append({
                "doc_id": doc_id,
                "batch_id": "B001",
                "batch_index": 0,
                "max_batch_chars": max_chars,
                "items": [],
                "oversize_sections": oversize,
                "chunk_ids": [],
            })

    return batches


def _manualize_batch(batch: dict, doc_id: str) -> list:
    """Process a single batch via LLM. Returns list of per-item results."""
    items = batch.get("items", [])
    if not items:
        return []

    # Build batch JSON for prompt
    batch_input = {
        "doc_id": doc_id,
        "batch_id": batch["batch_id"],
        "max_batch_chars": batch["max_batch_chars"],
        "items": [
            {"item_id": it["item_id"], "section_title": it["section_title"],
             "section_text": it["section_text"], "char_len": it["char_len"]}
            for it in items
        ],
        "oversize_sections": batch.get("oversize_sections", []),
    }

    for attempt in range(2):
        try:
            content = call_llm(
                MANUALIZE_BATCH_PROMPT.format(
                    batches_json=json.dumps(batch_input, ensure_ascii=False)
                ),
                temperature=0.3
            )
            if content:
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    parsed = _clean_llm_json(json_match.group())
                    results = parsed.get("results", [])
                    if results:
                        return results
        except Exception as e:
            print(f"[MANUALIZE_BATCH] attempt {attempt + 1} failed for {batch['batch_id']}: {e}")

    # Fallback: return raw text as-is for each item
    fallback = []
    for it in items:
        fallback.append({
            "item_id": it["item_id"],
            "section_name": it["section_title"] or f"[ë¯¸ì²˜ë¦¬] {it['item_id']}",
            "section_text": it["section_text"][:3000],
            "evidence_spans": [],
        })
    return fallback


# ============ CHUNK-BASED PIPELINE: CORE FUNCTIONS ============

def _split_document(raw_text: str, doc_id: str, cursor):
    """Split document into chunks using rule-based splitting (no LLM).
    Returns list of resolved chunks with raw_chunk text."""
    chunks_raw = []

    # Rule-based: heading split
    heading_chunks = _split_raw_by_headings(raw_text)
    if heading_chunks:
        for i, hc in enumerate(heading_chunks):
            start_anchor = hc["body"][:50].strip() if hc["body"] else ""
            end_anchor = hc["body"][-50:].strip() if hc["body"] else ""
            chunks_raw.append({
                "chunk_id": f"C{i + 1}",
                "start_anchor": start_anchor,
                "end_anchor": end_anchor,
                "split_basis": "heading",
                "notes": hc["heading"]
            })

    # Fallback: single chunk (entire document)
    if not chunks_raw:
        print(f"[SPLIT] No headings found, single chunk for {doc_id}")
        chunks_raw = [{
            "chunk_id": "C1",
            "start_anchor": raw_text[:50].strip(),
            "end_anchor": raw_text[-50:].strip(),
            "split_basis": "single_document",
            "notes": "No split possible"
        }]

    # Resolve anchors to char positions and extract raw_chunk
    resolved = _resolve_anchors(raw_text, chunks_raw)

    # Save to DB
    saved = _save_doc_chunks(doc_id, resolved, cursor)
    print(f"[SPLIT] {doc_id}: {len(saved)} chunks created")
    return saved


def _manualize_chunk(chunk: dict, doc_id: str) -> dict:
    """Manualize a single chunk. Returns dict with section_name, section_text, evidence_spans, chunk_id.
    Retry once on failure."""
    raw_chunk = chunk.get("raw_chunk", "")
    chunk_id = chunk.get("chunk_id", "")

    for attempt in range(2):
        try:
            content = call_llm(
                MANUALIZE_CHUNK_PROMPT.format(raw_chunk=raw_chunk[:10000]),
                temperature=0.3
            )
            if content:
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    result = _clean_llm_json(json_match.group())
                    section_name = result.get("section_name", f"Chunk-{chunk.get('chunk_index', 0)}")
                    section_text = result.get("section_text", "")
                    evidence_spans = result.get("evidence_spans", [])

                    # Index evidence spans
                    indexed_spans = _index_evidence_spans(evidence_spans, raw_chunk)

                    return {
                        "section_name": section_name,
                        "section_text": section_text,
                        "evidence_spans": indexed_spans,
                        "chunk_id": chunk_id,
                        "success": True
                    }
        except Exception as e:
            print(f"[MANUALIZE_CHUNK] attempt {attempt + 1} failed for {chunk_id}: {e}")

    # Both attempts failed â€” return raw chunk as-is
    return {
        "section_name": f"[ë¯¸ì²˜ë¦¬] Chunk-{chunk.get('chunk_index', 0)}",
        "section_text": raw_chunk[:3000],
        "evidence_spans": [],
        "chunk_id": chunk_id,
        "success": False
    }


def _gate_chunk(section_text: str, raw_chunk: str) -> dict:
    """Run gate check on a chunk using only the raw_chunk (not full raw_text)."""
    gate_result = {"status": "PASS", "score": 100, "reasons": [], "required_actions": []}

    if not is_llm_available():
        return gate_result

    try:
        content = call_llm(GATE_CHECK_PROMPT.format(
            section_text=section_text[:3000],
            raw_text=raw_chunk[:4000]
        ), temperature=0.3)
        if content:
            gm = re.search(r'\{[\s\S]*\}', content)
            if gm:
                gate_result = _clean_llm_json(gm.group())
    except Exception as e:
        print(f"[GATE_CHUNK] error: {e}")

    return gate_result


def _merge_chunks(chunk_results: list, gate_results: list) -> dict:
    """Merge chunk manualize results. PASS â†’ BODY, NEED_FIX â†’ APPENDIX.
    Returns {sections_map, gate_map, evidence_map, merge_info}."""
    sections_map = {}
    gate_map = {}
    evidence_map = {}
    merge_info = {}

    for i, result in enumerate(chunk_results):
        gate = gate_results[i] if i < len(gate_results) else {"status": "PASS", "score": 100, "reasons": []}
        name = result["section_name"]
        gate_status = gate.get("status", "PASS")

        # Determine merge_status
        if gate_status in ("NEED_FIX", "BLOCK"):
            merge_status = "APPENDIX"
            display_name = f"[ë¶€ë¡] {name}"
        else:
            merge_status = "BODY"
            display_name = name

        # Conservative dedup: if same name exists, append index
        if display_name in sections_map:
            display_name = f"{display_name} ({i + 1})"

        sections_map[display_name] = result["section_text"]
        gate_map[display_name] = gate
        evidence_map[display_name] = result.get("evidence_spans", [])
        merge_info[display_name] = {
            "merge_status": merge_status,
            "chunk_id": result.get("chunk_id", ""),
            "original_name": name,
        }

    return {
        "sections_map": sections_map,
        "gate_map": gate_map,
        "evidence_map": evidence_map,
        "merge_info": merge_info,
    }


FILL_SECTION_TEXT_PROMPT_V3 = """ë‹¹ì‹ ì€ RAG ì²­í¬(ì„¹ì…˜ í…ìŠ¤íŠ¸)ë¥¼ 'ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ë§¤ë‰´ì–¼'ë¡œ ë³´ê°•í•˜ëŠ” í¸ì§‘ìì…ë‹ˆë‹¤.
ì¤‘ìš”: ì´ ì‘ì—…ì€ 'ìƒˆ ì •ë³´ ì¶”ê°€'ê°€ ì•„ë‹ˆë¼, ë™ì¼ ë¬¸ì„œ(raw_text) ë‚´ë¶€ì˜ ê´€ë ¨ ë‚´ìš©ì„ ëª¨ì•„ ì¬êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì…ë ¥]
section_text: {section_text}
raw_text: {raw_text}

[ì ˆëŒ€ ê·œì¹™]
1) ì›ë¬¸ì— ì—†ëŠ” ì •ë³´(ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì •ì±…/ì˜ˆì™¸)ë¥¼ ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
2) ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë‚´ìš©ì„ ì±„ìš°ì§€ ë§ê³ , í•´ë‹¹ ì§€ì ì—ë§Œ "[í™•ì¸ í•„ìš”: ë¬´ì—‡ì„ í™•ì¸?]" ë¼ë²¨ì„ ë¶™ì´ì„¸ìš”.
3) ì•”ë¬µ ì¡°ê±´/ì „ì œëŠ” raw_textì— ì•”ì‹œ/í‘œí˜„ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ëª…ì‹œì ìœ¼ë¡œ í’€ì–´ì“°ì„¸ìš”.
4) ì•½ì–´/ë‚´ë¶€ ìš©ì–´ëŠ” ì›ë¬¸ì— ë“±ì¥í•œ ê²ƒë§Œ í’€ì–´ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”. (ì›ë¬¸ì— ì—†ìœ¼ë©´ ê¸ˆì§€)
5) ê°œì¸ì •ë³´(ì „í™”/ì´ë©”ì¼/ê³„ì¢Œ/ìƒì„¸ì£¼ì†Œ/ì‹ë³„ë²ˆí˜¸ ë“±)ëŠ” ***ë¡œ ë§ˆìŠ¤í‚¹ì„ ìœ ì§€í•˜ì„¸ìš”. ì›ë¬¸ì— ìˆì–´ë„ ê·¸ëŒ€ë¡œ ë…¸ì¶œ ê¸ˆì§€.
6) [Q&A ì •ì±…] {qa_policy_text}
7) ê¸°ì¡´ section_textì˜ ì£¼ì œ/ë²”ìœ„ë¥¼ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”. (ë‹¤ë¥¸ ì„¹ì…˜ ì£¼ì œë¥¼ ì„ì–´ ë„£ì§€ ë§ ê²ƒ)

[ê°œì„  ëª©í‘œ]
- ì•ë’¤ ì„¹ì…˜ ì—†ì´ë„ ì´ section_textë§Œ ì½ê³  ë‹µë³€ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ,
  ì›ë¬¸ì— í©ì–´ì§„ ê´€ë ¨ ê·œì¹™/ì¡°ê±´/ì±„ë„/ì˜ˆì™¸ë¥¼ ì´ ì„¹ì…˜ ì•ˆì— í†µí•©í•˜ì„¸ìš”.
- ì¤‘ë³µ bullet ì œê±°, í‘œí˜„ ì •ëˆ, í•­ëª© ì œëª©ì„ ëª…í™•íˆ.
- ë„ˆë¬´ ê¸´ í•­ëª©ì€ ê°™ì€ ì£¼ì œ ì•ˆì—ì„œ 2ê°œë¡œ ìª¼ê°œë˜ í˜•ì‹ì€ ìœ ì§€í•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ìœ ì§€)]
- ì˜¤ì§ ê°œì„ ëœ section_text ì „ì²´ë¥¼ plain textë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ì„¹ì…˜ ì‹œì‘: "## "
- í•­ëª© ì œëª©: "### "
- ë³¸ë¬¸: "-" bullet
- Q&AëŠ” allow_qa=true ì´ê±°ë‚˜ ì›ë˜ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ, í•­ëª© í•˜ë‹¨ì— ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ í¬í•¨:
  - Q: ...
  - A: ...

[ì¶”ê°€ ê°€ì´ë“œ]
- raw_textì—ì„œ ê·¼ê±°ê°€ ëª…í™•í•œ ë‚´ìš©ë§Œ 'ëª¨ì•„ì„œ' ë„£ê³ , ê·¼ê±° ì—†ëŠ” ë¶€ë¶„ì€ ì±„ìš°ì§€ ì•ŠìŠµë‹ˆë‹¤.
- "[í™•ì¸ í•„ìš”]"ëŠ” ë‚¨ë°œí•˜ì§€ ë§ê³  'í•„ìˆ˜ íŒë‹¨ì— í•„ìš”í•œ í•µì‹¬ ë¹ˆì¹¸'ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì„¤ëª…/í•´ì„¤/JSON ì¶œë ¥ ê¸ˆì§€. ì˜¤ì§ ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""


FINALIZE_SECTION_TEXT_PROMPT_V1 = """ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ì ì¤‘ë¥ ê³¼ ë‹µë³€ ì¼ê´€ì„±ì„ ë†’ì´ê¸° ìœ„í•´ section_text(plain text)ë¥¼ 'ìµœì¢… ë¬¸êµ¬'ë¡œ ë‹¤ë“¬ëŠ” í¸ì§‘ìì…ë‹ˆë‹¤.
ì¤‘ìš”: ì‚¬ì‹¤/ì •ì±…/ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡ ë“± ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ í‘œí˜„ê³¼ êµ¬ì¡°ë§Œ ìµœì í™”í•©ë‹ˆë‹¤.

âš ï¸ ë‹¹ì‹ ì´ ë‹¤ë“¬ì–´ì•¼ í•˜ëŠ” ëŒ€ìƒì€ ì•„ë˜ [section_text]ë¿ì…ë‹ˆë‹¤.
âš ï¸ [raw_text]ëŠ” ì›ë³¸ ì°¸ê³ ìš©ì…ë‹ˆë‹¤. raw_textì˜ ë‹¤ë¥¸ ì„¹ì…˜ ë‚´ìš©ì„ section_textì— ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
âš ï¸ ì…ë ¥ëœ section_text ë²”ìœ„ ë°–ì˜ ë‚´ìš©ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

[ì…ë ¥]
section_text (ë‹¤ë“¬ì„ ëŒ€ìƒ â€” ì´ ì„¹ì…˜ë§Œ ì¶œë ¥): {section_text}

raw_text (ì°¸ê³ ìš© ì›ë³¸ â€” ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”): {raw_text}

[ì ˆëŒ€ ê·œì¹™]
1) ì´ ì„¹ì…˜(section_text)ë§Œ ë‹¤ë“¬ì–´ ì¶œë ¥. ë‹¤ë¥¸ ì„¹ì…˜ ë‚´ìš© í¬í•¨ ê¸ˆì§€
2) ìƒˆ ì •ë³´ ì¶”ê°€ ê¸ˆì§€(ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì •ì±…/ì˜ˆì™¸/ì ˆì°¨ ì°½ì‘ ê¸ˆì§€)
3) ì›ë¬¸/í˜„ section_textì™€ ë‹¤ë¥¸ ì‚¬ì‹¤ ìƒì„± ê¸ˆì§€
4) ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ìœ ì§€(***)
5) ê·¼ê±°ê°€ ë¶ˆëª…í™•í•œ ë¬¸ì¥ ì¶”ê°€ ê¸ˆì§€. ë¶ˆëª…í™•í•˜ë©´ "[í™•ì¸ í•„ìš”: ...]"ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë” ëª…í™•íˆ ì‘ì„±

[ìµœì í™” ëª©í‘œ]
- ê²€ìƒ‰ í‚¤ì›Œë“œì— ì˜ ê±¸ë¦¬ë„ë¡ í•­ëª© ì œëª©(###)ì„ 'ì§ˆë¬¸í˜• ë˜ëŠ” í‚¤ì›Œë“œí˜•'ìœ¼ë¡œ ì„ ëª…í•˜ê²Œ
  ì˜ˆ: "í™˜ë¶ˆ ê·œì •" â†’ "í™˜ë¶ˆ ê·œì •/ê¸°í•œ/ìœ„ì•½ê¸ˆ"
- bulletsë¥¼ ì§§ê³  ë³‘ë ¬ êµ¬ì¡°ë¡œ ì •ë¦¬(ì¤‘ë³µ ì œê±°)
- ê°™ì€ ì˜ë¯¸ì˜ í‘œí˜„ì„ í†µì¼(ìš©ì–´ í‘œì¤€í™”)
- Q&Aê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´, ì§ˆë¬¸ì„ ë” ëª…í™•íˆ í•˜ë˜ ë‹µì€ ê·¸ëŒ€ë¡œ(ë‚´ìš© ì¶”ê°€ ê¸ˆì§€)
- ë„ˆë¬´ ê¸´ bulletì€ 2ê°œë¡œ ë¶„ë¦¬í•˜ë˜ ì˜ë¯¸ ìœ ì§€

[ì¶œë ¥]
- ì˜¤ì§ ì´ ì„¹ì…˜ì˜ ìµœì¢… section_textë§Œ plain textë¡œ ì¶œë ¥í•˜ì„¸ìš”.
- ë‹¤ë¥¸ ì„¹ì…˜ ë‚´ìš©ì„ í•©ì³ì„œ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
- í˜•ì‹ ìœ ì§€:
  - "## " ì„¹ì…˜
  - "### " í•­ëª©
  - "-" bullet
  - Q/AëŠ” ì¡´ì¬í•  ë•Œë§Œ ìœ ì§€
- ì„¤ëª…/í•´ì„¤/JSON ê¸ˆì§€
"""


@router.post("/doc/{doc_id}/refine-text")
def refine_text(doc_id: str, req: RefineRequest):
    """AI helper: fill (ë§¥ë½ ë³´ê°•), refine (RAG ìµœì í™”), recommend (í‘œì¤€ í…œí”Œë¦¿ ì œì•ˆ)."""
    if not is_llm_available():
        raise HTTPException(status_code=503, detail="LLM ì‚¬ìš© ë¶ˆê°€")

    # Fetch original document raw_text for reference
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    conn.close()

    raw_text = ""
    if doc and doc["raw_text"]:
        raw_text = doc["raw_text"][:4000]

    raw_text_safe = raw_text or "(ì›ë³¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”.)"

    try:
        if req.task == "fill":
            allow_qa = _to_bool_allow_qa(req.allow_qa)
            qa_policy_text = (
                "ì›ë¬¸ ê·¼ê±°ê°€ ëª…í™•í•œ ê²½ìš°ì—ë§Œ Q&Aë¥¼ 1~3ê°œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹µì´ ë¶ˆëª…í™•í•˜ë©´ Q&Aë¥¼ ë§Œë“¤ì§€ ë§ê³  [í™•ì¸ í•„ìš”]ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”."
                if allow_qa else
                "Q&AëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ê¸°ì¡´ Q&Aë§Œ ìœ ì§€/ì •ë¦¬í•˜ì„¸ìš”."
            )
            prompt = FILL_SECTION_TEXT_PROMPT_V3.format(
                section_text=req.text,
                raw_text=raw_text_safe,
                qa_policy_text=qa_policy_text
            )
        elif req.task == "refine":
            prompt = FINALIZE_SECTION_TEXT_PROMPT_V1.format(
                section_text=req.text,
                raw_text=raw_text_safe
            )
        else:
            # recommend: use fill prompt with Q&A disabled
            prompt = FILL_SECTION_TEXT_PROMPT_V3.format(
                section_text=req.text,
                raw_text=raw_text_safe,
                qa_policy_text="Q&AëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ê¸°ì¡´ Q&Aë§Œ ìœ ì§€/ì •ë¦¬í•˜ì„¸ìš”."
            )

        suggestion = call_llm(prompt, temperature=0.3)
        return {"success": True, "suggestion": suggestion}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ STEP 2: APPROVE ============

@router.post("/doc/{doc_id}/approve")
def approve(doc_id: str):
    """Approve document if no RED issues open and no unresolved evidence failures."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT doc_id FROM documents WHERE doc_id = %s", (doc_id,))
    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    cursor.execute("SELECT COUNT(*) as cnt FROM qa_issues WHERE doc_id = %s AND severity = 'RED' AND status = 'OPEN'", (doc_id,))
    red_count = cursor.fetchone()["cnt"]

    if red_count > 0:
        conn.close()
        raise HTTPException(status_code=400, detail=f"Cannot approve: {red_count} RED issues open")

    # Check evidence span indexing failures (non-PII spans with char_start=-1)
    cursor.execute("SELECT section_name, evidence_json FROM manual_sections WHERE doc_id = %s AND evidence_json IS NOT NULL", (doc_id,))
    evidence_failures = []
    for row in cursor.fetchall():
        try:
            spans = json.loads(row["evidence_json"] or "[]")
            for span in spans:
                if span.get("char_start", 0) == -1 and not span.get("is_pii", False):
                    evidence_failures.append(row["section_name"])
                    break
        except (json.JSONDecodeError, TypeError):
            pass

    if evidence_failures:
        conn.close()
        sections_str = ", ".join(evidence_failures[:3])
        raise HTTPException(
            status_code=400,
            detail=f"Evidence span ë§¤ì¹­ ì‹¤íŒ¨ ì„¹ì…˜ {len(evidence_failures)}ê°œ ({sections_str}). Manualizeë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    cursor.execute("UPDATE documents SET status = 'APPROVED', updated_at = %s WHERE doc_id = %s",
                   (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()

    # Auto-reindex after approval
    reindex_result = reindex(doc_id)

    return {"success": True, "doc_id": doc_id, "status": "APPROVED", "reindex": reindex_result}


@router.post("/doc/{doc_id}/publish-all")
def publish_all(doc_id: str):
    """Index PASS sections only into RAG chunks. Update doc status to APPROVED.
    RAG optimize is already done in fill-all, so this just chunks and indexes."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT section_name, section_text, gate_status FROM manual_sections WHERE doc_id = %s", (doc_id,))
    all_sections = cursor.fetchall()
    if not all_sections:
        conn.close()
        raise HTTPException(status_code=400, detail="No sections found")

    pass_sections = [s for s in all_sections if s["gate_status"] == "PASS"]
    excluded_count = len(all_sections) - len(pass_sections)

    published_count = 0
    failed_count = 0
    failed_sections = []

    # Delete existing chunks for this doc (will re-insert PASS only)
    cursor.execute("DELETE FROM chunks WHERE doc_id = %s", (doc_id,))

    chunk_size = 500
    overlap = 100

    for sec in pass_sections:
        section_name = sec["section_name"]
        text = sec["section_text"]

        try:
            if not text or text == "ì •ë³´ ì—†ìŒ":
                continue

            start = 0
            chunk_index = 0
            while start < len(text):
                end = start + chunk_size
                chunk_text = text[start:end]

                if chunk_text.strip():
                    chunk_id = f"chunk_{uuid.uuid4().hex[:8]}"
                    # Generate embedding
                    embedding_blob = None
                    try:
                        emb = get_embedding(chunk_text)
                        embedding_blob = np.array(emb, dtype=np.float32).tobytes()
                    except Exception as emb_err:
                        print(f"[PUBLISH_ALL] Embedding failed for {chunk_id}: {emb_err}")
                    cursor.execute(
                        "INSERT INTO chunks (chunk_id, doc_id, section_name, chunk_index, chunk_text, embedding, created_at) VALUES (%s, %s, %s, %s, %s, %s, %s)",
                        (chunk_id, doc_id, section_name, chunk_index, chunk_text, embedding_blob, datetime.now().isoformat())
                    )
                    chunk_index += 1

                start = end - overlap
                if start >= len(text):
                    break

            published_count += 1
        except Exception as e:
            print(f"[PUBLISH_ALL] Indexing failed for {section_name}: {e}")
            failed_count += 1
            failed_sections.append({"section_name": section_name, "reason": str(e)})

    # Update doc status to APPROVED
    cursor.execute("UPDATE documents SET status = 'APPROVED', updated_at = %s WHERE doc_id = %s",
                   (datetime.now().isoformat(), doc_id))

    conn.commit()
    conn.close()

    # Build FAISS index for this apt_id
    embed_stats = {}
    try:
        conn2 = get_connection()
        c2 = conn2.cursor()
        c2.execute("SELECT apt_id FROM documents WHERE doc_id = %s", (doc_id,))
        doc_row = c2.fetchone()
        conn2.close()
        if doc_row:
            embed_stats = rag_build_index(doc_row["apt_id"])
    except Exception as idx_err:
        print(f"[PUBLISH_ALL] FAISS index build failed: {idx_err}")
        embed_stats = {"error": str(idx_err)}

    published_names = [s["section_name"] for s in pass_sections if s["section_name"] not in [f["section_name"] for f in failed_sections]]
    excluded_names = [s["section_name"] for s in all_sections if s["gate_status"] != "PASS"]
    return {
        "success": True,
        "published_count": published_count,
        "failed_count": failed_count,
        "excluded_count": excluded_count,
        "failed_sections": failed_sections,
        "published_names": published_names,
        "excluded_names": excluded_names,
        "embed_stats": embed_stats
    }


# ============ STEP 2: REINDEX ============

@router.post("/doc/{doc_id}/reindex")
def reindex(doc_id: str):
    """Chunk manual sections for RAG retrieval."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
    sections = cursor.fetchall()
    if not sections:
        conn.close()
        raise HTTPException(status_code=400, detail="No sections to index")
    
    # Delete existing chunks
    cursor.execute("DELETE FROM chunks WHERE doc_id = %s", (doc_id,))
    
    chunk_size = 500
    overlap = 100
    chunk_count = 0
    
    for section in sections:
        text = section["section_text"]
        section_name = section["section_name"]
        
        if not text or text == "ì •ë³´ ì—†ìŒ":
            continue
        
        # Simple chunking with overlap
        start = 0
        chunk_index = 0
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            
            if chunk_text.strip():
                chunk_id = f"chunk_{uuid.uuid4().hex[:8]}"
                # Generate embedding
                embedding_blob = None
                try:
                    emb = get_embedding(chunk_text)
                    embedding_blob = np.array(emb, dtype=np.float32).tobytes()
                except Exception as emb_err:
                    print(f"[REINDEX] Embedding failed for {chunk_id}: {emb_err}")
                cursor.execute(
                    "INSERT INTO chunks (chunk_id, doc_id, section_name, chunk_index, chunk_text, embedding, created_at) VALUES (%s, %s, %s, %s, %s, %s, %s)",
                    (chunk_id, doc_id, section_name, chunk_index, chunk_text, embedding_blob, datetime.now().isoformat())
                )
                chunk_count += 1
                chunk_index += 1

            start = end - overlap
            if start >= len(text):
                break

    conn.commit()
    conn.close()

    # Build FAISS index for this apt_id
    embed_stats = {}
    try:
        conn2 = get_connection()
        c2 = conn2.cursor()
        c2.execute("SELECT apt_id FROM documents WHERE doc_id = %s", (doc_id,))
        doc_row = c2.fetchone()
        conn2.close()
        if doc_row:
            embed_stats = rag_build_index(doc_row["apt_id"])
    except Exception as idx_err:
        print(f"[REINDEX] FAISS index build failed: {idx_err}")
        embed_stats = {"error": str(idx_err)}

    return {"success": True, "doc_id": doc_id, "chunk_count": chunk_count, "embed_stats": embed_stats}


# ============ STEP 2: GET SECTIONS ============

@router.get("/doc/{doc_id}/sections")
def get_sections(doc_id: str):
    """Get manual sections for a document (includes chunk-based fields)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("""SELECT section_name, section_text, gate_status, gate_reasons_json, gate_stale,
                             evidence_json, source_chunk_id, merge_status
                      FROM manual_sections WHERE doc_id = %s""", (doc_id,))
    sections = [dict(row) for row in cursor.fetchall()]
    # Include document-level completed_phases
    cursor.execute("SELECT completed_phases FROM documents WHERE doc_id = %s", (doc_id,))
    doc_row = cursor.fetchone()
    completed_phases = (doc_row["completed_phases"] or "") if doc_row else ""
    conn.close()
    return {"sections": sections, "completed_phases": completed_phases}


def _split_raw_by_headings(raw_text: str) -> list:
    """Split raw_text into chunks by heading patterns."""
    heading_pattern = re.compile(r'^(?:#{1,4}\s+.+|(?:\d+[\.\)]\s*).+|.+[:\uff1a]\s*)$', re.MULTILINE)
    matches = list(heading_pattern.finditer(raw_text))
    if not matches:
        return []
    chunks = []
    for i, m in enumerate(matches):
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(raw_text)
        heading = m.group().strip().lstrip("#").strip()
        body = raw_text[start:end].strip()
        chunks.append({"heading": heading, "body": body})
    return chunks


def _heading_match(section_name: str, headings: list) -> int:
    """Match section name to heading index by word overlap. Returns -1 if no match."""
    sec_words = set(re.findall(r'[\wê°€-í£]+', section_name.lower()))
    if not sec_words:
        return -1
    best_idx, best_score = -1, 0
    for i, h in enumerate(headings):
        h_words = set(re.findall(r'[\wê°€-í£]+', h["heading"].lower()))
        overlap = len(sec_words & h_words)
        score = overlap / max(len(sec_words | h_words), 1)
        if score > best_score and score >= 0.3:
            best_score = score
            best_idx = i
    return best_idx


def _body_fallback_match(section_text: str, raw_text: str, min_phrase_len: int = 6) -> str | None:
    """Fallback: find the best matching region in raw_text using keyword phrases from section_text.
    Returns the matched raw_text region or None."""
    if not section_text or not raw_text:
        return None

    # Extract meaningful phrases (lines stripped of markdown markers)
    sec_lines = [ln.lstrip("#>*- \t").strip() for ln in section_text.split("\n")]
    sec_lines = [ln for ln in sec_lines if len(ln) >= min_phrase_len]
    if not sec_lines:
        return None

    # Find matching lines in raw_text
    raw_lines = raw_text.split("\n")
    matched_raw_indices = set()
    for sec_ln in sec_lines:
        # Try exact substring match first
        sec_ln_clean = sec_ln.lstrip("#>*- \t").strip()
        if len(sec_ln_clean) < min_phrase_len:
            continue
        for ri, raw_ln in enumerate(raw_lines):
            raw_ln_clean = raw_ln.lstrip("#>*- \t").strip()
            # Check if significant portion overlaps
            if len(raw_ln_clean) < 4:
                continue
            # Extract keywords (3+ chars) from section line
            sec_words = set(re.findall(r'[\wê°€-í£]{3,}', sec_ln_clean.lower()))
            raw_words = set(re.findall(r'[\wê°€-í£]{3,}', raw_ln_clean.lower()))
            if not sec_words:
                continue
            overlap = len(sec_words & raw_words) / len(sec_words)
            if overlap >= 0.5:
                matched_raw_indices.add(ri)

    if not matched_raw_indices:
        return None

    # Build contiguous region: from first matched line to last matched line (with 1-line padding)
    first = max(0, min(matched_raw_indices) - 1)
    last = min(len(raw_lines) - 1, max(matched_raw_indices) + 1)
    region = "\n".join(raw_lines[first:last + 1]).strip()
    return region if region else None


@router.get("/doc/{doc_id}/source-map")
def get_source_map(doc_id: str):
    """Get per-section matched raw text.
    Chunk-based: uses doc_chunks JOIN for precise raw_chunk mapping.
    Legacy: falls back to heading matching."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = %s", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    cursor.execute("SELECT section_name, source_chunk_id, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
    sections = cursor.fetchall()
    conn.close()

    raw_text = doc["raw_text"] or ""

    # Check if any section has source_chunk_id (chunk-based document)
    has_chunks = any(row["source_chunk_id"] for row in sections)

    if has_chunks:
        # Chunk-based: JOIN doc_chunks for raw_chunk
        conn2 = get_connection()
        cursor2 = conn2.cursor()
        source_map = {}
        matched_count = 0
        for row in sections:
            name = row["section_name"]
            chunk_id = row["source_chunk_id"]
            if chunk_id:
                cursor2.execute("SELECT raw_chunk FROM doc_chunks WHERE chunk_id = %s", (chunk_id,))
                chunk_row = cursor2.fetchone()
                if chunk_row and chunk_row["raw_chunk"]:
                    source_map[name] = chunk_row["raw_chunk"]
                    matched_count += 1
                else:
                    source_map[name] = None
            else:
                source_map[name] = None
        conn2.close()
        # Body fallback for unmatched sections
        for row in sections:
            name = row["section_name"]
            if source_map.get(name) is None:
                fallback = _body_fallback_match(row["section_text"] or "", raw_text)
                if fallback:
                    source_map[name] = fallback
                    matched_count += 1
        return {"source_map": source_map, "raw_text": raw_text, "matched": matched_count}

    # Legacy: heading-based matching
    section_names = [row["section_name"] for row in sections]
    section_texts = {row["section_name"]: row["section_text"] or "" for row in sections}
    chunks = _split_raw_by_headings(raw_text)
    if not chunks:
        # No headings found â€” try body fallback for every section
        source_map = {}
        for name in section_names:
            source_map[name] = _body_fallback_match(section_texts[name], raw_text)
        return {"source_map": source_map, "raw_text": raw_text, "matched": sum(1 for v in source_map.values() if v is not None)}

    match_indices = []
    for sec_name in section_names:
        idx = _heading_match(sec_name, chunks)
        match_indices.append(idx)

    source_map = {}
    for i, sec_name in enumerate(section_names):
        idx = match_indices[i]
        if idx < 0:
            # Heading match failed â€” try body fallback
            source_map[sec_name] = _body_fallback_match(section_texts[sec_name], raw_text)
            continue
        next_chunk_idx = len(chunks)
        for j in range(i + 1, len(section_names)):
            if match_indices[j] > idx:
                next_chunk_idx = match_indices[j]
                break
        parts = [chunks[k]["body"] for k in range(idx, min(next_chunk_idx, len(chunks)))]
        source_map[sec_name] = "\n\n".join(parts)

    return {"source_map": source_map, "raw_text": raw_text, "matched": sum(1 for v in source_map.values() if v is not None)}


@router.get("/doc/{doc_id}/issues")
def get_issues(doc_id: str):
    """Get QA issues for a document."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT issue_id, severity, issue_type, message, suggestion, status FROM qa_issues WHERE doc_id = %s", (doc_id,))
    issues = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return issues


# ============ STEP 3: CHAT WITH RAG ============

class ChatRequest(BaseModel):
    apt_id: str
    client_id: str = "default"
    conversation_id: Optional[str] = None
    message: str
    model: Optional[str] = None


CHAT_PROMPT = """ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ ì»¤ë®¤ë‹ˆí‹° ê·œì •ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

JSON í˜•ì‹:
{{
  "reply_text": "ë‹µë³€ ë‚´ìš© (ê°„ê²°í•˜ê²Œ)",
  "citations": [
    {{"doc_id": "ë¬¸ì„œID", "doc_title": "ë¬¸ì„œëª…", "section_name": "ì„¹ì…˜ëª…", "snippet": "ì¸ìš© ë¶€ë¶„ 120ì ì´ë‚´"}}
  ],
  "confidence": "HIGH|MED|LOW",
  "next_question": null ë˜ëŠ” "ì¶”ê°€ ì§ˆë¬¸(ê·¼ê±° ë¶€ì¡±ì‹œ)",
  "actions": []
}}

ê·œì¹™:
- citationsì´ ì—†ìœ¼ë©´ confidenceëŠ” LOWë¡œ ì„¤ì •
- LOWì¼ ê²½ìš° next_questionì— í™•ì¸í•  ì§ˆë¬¸ 1ê°œ í¬í•¨
- ì˜ˆì•½ìƒì„±/ë¬¸ìë°œì†¡ ë“± ì‹¤í–‰ ì•¡ì…˜ì€ ê¸ˆì§€ (actionsëŠ” í•­ìƒ [])
- ê·¼ê±° ì—†ì´ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""


def keyword_search(query: str, chunks: list, top_k: int = 5) -> list:
    """Simple keyword-based search scoring."""
    keywords = set(query.lower().replace("?", "").replace(".", "").split())
    scored = []
    for chunk in chunks:
        text = chunk["chunk_text"].lower()
        score = sum(1 for kw in keywords if kw in text)
        if score > 0:
            scored.append((score, chunk))
    scored.sort(key=lambda x: -x[0])
    return [c for _, c in scored[:top_k]]


@router.post("/chat")
def chat(req: ChatRequest):
    """Chat with RAG retrieval and citations."""
    import time as _time
    t_start = _time.time()
    timing = {}

    conn = get_connection()
    cursor = conn.cursor()

    # Get or create conversation
    if req.conversation_id:
        conv_id = req.conversation_id
        cursor.execute("UPDATE conversations SET last_at = %s WHERE conversation_id = %s",
                       (datetime.now().isoformat(), conv_id))
    else:
        conv_id = f"conv_{uuid.uuid4().hex[:12]}"
        cursor.execute(
            "INSERT INTO conversations (conversation_id, apt_id, client_id, created_at, last_at) VALUES (%s, %s, %s, %s, %s)",
            (conv_id, req.apt_id, req.client_id, datetime.now().isoformat(), datetime.now().isoformat())
        )
    
    # Save user message
    user_msg_id = f"msg_{uuid.uuid4().hex[:8]}"
    cursor.execute(
        "INSERT INTO messages (msg_id, conversation_id, role, text, created_at) VALUES (%s, %s, 'user', %s, %s)",
        (user_msg_id, conv_id, req.message, datetime.now().isoformat())
    )
    
    # FAISS vector search with keyword fallback
    t_rag = _time.time()
    search_method = "faiss"
    top_chunks = []
    try:
        top_chunks = faiss_search(req.apt_id, req.message, top_k=5)
    except Exception as faiss_err:
        print(f"[CHAT] FAISS search failed, falling back to keyword: {faiss_err}")

    if not top_chunks:
        # Fallback: keyword search
        search_method = "keyword"
        cursor.execute("""
            SELECT c.chunk_id, c.doc_id, c.section_name, c.chunk_text, d.title as doc_title
            FROM chunks c
            JOIN documents d ON c.doc_id = d.doc_id
            WHERE d.apt_id = %s AND d.status = 'APPROVED'
        """, (req.apt_id,))
        all_chunks = [dict(row) for row in cursor.fetchall()]
        top_chunks = keyword_search(req.message, all_chunks, top_k=5)
    timing["rag_s"] = round(_time.time() - t_rag, 2)
    
    # Default response
    response = {
        "conversation_id": conv_id,
        "reply_text": "",
        "citations": [],
        "confidence": "LOW",
        "next_question": None,
        "actions": []
    }
    
    if not top_chunks:
        response["reply_text"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        response["next_question"] = "ì–´ë–¤ ë‚´ìš©ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
    else:
        # Build context
        context_parts = []
        for chunk in top_chunks:
            context_parts.append(f"[ë¬¸ì„œ: {chunk['doc_title']} / ì„¹ì…˜: {chunk['section_name']}]\n{chunk['chunk_text']}")
        context = "\n\n".join(context_parts)
        
        if is_llm_available():
            try:
                t_llm = _time.time()
                content = call_llm(CHAT_PROMPT.format(context=context, question=req.message), temperature=0.3, model=req.model)
                timing["llm_s"] = round(_time.time() - t_llm, 2)
                if content:
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        parsed = _clean_llm_json(json_match.group())
                        response["reply_text"] = parsed.get("reply_text", "")
                        response["citations"] = parsed.get("citations", [])
                        response["confidence"] = parsed.get("confidence", "MED")
                        response["next_question"] = parsed.get("next_question")
            except Exception as e:
                response["reply_text"] = f"LLM ì˜¤ë¥˜: {str(e)[:50]}"
        
        if not response["reply_text"]:
            # Mock response without LLM
            response["reply_text"] = f"ë¬¸ì„œì—ì„œ {len(top_chunks)}ê°œì˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            response["citations"] = [
                {
                    "doc_id": c["doc_id"],
                    "doc_title": c["doc_title"],
                    "section_name": c["section_name"],
                    "snippet": c["chunk_text"][:150] + "..." if len(c["chunk_text"]) > 150 else c["chunk_text"]
                } for c in top_chunks[:2]
            ]
            response["confidence"] = "MED" if top_chunks else "LOW"
    
    # Save assistant message
    asst_msg_id = f"msg_{uuid.uuid4().hex[:8]}"
    meta_json = json.dumps({
        "citations": response["citations"],
        "confidence": response["confidence"],
        "retrieval_count": len(top_chunks)
    }, ensure_ascii=False)
    cursor.execute(
        "INSERT INTO messages (msg_id, conversation_id, role, text, meta_json, created_at) VALUES (%s, %s, 'assistant', %s, %s, %s)",
        (asst_msg_id, conv_id, response["reply_text"], meta_json, datetime.now().isoformat())
    )
    
    conn.commit()
    conn.close()

    timing["total_s"] = round(_time.time() - t_start, 2)
    response["timing"] = timing
    response["search_method"] = search_method

    return response


# ============ STEP 3: IMPROVEMENTS GENERATOR ============

@router.post("/improvements/generate")
def generate_improvements(apt_id: str):
    """Generate improvement suggestions from chat logs."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get recent assistant messages with LOW confidence or empty citations
    cursor.execute("""
        SELECT m.msg_id, m.text, m.meta_json, m.created_at, c.apt_id
        FROM messages m
        JOIN conversations c ON m.conversation_id = c.conversation_id
        WHERE c.apt_id = %s AND m.role = 'assistant'
        ORDER BY m.created_at DESC
        LIMIT 50
    """, (apt_id,))
    messages = cursor.fetchall()
    
    suggestions = []
    seen_topics = set()
    
    for msg in messages:
        meta = json.loads(msg["meta_json"] or "{}")
        confidence = meta.get("confidence", "HIGH")
        citations = meta.get("citations", [])
        
        # Check for LOW confidence or empty citations
        if confidence == "LOW" or not citations:
            # Extract topic from message text
            text = msg["text"][:100]
            topic_key = text[:30]
            
            if topic_key not in seen_topics and len(suggestions) < 5:
                seen_topics.add(topic_key)
                
                # Determine target section
                target_section = "ì˜ˆì™¸/ë¬¸ì˜/ê¶Œí•œ"
                if "í™˜ë¶ˆ" in text or "ì •ì‚°" in text:
                    target_section = "í™˜ë¶ˆ/ìœ„ì•½/ì •ì‚°"
                elif "ì˜ˆì•½" in text or "ì·¨ì†Œ" in text:
                    target_section = "ì˜ˆì•½/ì·¨ì†Œ/ë³€ê²½"
                elif "ìš´ì˜" in text or "ì‹œê°„" in text:
                    target_section = "ìš´ì˜ì‹œê°„/íœ´ë¬´"
                
                suggestions.append({
                    "title": f"ì •ë³´ ë³´ì™„ í•„ìš”: {text[:30]}...",
                    "reason": f"confidence={confidence}, citations={len(citations)}ê°œ",
                    "target_section_name": target_section,
                    "proposed_patch": ""
                })
    
    # Get the latest APPROVED doc for this apt
    cursor.execute(
        "SELECT doc_id FROM documents WHERE apt_id = %s AND status = 'APPROVED' ORDER BY version DESC LIMIT 1",
        (apt_id,)
    )
    doc_row = cursor.fetchone()
    target_doc_id = doc_row["doc_id"] if doc_row else None
    
    # Save suggestions
    for sug in suggestions:
        sug_id = f"sug_{uuid.uuid4().hex[:8]}"
        cursor.execute("""
            INSERT INTO improve_suggestions (sug_id, apt_id, title, reason, proposed_patch, target_doc_id, target_section_name, status, created_at, updated_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, 'PENDING', %s, %s)
        """, (sug_id, apt_id, sug["title"], sug["reason"], sug["proposed_patch"], target_doc_id, sug["target_section_name"],
              datetime.now().isoformat(), datetime.now().isoformat()))
    
    conn.commit()
    conn.close()
    
    return {"success": True, "count": len(suggestions), "suggestions": suggestions}


# ============ STEP 3: ONE-CLICK PATCH APPLY ============

PATCH_PROMPT = """ë¬¸ì„œ ì„¹ì…˜ì— ì¶”ê°€í•  FAQ í•­ëª©ì„ ìƒì„±í•˜ì„¸ìš”.

ì œì•ˆ ì œëª©: {title}
ì œì•ˆ ì´ìœ : {reason}
ëŒ€ìƒ ì„¹ì…˜: {section_name}

í˜„ì¬ ì„¹ì…˜ ë‚´ìš©:
{section_text}

ê·œì¹™:
- ì—†ëŠ” ê·œì •ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "í™•ì¸ í•„ìš”" í˜•íƒœë¡œ ì‘ì„±
- ê°„ê²°í•œ Q&A í˜•ì‹ìœ¼ë¡œ ì‘ì„±

ì¶œë ¥ í˜•ì‹ (ì¶”ê°€í•  í…ìŠ¤íŠ¸ë§Œ):
---FAQ---
Q: ì§ˆë¬¸
A: ë‹µë³€
---"""


@router.post("/improvements/{sug_id}/apply")
def apply_improvement(sug_id: str):
    """Apply improvement suggestion with one-click patch."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get suggestion
    cursor.execute("SELECT * FROM improve_suggestions WHERE sug_id = %s", (sug_id,))
    sug = cursor.fetchone()
    if not sug:
        conn.close()
        raise HTTPException(status_code=404, detail="Suggestion not found")
    
    if sug["status"] == "APPLIED":
        conn.close()
        raise HTTPException(status_code=400, detail="Already applied")
    
    target_doc_id = sug["target_doc_id"]
    target_section = sug["target_section_name"]
    
    # Get current section text
    cursor.execute(
        "SELECT section_id, section_text FROM manual_sections WHERE doc_id = %s AND section_name = %s",
        (target_doc_id, target_section)
    )
    section_row = cursor.fetchone()
    
    if not section_row:
        conn.close()
        raise HTTPException(status_code=404, detail="Target section not found")
    
    current_text = section_row["section_text"]
    section_id = section_row["section_id"]
    
    # Generate patch
    patch_text = ""
    if is_llm_available():
        try:
            prompt = PATCH_PROMPT.format(
                title=sug["title"],
                reason=sug["reason"],
                section_name=target_section,
                section_text=current_text[:2000]
            )
            patch_text = call_llm(prompt, temperature=0.3)
        except Exception as e:
            print(f"[PATCH] Error: {e}")
            patch_text = f"\n\n---FAQ---\nQ: {sug['title']}\nA: í™•ì¸ í•„ìš” - ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
    
    if not patch_text:
        patch_text = f"\n\n---FAQ---\nQ: {sug['title']}\nA: í™•ì¸ í•„ìš” - ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
    
    # Append patch to section
    new_text = current_text + "\n" + patch_text
    cursor.execute(
        "UPDATE manual_sections SET section_text = %s WHERE section_id = %s",
        (new_text, section_id)
    )
    
    # Update suggestion status
    cursor.execute(
        "UPDATE improve_suggestions SET status = 'APPLIED', updated_at = %s WHERE sug_id = %s",
        (datetime.now().isoformat(), sug_id)
    )
    
    conn.commit()
    conn.close()
    
    # Reindex the document
    reindex_result = reindex(target_doc_id)
    
    return {"success": True, "sug_id": sug_id, "status": "APPLIED", "reindexed": reindex_result}


# ============ STEP 3: API SPEC EXTRACTOR ============

API_SPEC_PROMPT = """ì•„ë˜ ë§¤ë‰´ì–¼ ì„¹ì…˜ì—ì„œ ì‹œìŠ¤í…œ APIê°€ í•„ìš”í•œ ì˜ë„(intent)ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ë§¤ë‰´ì–¼ ë‚´ìš©:
{sections_text}

í’ˆì§ˆ ì´ìŠˆ (API_NEEDED):
{api_issues}

ê° intentì— ëŒ€í•´ API ìŠ¤í™ì„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
[
  {{
    "intent_name": "ì˜ˆì•½ ìƒì„±",
    "endpoint": "/api/booking/create",
    "method": "POST",
    "request_fields": ["member_id", "class_id", "date"],
    "response_fields": ["booking_id", "status"],
    "auth": "ì…ì£¼ë¯¼|ê´€ë¦¬ì|ì‹œìŠ¤í…œ",
    "notes": "ë¹„ê³ "
  }}
]

JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""


@router.post("/doc/{doc_id}/extract-api-spec")
def extract_api_spec(doc_id: str):
    """Extract API specifications from document."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get sections
    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = %s", (doc_id,))
    sections = cursor.fetchall()
    
    # Get API_NEEDED issues
    cursor.execute("SELECT message FROM qa_issues WHERE doc_id = %s AND issue_type = 'API_NEEDED'", (doc_id,))
    api_issues = [row["message"] for row in cursor.fetchall()]
    
    sections_text = "\n\n".join([f"[{s['section_name']}]\n{s['section_text']}" for s in sections])
    
    specs = []
    
    if is_llm_available():
        try:
            prompt = API_SPEC_PROMPT.format(
                sections_text=sections_text[:4000],
                api_issues=", ".join(api_issues)
            )
            content = call_llm(prompt, temperature=0.3)
            if content:
                json_match = re.search(r'\[[\s\S]*\]', content)
                if json_match:
                    specs = _clean_llm_json(json_match.group())
        except Exception as e:
            print(f"[API_SPEC] Error: {e}")
    
    # Fallback: generate from API_NEEDED issues
    if not specs:
        for issue in api_issues:
            intent = issue.replace("ì‹œìŠ¤í…œ ì—°ë™ í•„ìš”: ", "").replace("'", "")
            specs.append({
                "intent_name": intent,
                "endpoint": f"/api/{intent.replace(' ', '-').lower()}",
                "method": "POST",
                "request_fields": ["member_id"],
                "response_fields": ["status", "message"],
                "auth": "ê´€ë¦¬ì",
                "notes": "ìë™ ì¶”ì¶œë¨ - ê²€í†  í•„ìš”"
            })
    
    # Clear and save specs
    cursor.execute("DELETE FROM api_specs WHERE doc_id = %s", (doc_id,))
    for spec in specs:
        spec_id = f"spec_{uuid.uuid4().hex[:8]}"
        cursor.execute("""
            INSERT INTO api_specs (spec_id, doc_id, intent, endpoint, method, req_fields_json, resp_fields_json, auth, errors_json, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (spec_id, doc_id, spec.get("intent_name", ""), spec.get("endpoint", ""), spec.get("method", "POST"),
              json.dumps(spec.get("request_fields", []), ensure_ascii=False),
              json.dumps(spec.get("response_fields", []), ensure_ascii=False),
              spec.get("auth", ""), "[]", datetime.now().isoformat()))
    
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id, "spec_count": len(specs), "specs": specs}


@router.get("/doc/{doc_id}/api-spec/export")
def export_api_spec(doc_id: str, format: str = "json"):
    """Export API specs as JSON or YAML."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT intent, endpoint, method, req_fields_json, resp_fields_json, auth, errors_json
        FROM api_specs WHERE doc_id = %s
    """, (doc_id,))
    rows = cursor.fetchall()
    conn.close()
    
    specs = []
    for row in rows:
        specs.append({
            "intent": row["intent"],
            "endpoint": row["endpoint"],
            "method": row["method"],
            "request_fields": json.loads(row["req_fields_json"] or "[]"),
            "response_fields": json.loads(row["resp_fields_json"] or "[]"),
            "auth": row["auth"],
            "errors": json.loads(row["errors_json"] or "[]")
        })
    
    if format == "yaml":
        # Simple YAML conversion
        yaml_lines = ["api_specs:"]
        for spec in specs:
            yaml_lines.append(f"  - intent: {spec['intent']}")
            yaml_lines.append(f"    endpoint: {spec['endpoint']}")
            yaml_lines.append(f"    method: {spec['method']}")
            yaml_lines.append(f"    auth: {spec['auth']}")
            yaml_lines.append(f"    request_fields: {spec['request_fields']}")
            yaml_lines.append(f"    response_fields: {spec['response_fields']}")
        return {"format": "yaml", "content": "\n".join(yaml_lines)}
    
    return {"format": "json", "specs": specs}


# ============ STEP 3: GET SUGGESTIONS ============

@router.get("/improvements")
def list_improvements(apt_id: Optional[str] = None):
    """List improvement suggestions."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute("""
            SELECT s.*, a.name as apt_name
            FROM improve_suggestions s
            LEFT JOIN apartments a ON s.apt_id = a.apt_id
            WHERE s.apt_id = %s
            ORDER BY s.created_at DESC
        """, (apt_id,))
    else:
        cursor.execute("""
            SELECT s.*, a.name as apt_name
            FROM improve_suggestions s
            LEFT JOIN apartments a ON s.apt_id = a.apt_id
            ORDER BY s.created_at DESC
        """)
    
    suggestions = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return suggestions


@router.get("/conversations")
def list_conversations(apt_id: Optional[str] = None):
    """List conversations."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute("SELECT * FROM conversations WHERE apt_id = %s ORDER BY last_at DESC", (apt_id,))
    else:
        cursor.execute("SELECT * FROM conversations ORDER BY last_at DESC")
    
    convs = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return convs


@router.get("/conversations/{conversation_id}/messages")
def get_conversation_messages(conversation_id: str):
    """Get messages for a conversation."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM messages WHERE conversation_id = %s ORDER BY created_at", (conversation_id,))
    messages = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return messages


# ============ STEP 4-1: BRANCH CLASS SYNC ============

@router.post("/branch/{branch_id}/classes/sync")
def sync_branch_classes(
    branch_id: str,
    apt_id: str = Form(...),
    classes_json: str = Form(...),
    asof: Optional[str] = Form(None)
):
    """Sync branch classes from form-urlencoded data.
    
    Accepts application/x-www-form-urlencoded with:
    - apt_id: apartment ID (required)
    - classes_json: JSON string array of class objects (required)
    - asof: timestamp string (optional)
    """
    # Parse classes_json
    try:
        classes = json.loads(classes_json)
        if not isinstance(classes, list):
            raise ValueError("classes_json must be a JSON array")
    except json.JSONDecodeError as e:
        return {
            "success": False,
            "error": str(e),
            "preview": classes_json[:100] if classes_json else "",
            "hint": "classes_json must be valid JSON array"
        }
    except ValueError as e:
        return {
            "success": False,
            "error": str(e),
            "preview": classes_json[:100] if classes_json else "",
            "hint": "classes_json must be valid JSON array"
        }
    
    conn = get_connection()
    cursor = conn.cursor()
    
    now = datetime.now().isoformat()
    asof_value = asof or now
    upserted = 0
    
    for cls in classes:
        class_id = cls.get("class_id") or cls.get("id") or f"cls_{uuid.uuid4().hex[:8]}"
        name = cls.get("name", "")
        start = cls.get("start", "")
        end = cls.get("end", "")
        capacity = cls.get("capacity", 0)
        reserved = cls.get("reserved", 0)
        
        # Upsert using INSERT OR REPLACE
        cursor.execute("""
            REPLACE INTO branch_class_cache
            (apt_id, branch_id, class_id, name, start, end, capacity, reserved, asof, updated_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
        """, (apt_id, branch_id, class_id, name, start, end, capacity, reserved, asof_value, now))
        upserted += 1
    
    conn.commit()
    conn.close()
    
    return {"success": True, "upserted": upserted, "branch_id": branch_id, "apt_id": apt_id}


@router.get("/branch/{branch_id}/classes")
def get_branch_classes(branch_id: str, apt_id: str, date: Optional[str] = None):
    """Get cached classes for a branch."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if date:
        # Filter by date (classes where start contains the date string)
        cursor.execute("""
            SELECT * FROM branch_class_cache
            WHERE branch_id = %s AND apt_id = %s AND start LIKE %s
            ORDER BY start
        """, (branch_id, apt_id, f"{date}%"))
    else:
        cursor.execute("""
            SELECT * FROM branch_class_cache
            WHERE branch_id = %s AND apt_id = %s
            ORDER BY start
        """, (branch_id, apt_id))
    
    classes = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return {"branch_id": branch_id, "apt_id": apt_id, "classes": classes, "count": len(classes)}


@router.get("/classes")
def list_all_classes(apt_id: Optional[str] = None):
    """List all cached classes, optionally filtered by apt_id."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute("SELECT * FROM branch_class_cache WHERE apt_id = %s ORDER BY start", (apt_id,))
    else:
        cursor.execute("SELECT * FROM branch_class_cache ORDER BY start")
    
    classes = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return {"classes": classes, "count": len(classes)}
