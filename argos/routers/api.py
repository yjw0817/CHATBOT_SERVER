"""API routes for apartments, documents, and file upload."""
import hashlib
import json
import os
import re
import uuid
from datetime import datetime
from pathlib import Path
from typing import Optional

from fastapi import APIRouter, File, Form, UploadFile, HTTPException
from pydantic import BaseModel

from database import get_connection

router = APIRouter(prefix="/api", tags=["api"])

UPLOAD_DIR = Path(__file__).parent.parent / "uploads"
UPLOAD_DIR.mkdir(exist_ok=True)

# Import unified LLM client
from llm_client import call_llm, is_llm_available, get_llm_info, LLM_MODEL

@router.get("/debug/llm")
def debug_llm():
    """Debug endpoint to check LLM config."""
    import os
    return {
        "is_llm_available": is_llm_available(),
        "get_llm_info": get_llm_info(),
        "env_LLM_ENABLED": os.getenv("LLM_ENABLED"),
        "env_LLM_PROVIDER": os.getenv("LLM_PROVIDER"),
        "env_LLM_API_KEY_set": bool(os.getenv("LLM_API_KEY"))
    }


class ApartmentCreate(BaseModel):
    apt_id: str
    name: str


class ApartmentResponse(BaseModel):
    apt_id: str
    name: str
    created_at: str


# ============ APARTMENTS ============

@router.post("/apartments")
def create_apartment(apt_id: str = Form(...), name: str = Form(...)):
    """Create a new apartment (accepts form data)."""
    conn = get_connection()
    cursor = conn.cursor()
    try:
        cursor.execute(
            "INSERT INTO apartments (apt_id, name) VALUES (?, ?)",
            (apt_id, name)
        )
        conn.commit()
        return {"success": True, "apt_id": apt_id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))
    finally:
        conn.close()


@router.get("/apartments")
def list_apartments():
    """List all apartments."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT apt_id, name, created_at FROM apartments ORDER BY created_at DESC")
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]


@router.delete("/apartments/{apt_id}")
def delete_apartment(apt_id: str):
    """Delete apartment and all related data (cascading)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT apt_id FROM apartments WHERE apt_id = ?", (apt_id,))
    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Apartment not found")

    cursor.execute("SELECT doc_id FROM documents WHERE apt_id = ?", (apt_id,))
    doc_ids = [r["doc_id"] for r in cursor.fetchall()]
    cursor.execute("SELECT conversation_id FROM conversations WHERE apt_id = ?", (apt_id,))
    conv_ids = [r["conversation_id"] for r in cursor.fetchall()]
    deleted = {}
    if doc_ids:
        ph = ",".join("?" * len(doc_ids))
        for tbl in ["manual_section_revisions", "manual_sections", "qa_issues", "chunks", "api_specs"]:
            cursor.execute(f"DELETE FROM {tbl} WHERE doc_id IN ({ph})", doc_ids)
            deleted[tbl] = cursor.rowcount
    if conv_ids:
        ph2 = ",".join("?" * len(conv_ids))
        cursor.execute(f"DELETE FROM messages WHERE conversation_id IN ({ph2})", conv_ids)
        deleted["messages"] = cursor.rowcount
    for tbl in ["conversations", "improve_suggestions", "branch_class_cache", "documents"]:
        cursor.execute(f"DELETE FROM {tbl} WHERE apt_id = ?", (apt_id,))
        deleted[tbl] = cursor.rowcount
    cursor.execute("DELETE FROM apartments WHERE apt_id = ?", (apt_id,))
    deleted["apartments"] = cursor.rowcount
    conn.commit()
    conn.close()
    return {"success": True, "apt_id": apt_id, "deleted": deleted}


@router.delete("/doc/{doc_id}")
def delete_document(doc_id: str):
    """Delete a document and all related data."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT doc_id FROM documents WHERE doc_id = ?", (doc_id,))
    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    deleted = {}
    for tbl in ["manual_section_revisions", "manual_sections", "qa_issues", "chunks", "api_specs"]:
        cursor.execute(f"DELETE FROM {tbl} WHERE doc_id = ?", (doc_id,))
        deleted[tbl] = cursor.rowcount
    cursor.execute("DELETE FROM documents WHERE doc_id = ?", (doc_id,))
    deleted["documents"] = cursor.rowcount
    conn.commit()
    conn.close()
    return {"success": True, "doc_id": doc_id, "deleted": deleted}


# ============ UPLOAD ============

@router.post("/upload")
async def upload_document(
    apt_id: str = Form(...),
    file: UploadFile = File(...)
):
    """Upload a document file."""
    content = await file.read()
    content_hash = hashlib.sha256(content).hexdigest()
    
    filename = file.filename or "unknown"
    ext = filename.rsplit(".", 1)[-1].lower() if "." in filename else "txt"
    source_type = ext if ext in ("docx", "pdf", "txt", "md") else "txt"
    
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute(
        "SELECT doc_id, version FROM documents WHERE apt_id = ? AND content_hash = ? AND status != 'ARCHIVED' ORDER BY version DESC LIMIT 1",
        (apt_id, content_hash)
    )
    existing = cursor.fetchone()
    
    if existing:
        new_version = existing["version"] + 1
        cursor.execute(
            "UPDATE documents SET status = 'ARCHIVED', updated_at = ? WHERE apt_id = ? AND content_hash = ? AND status != 'ARCHIVED'",
            (datetime.now().isoformat(), apt_id, content_hash)
        )
    else:
        cursor.execute("SELECT MAX(version) as max_ver FROM documents WHERE apt_id = ?", (apt_id,))
        row = cursor.fetchone()
        new_version = (row["max_ver"] or 0) + 1
    
    doc_id = f"doc_{uuid.uuid4().hex[:12]}"
    file_path = UPLOAD_DIR / f"{doc_id}.{source_type}"
    with open(file_path, "wb") as f:
        f.write(content)
    
    raw_text = f"[Placeholder: extract text first]"
    
    now = datetime.now().isoformat()
    cursor.execute("""
        INSERT INTO documents (doc_id, apt_id, title, source_filename, source_type, content_hash, raw_text, version, status, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'DRAFT', ?, ?)
    """, (doc_id, apt_id, filename, filename, source_type, content_hash, raw_text, new_version, now, now))
    
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id, "version": new_version, "content_hash": content_hash, "status": "DRAFT"}


@router.get("/docs")
def list_documents(apt_id: Optional[str] = None):
    """List documents, optionally filtered by apartment."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute(
            "SELECT doc_id, apt_id, title, source_filename, source_type, version, status, created_at FROM documents WHERE apt_id = ? ORDER BY created_at DESC",
            (apt_id,)
        )
    else:
        cursor.execute(
            "SELECT doc_id, apt_id, title, source_filename, source_type, version, status, created_at FROM documents ORDER BY created_at DESC"
        )
    
    rows = cursor.fetchall()
    conn.close()
    return [dict(row) for row in rows]


# ============ STEP 2: EXTRACT TEXT ============

@router.post("/doc/{doc_id}/extract-text")
def extract_text(doc_id: str):
    """Extract text from uploaded document (DOCX supported)."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT source_filename, source_type FROM documents WHERE doc_id = ?", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    
    source_type = doc["source_type"]
    file_path = UPLOAD_DIR / f"{doc_id}.{source_type}"
    
    if not file_path.exists():
        conn.close()
        raise HTTPException(status_code=404, detail="File not found on disk")
    
    raw_text = ""
    
    if source_type == "docx":
        try:
            from docx import Document
            docx_doc = Document(str(file_path))
            paragraphs = [p.text for p in docx_doc.paragraphs if p.text.strip()]
            raw_text = "\n".join(paragraphs)
        except Exception as e:
            conn.close()
            raise HTTPException(status_code=500, detail=f"DOCX extraction failed: {str(e)}")
    elif source_type in ("txt", "md"):
        raw_text = file_path.read_text(encoding="utf-8", errors="ignore")
    else:
        raw_text = f"[PDF extraction not implemented - file: {doc['source_filename']}]"
    
    cursor.execute("UPDATE documents SET raw_text = ?, updated_at = ? WHERE doc_id = ?",
                   (raw_text, datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()
    
    return {
        "success": True,
        "doc_id": doc_id,
        "chars": len(raw_text),
        "preview": raw_text[:300] if raw_text else ""
    }


# ============ STEP 2: MANUALIZE ============

MANUALIZE_PROMPT = """ë‹¹ì‹ ì€ ì˜ì—…/ìš´ì˜ ë¬¸ì„œë¥¼ **RAG(ê²€ìƒ‰ ê¸°ë°˜ ë‹µë³€)**ì— ë„£ê¸° ì í•©í•œ **êµ¬ì¡°í™” ë§¤ë‰´ì–¼ ë°ì´í„°**ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ëª©í‘œ]
- ì›ë¬¸(raw_text)ì˜ **ì‚¬ì‹¤/êµ¬ì¡°(í—¤ë”©/ë²ˆí˜¸/ëª©ì°¨/êµ¬ë¶„)**ë¥¼ **ìˆëŠ” ê·¸ëŒ€ë¡œ ë³´ì¡´**í•˜ë©´ì„œ, ê²€ìƒ‰/ì¸ìš©/ê²€ì¦/ì—…ë°ì´íŠ¸ê°€ ì‰¬ìš´ **ì •ì œëœ ë§¤ë‰´ì–¼ JSON**ì„ ë§Œë“­ë‹ˆë‹¤.
- ì›ë¬¸ì— ì—†ëŠ” ì •ë³´ë¥¼ **ì¶”ê°€/ì¶”ì¸¡/ì°½ì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
- ê°œì¸ì •ë³´(PII)ëŠ” **íƒì§€ + ë§ˆìŠ¤í‚¹**í•˜ì—¬ RAGì— ì•ˆì „í•˜ê²Œ ì €ì¥ ê°€ëŠ¥í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

[ì…ë ¥]
raw_text: {raw_text}

[ì¶œë ¥ í˜•ì‹]
- ì•„ë˜ ìŠ¤í‚¤ë§ˆë¥¼ ë§Œì¡±í•˜ëŠ” **RFC8259 ìœ íš¨ JSON**ë§Œ ë°˜í™˜í•˜ì‹­ì‹œì˜¤.
- ì½”ë“œë¸”ë¡(```), ì£¼ì„, ì„¤ëª… ë¬¸ì¥ ê¸ˆì§€. **ì˜¤ì§ JSON í…ìŠ¤íŠ¸ë§Œ** ì¶œë ¥í•©ë‹ˆë‹¤.
- ëª¨ë“  ë¬¸ìì—´ì€ í°ë”°ì˜´í‘œ(")ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. trailing comma ê¸ˆì§€.
- ì•„ë˜ ìŠ¤í‚¤ë§ˆì˜ ëª¨ë“  í•„ë“œë¥¼ **ë°˜ë“œì‹œ í¬í•¨**í•˜ì„¸ìš”. (í•´ë‹¹ ì—†ìŒì´ë©´ ë¹ˆ ë¬¸ìì—´ "" ë˜ëŠ” ë¹ˆ ë°°ì—´ [] ì‚¬ìš©)

{{
  "doc_title": "ë¬¸ì„œ ì œëª©(ì›ë¬¸ì—ì„œ ì¶”ì¶œ, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
  "doc_type": "POLICY|PROCESS|FAQ|NOTICE|MIXED",
  "summary": "ë¬¸ì„œ í•µì‹¬ 2~4ë¬¸ì¥ ìš”ì•½(ì¶”ì¸¡ ê¸ˆì§€, ì¤„ë°”ê¿ˆ ì—†ì´)",
  "sections": [
    {{
      "section_id": "stable_slug_like_this",
      "name": "ì„¹ì…˜ ì´ë¦„(ì›ë¬¸ í—¤ë”©/ë²ˆí˜¸ ì œëª© ê·¸ëŒ€ë¡œ)",
      "tags": ["í‚¤ì›Œë“œ", "ì—…ë¬´ì˜ì—­", "ëŒ€ìƒ", "ì•„íŒŒíŠ¸ëª…/ì§€ì ëª…(ìˆìœ¼ë©´)"],
      "content": [
        {{
          "rule_id": "S1-R1",
          "title": "í•­ëª© ì œëª©(ì§§ê²Œ, ì›ë¬¸ ì†Œì œëª©/ë¬¸ë‹¨ ì£¼ì œ ê¸°ë°˜)",
          "bullets": [
            "ì›ë¬¸ì—ì„œ í™•ì¸ë˜ëŠ” ê·œì¹™/ì •ì˜/ì ˆì°¨/ì•ˆë‚´ë¥¼ ì§§ì€ bulletë¡œ ì •ë¦¬(ì¶”ì¸¡ ê¸ˆì§€)"
          ],
          "structured": {{
            "target": "ëŒ€ìƒ(ì›ë¬¸ì— ëª…ì‹œëœ ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
            "condition": "ì ìš© ì¡°ê±´(ì›ë¬¸ì— ëª…ì‹œëœ ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
            "procedure": [],
            "exceptions": [],
            "owner": "ë‹´ë‹¹/ì£¼ì²´(ì›ë¬¸ì— ëª…ì‹œëœ ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
            "channel": "ë¬¸ì˜/ì ‘ìˆ˜ ì±„ë„(ì›ë¬¸ì— ëª…ì‹œëœ ê²½ìš°ë§Œ, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)"
          }},
          "source_quotes": [],
          "issues": []
        }}
      ]
    }}
  ],
  "clarification_questions": [],
  "pii_handling": {{
    "pii_found": false,
    "pii_types": [],
    "masking_policy": []
  }},
  "change_summary": "ì´ë²ˆ ë³€í™˜ì—ì„œ ìˆ˜í–‰í•œ ì‘ì—… ìš”ì•½(ì‚¬ì‹¤ ì¶”ê°€ ê¸ˆì§€)"
}}

[ë¬¸ì„œ íƒ€ì…(doc_type) íŒì • ê·œì¹™]
- POLICY: í•´ì•¼/ê¸ˆì§€/ì¡°ê±´/ê¸°ì¤€/ê·œì • ì¤‘ì‹¬
- PROCESS: ë‹¨ê³„/ì ˆì°¨/íë¦„(ì—…ë¬´ í”„ë¡œì„¸ìŠ¤) ì¤‘ì‹¬
- FAQ: ì§ˆë¬¸-ë‹µë³€(Q/A) ë‹¤ìˆ˜
- NOTICE: ê³µì§€/ì•ˆë‚´/ì†Œê°œ/ë³€ê²½ì‚¬í•­ ì¤‘ì‹¬
- MIXED: ìœ„ ì„±ê²©ì´ í˜¼í•©ë˜ì–´ ì§€ë°°ì ì¸ í•˜ë‚˜ë¡œ ë‹¨ì •í•˜ê¸° ì–´ë ¤ì›€

[ì„¹ì…˜ êµ¬ì„± ê·œì¹™ - ìµœìš°ì„ (ì¤‘ìš”)]
1) `sections`ëŠ” **ì›ë¬¸ì— ì¡´ì¬í•˜ëŠ” í—¤ë”©/ë²ˆí˜¸/ëª©ì°¨/êµ¬ë¶„ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ** ë§¤í•‘í•©ë‹ˆë‹¤.
2) ì„ì˜ë¡œ ì„¹ì…˜ì„ **ëŠ˜ë¦¬ê±°ë‚˜/ì¤„ì´ê±°ë‚˜/ë³‘í•©í•˜ê±°ë‚˜/ë¶„ë¦¬í•˜ì§€ ë§ˆì„¸ìš”.**
3) ì›ë¬¸ì— ì„¹ì…˜ êµ¬ë¶„ì´ ì „í˜€ ì—†ë‹¤ë©´ `sections`ëŠ” **1ê°œë§Œ** ë§Œë“¤ê³ :
   - section_id="general"
   - name="ì¼ë°˜"
4) `name`ì€ ì›ë¬¸ ì„¹ì…˜ ì œëª©ì„ ê°€ëŠ¥í•œ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤(ë²ˆí˜¸ í¬í•¨ ê°€ëŠ¥).
5) `section_id`ëŠ” `name` ê¸°ë°˜ ì•ˆì • slug(ì˜ë¬¸ ì†Œë¬¸ì + í•˜ì´í”ˆ). ì„¹ì…˜ëª…ì´ ì—†ìœ¼ë©´ "general".
6) `rule_id`ëŠ” ì„¹ì…˜ ìˆœì„œ/í•­ëª© ìˆœì„œ ê¸°ì¤€ìœ¼ë¡œ ê³ ì •:
   - ì²« ì„¹ì…˜ì˜ ì²« í•­ëª©: "S1-R1"
   - ì²« ì„¹ì…˜ì˜ ë‘˜ì§¸ í•­ëª©: "S1-R2"
   - ë‘˜ì§¸ ì„¹ì…˜ì˜ ì²« í•­ëª©: "S2-R1" ... ë°©ì‹

[í•­ëª©(rule) ì¶”ì¶œ ê·œì¹™]
- ê° ì„¹ì…˜ ì•ˆì—ì„œ ì›ë¬¸ì— ì¡´ì¬í•˜ëŠ” ì†Œì œëª©(###), ë²ˆí˜¸ ëª©ë¡, ë¶ˆë¦¿ ëª©ë¡, ë¬¸ë‹¨ ì£¼ì œ ë‹¨ìœ„ë¡œ ruleì„ êµ¬ì„±í•©ë‹ˆë‹¤.
- ì›ë¬¸ì— ë¶ˆë¦¿ ëª©ë¡ì´ ìˆìœ¼ë©´ bulletsì— **í•­ëª©ì„ ê·¸ëŒ€ë¡œ(ì˜ë¯¸ ë³´ì¡´)** ë‚˜ì—´í•©ë‹ˆë‹¤. (ë¬¸ì¥ ë‹¤ë“¬ê¸° ìµœì†Œí™”)
- ì›ë¬¸ì— "ë²„ì „/ì›¹ì‚¬ì´íŠ¸/ì—°ë½ì²˜" ê°™ì€ ë©”íƒ€ ì •ë³´ê°€ ìˆìœ¼ë©´ ê°ê° ë³„ë„ ruleë¡œ ë¶„ë¦¬(ë‹¨, ì›ë¬¸ êµ¬ì¡°ê°€ ì´ë¯¸ ë¶„ë¦¬ë¼ ìˆì§€ ì•Šë‹¤ë©´ ë¬¸ë‹¨ ë‹¨ìœ„ì—ì„œë§Œ ë¶„ë¦¬).
- ì„œë¡œ ë‹¤ë¥¸ ìœ„ì¹˜ì— í©ì–´ì§„ ë™ì¼ ì£¼ì œ ì •ë³´ë¼ë„ **ì›ë¬¸ì´ ë™ì¼ ì„¹ì…˜ ì•ˆ**ì— ìˆê³  ëª…í™•íˆ ê°™ì€ ì£¼ì œì¼ ë•Œë§Œ, í•œ ruleë¡œ í•©ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  (ë‹¤ë¥¸ ì„¹ì…˜ì˜ ë‚´ìš©ì„ ëŒì–´ì™€ í•©ì¹˜ì§€ ë§ˆì„¸ìš”)

[ì •ì œ ê·œì¹™]
- ì›ë¬¸ í‘œí˜„ì„ ê³¼ë„í•˜ê²Œ ë¯¸í™”/í™•ì¥í•˜ì§€ ë§ê³ , **ì§§ê³  ëª…í™•í•œ ì‚¬ì‹¤/ê·œì¹™ í˜•íƒœ**ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤.
- ì›ë¬¸ì— ì—†ëŠ” "ëŒ€ìƒ/ë‹´ë‹¹/ì±„ë„/ì ˆì°¨"ë¥¼ **ì¼ë°˜ ìƒì‹ìœ¼ë¡œ ì±„ìš°ì§€ ë§ˆì„¸ìš”.**
- `structured`ëŠ” ì›ë¬¸ì— ëª…ì‹œëœ ê²ƒë§Œ ì±„ìš°ê³ , ì—†ìœ¼ë©´:
  - ë¬¸ìì—´ í•„ë“œ: ""
  - ë°°ì—´ í•„ë“œ(procedure/exceptions): []

[source_quotes ê·œì¹™(ê·¼ê±° ì¸ìš©)]
- ê° ruleë§ˆë‹¤ ì›ë¬¸ ê·¼ê±°ë¥¼ 0~2ê°œê¹Œì§€ `source_quotes`ì— ë„£ìŠµë‹ˆë‹¤.
- ê° quoteëŠ” ì›ë¬¸ì—ì„œ **ì—°ì†ëœ ë¬¸ìì—´**ì„ ê·¸ëŒ€ë¡œ ê°€ì ¸ì˜¤ë˜, ê¸¸ì´ëŠ”:
  - ì˜ì–´: ìµœëŒ€ 25ë‹¨ì–´
  - í•œêµ­ì–´: 50ì ë‚´ì™¸(ê°€ëŠ¥í•˜ë©´ 20~70ì ë²”ìœ„)
- ê·¼ê±°ê°€ ëª…í™•í•˜ì§€ ì•Šìœ¼ë©´ `source_quotes`ëŠ” ë¹ˆ ë°°ì—´ []ë¡œ ë‘¡ë‹ˆë‹¤.
- PIIê°€ í¬í•¨ëœ quoteëŠ” ì•„ë˜ ê·œì¹™ëŒ€ë¡œ ë§ˆìŠ¤í‚¹ í›„ ì¸ìš©í•©ë‹ˆë‹¤.

[ì´ìŠˆ íƒì§€ ê·œì¹™]
- issuesëŠ” ë¬¸ì œê°€ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ [] ì…ë‹ˆë‹¤.
- MISSING: í•„ìˆ˜ ì •ë³´ ëˆ„ë½(ê¸°í•œ/ê¸ˆì•¡/ë‹´ë‹¹/ì±„ë„/ì¡°ê±´ ë“±)
- AMBIGUOUS: í•´ì„ì´ ê°ˆë¦¬ëŠ” í‘œí˜„("ì ë‹¹íˆ", "ë¹ ë¥´ê²Œ", "ê°€ëŠ¥í•˜ë©´" ë“±) ë˜ëŠ” ì •ì˜ ë¶ˆëª…
- CONFLICT: ë¬¸ì„œ ë‚´ ìƒì¶© ê·œì¹™/ì˜ˆì™¸ ì¶©ëŒ
- PII_RISK: ê°œì¸ì •ë³´/ì‹ë³„ì •ë³´ í¬í•¨ ë˜ëŠ” í¬í•¨ ê°€ëŠ¥ì„± ë†’ìŒ(ë§ˆìŠ¤í‚¹ í•„ìš” í¬í•¨)
- API_NEEDED: ìë™í™”/ì¡°íšŒê°€ í•„ìš”í•˜ì§€ë§Œ API/ê¶Œí•œ/ë°ì´í„°ê°€ ëª…ì‹œë˜ì§€ ì•ŠìŒ

[PII ì²˜ë¦¬ ê·œì¹™(ì¤‘ìš”)]
- ì›ë¬¸ì— PIIê°€ ìˆìœ¼ë©´:
  1) bullets ë° source_quotesì—ëŠ” **ë§ˆìŠ¤í‚¹ëœ í˜•íƒœë¡œë§Œ** ë‚¨ê¹ë‹ˆë‹¤.
  2) í•´ë‹¹ ruleì˜ issuesì— PII_RISKë¥¼ ê¸°ë¡í•˜ê³  severityëŠ” MEDIUM ì´ìƒ.
  3) pii_handling.pii_found=true
  4) pii_typesì— íƒì§€ëœ ìœ í˜•ì„ ì¶”ê°€(ì˜ˆ: ["PHONE","EMAIL"])
  5) masking_policyì—ëŠ” ì´ë²ˆ ë¬¸ì„œì—ì„œ ì‹¤ì œ ì ìš©í•œ ì •ì±…ë§Œ ê¸°ë¡(ì˜ˆ: ["PHONE: 010-****-1234","EMAIL: ab***@domain.com"])
- ì›ë¬¸ì— PIIê°€ ì—†ìœ¼ë©´:
  - pii_handling.pii_found=false, pii_types=[], masking_policy=[]

[ë§ˆìŠ¤í‚¹ í¬ë§·]
- ì „í™”ë²ˆí˜¸: 010-****-1234
- ì´ë©”ì¼: ab***@domain.com
- ì£¼ì†Œ: ì‹œ/êµ¬ê¹Œì§€ë§Œ ë‚¨ê¸°ê³  ìƒì„¸ëŠ” ***
- ê³„ì¢Œ/ì¹´ë“œ/ì‹ë³„ë²ˆí˜¸: ë’¤ 4~7ìë¦¬ **** ì²˜ë¦¬
- ì‹¤ëª…: í•„ìš” ì‹œ ì¼ë¶€ë§Œ ë‚¨ê¸°ê³  *** ì²˜ë¦¬

[clarification_questions ê·œì¹™]
- í™•ì¸ì´ í•„ìš”í•œ ì§ˆë¬¸ë§Œ 0~10ê°œ.
- ì›ë¬¸ì— ì—†ëŠ” í•µì‹¬ê°’(ê¸ˆì•¡/ê¸°í•œ/ë‹´ë‹¹/ì±„ë„/ì˜ˆì™¸/ì •ì˜)ì´ ì‹¤ì œ ìš´ì˜ íŒë‹¨ì— í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‘ì„±.
- ì‚¬ì†Œí•œ ë¬¸ì¥ ë‹¤ë“¬ê¸° ì§ˆë¬¸ì€ ê¸ˆì§€.

[ê¸ˆì§€]
- ì›ë¬¸ì— ì—†ëŠ” ì •ë³´ ìƒì„±/ì¶”ì¸¡ ê¸ˆì§€
- ì›ë¬¸ ì‚¬ì‹¤ê³¼ ë‹¤ë¥¸ ë‚´ìš© ìƒì„± ê¸ˆì§€
- ì¶”ì¸¡/ì¼ë°˜ ìƒì‹ìœ¼ë¡œ ë¹ˆì¹¸ ì±„ìš°ê¸° ê¸ˆì§€
- JSON ì™¸ í…ìŠ¤íŠ¸ ì¶œë ¥ ê¸ˆì§€"""


@router.post("/doc/{doc_id}/manualize")
def manualize(doc_id: str, force: bool = False):
    """Convert raw text to structured manual sections using V2 RAG-optimized prompt."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = ?", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    raw_text = doc["raw_text"]
    if not raw_text or raw_text.strip() == "" or raw_text.startswith("["):
        conn.close()
        error_msg = "ì¶”ì¶œëœ í…ìŠ¤íŠ¸ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € 'Extract'ë¥¼ ìˆ˜í–‰í•´ ì£¼ì„¸ìš”."
        if raw_text and raw_text.startswith("["):
            error_msg = f"í…ìŠ¤íŠ¸ ì¶”ì¶œì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤: {raw_text}"
        raise HTTPException(status_code=400, detail=error_msg)

    # Check if manual sections already exist (return cached if not forced)
    if not force:
        cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = ?", (doc_id,))
        existing = cursor.fetchall()
        if existing:
            sections = {row["section_name"]: row["section_text"] for row in existing}
            conn.close()
            return {
                "success": True,
                "doc_id": doc_id,
                "sections": list(sections.keys()),
                "section_details": sections,
                "llm_used": False,
                "cached": True,
                "todo_questions": []
            }

    # LLM is required
    if not is_llm_available():
        conn.close()
        raise HTTPException(status_code=503, detail="LLMì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")

    result_data = {}
    try:
        content = call_llm(MANUALIZE_PROMPT.format(raw_text=raw_text[:8000]), temperature=0.3)
        if content:
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                result_data = json.loads(json_match.group())
        if not result_data.get("sections"):
            conn.close()
            raise HTTPException(status_code=502, detail="LLM ì‘ë‹µì„ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except HTTPException:
        raise
    except Exception as e:
        conn.close()
        raise HTTPException(status_code=502, detail=f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}")

    sections_list = result_data.get("sections", [])
    todo_questions = result_data.get("clarification_questions", [])

    sections_map = {}
    all_issues = []

    if sections_list:
        for s in sections_list:
            name = s.get("name", "ë¯¸ë¶„ë¥˜")
            content_items = s.get("content", "")
            
            # V2: content is array of rule objects â†’ flatten to readable text
            if isinstance(content_items, list):
                text_parts = []
                for rule in content_items:
                    title = rule.get("title", "")
                    if title:
                        text_parts.append(f"### {title}")
                    
                    # Bullets
                    for bullet in rule.get("bullets", []):
                        text_parts.append(f"- {bullet}")
                    
                    # Structured info
                    structured = rule.get("structured", {})
                    if structured:
                        details = []
                        if structured.get("target"):
                            details.append(f"- ëŒ€ìƒ: {structured['target']}")
                        if structured.get("condition"):
                            details.append(f"- ì¡°ê±´: {structured['condition']}")
                        if structured.get("procedure"):
                            for i, step in enumerate(structured["procedure"], 1):
                                details.append(f"  {i}. {step}")
                        if structured.get("exceptions"):
                            for exc in structured["exceptions"]:
                                details.append(f"- âš ï¸ ì˜ˆì™¸: {exc}")
                        if structured.get("owner"):
                            details.append(f"- ë‹´ë‹¹: {structured['owner']}")
                        if structured.get("channel"):
                            details.append(f"- ì±„ë„: {structured['channel']}")
                        if details:
                            text_parts.extend(details)
                    
                    # Source quotes
                    quotes = rule.get("source_quotes", [])
                    if quotes:
                        text_parts.append(f"  ğŸ“Œ ê·¼ê±°: {'; '.join(quotes)}")
                    
                    text_parts.append("")  # blank line between rules
                    
                    # Collect issues from each rule
                    for issue in rule.get("issues", []):
                        severity_map = {"HIGH": "RED", "MEDIUM": "YELLOW", "LOW": "YELLOW"}
                        all_issues.append({
                            "severity": severity_map.get(issue.get("severity", "MEDIUM"), "YELLOW"),
                            "issue_type": issue.get("type"),
                            "message": f"[{name}] {issue.get('message', '')}",
                            "suggestion": issue.get("suggestion", "")
                        })
                
                sections_map[name] = "\n".join(text_parts).strip()
            else:
                # V1 fallback: content is plain string
                sections_map[name] = content_items if content_items else "ì •ë³´ ì—†ìŒ"

                # V1 issues at section level
                for issue in s.get("issues", []):
                    all_issues.append({
                        "severity": "RED" if issue.get("type") in ("MISSING", "CONFLICT", "PII_RISK") else "YELLOW",
                        "issue_type": issue.get("type"),
                        "message": f"[{name}] {issue.get('message')}",
                        "suggestion": issue.get("suggestion")
                    })
    
    # Save sections
    cursor.execute("DELETE FROM manual_sections WHERE doc_id = ?", (doc_id,))
    for section_name, section_text in sections_map.items():
        section_id = f"sec_{uuid.uuid4().hex[:8]}"
        cursor.execute(
            "INSERT INTO manual_sections (section_id, doc_id, section_name, section_text) VALUES (?, ?, ?, ?)",
            (section_id, doc_id, section_name, section_text if section_text else "ì •ë³´ ì—†ìŒ")
        )
    
    # Save issues found during manualization
    cursor.execute("DELETE FROM qa_issues WHERE doc_id = ?", (doc_id,))
    for issue in all_issues:
        issue_id = f"issue_{uuid.uuid4().hex[:8]}"
        cursor.execute(
            "INSERT INTO qa_issues (issue_id, doc_id, severity, issue_type, message, suggestion, status) VALUES (?, ?, ?, ?, ?, ?, 'OPEN')",
            (issue_id, doc_id, issue["severity"], issue["issue_type"], issue["message"], issue["suggestion"])
        )
    
    cursor.execute("UPDATE documents SET updated_at = ? WHERE doc_id = ?", (datetime.now().isoformat(), doc_id))
    conn.commit()

    # Auto Gate#1: run gate check on each section after manualize
    gate_results = {}
    if is_llm_available():
        raw_text_for_gate = raw_text[:4000] if raw_text else ""
        for section_name, section_text in sections_map.items():
            try:
                gate_content = call_llm(GATE_CHECK_PROMPT.format(
                    section_text=section_text[:3000],
                    raw_text=raw_text_for_gate
                ), temperature=0.3)
                gate_data = {"status": "PASS", "score": 100, "reasons": [], "required_actions": []}
                if gate_content:
                    gm = re.search(r'\{[\s\S]*\}', gate_content)
                    if gm:
                        gate_data = json.loads(gm.group())
                gate_results[section_name] = gate_data

                # Save gate result
                cursor.execute(
                    "UPDATE manual_sections SET gate_status = ?, gate_score = ?, gate_reasons_json = ?, gate_stale = 0, updated_at = ? WHERE doc_id = ? AND section_name = ?",
                    (gate_data.get("status", "PASS"), gate_data.get("score", 100),
                     json.dumps(gate_data.get("reasons", []), ensure_ascii=False),
                     datetime.now().isoformat(), doc_id, section_name)
                )
            except Exception as e:
                print(f"[MANUALIZE_GATE] Gate error for '{section_name}': {e}")
                gate_results[section_name] = {"status": "PASS", "score": 100, "reasons": []}
        conn.commit()

    conn.close()

    return {
        "success": True,
        "doc_id": doc_id,
        "sections": list(sections_map.keys()),
        "section_details": sections_map,
        "todo_questions": todo_questions,
        "change_summary": result_data.get("change_summary", ""),
        "pii_handling": result_data.get("pii_handling", {}),
        "gate_results": gate_results,
        "llm_used": True
    }


# ============ STEP 2: SECTION GATE (per-section AI check) ============

GATE_CHECK_PROMPT = """ë‹¹ì‹ ì€ RAG ë°˜ì˜ ì „, ë§¤ë‰´ì–¼ ì„¹ì…˜ í…ìŠ¤íŠ¸(section_text)ì˜ í’ˆì§ˆ/ë¦¬ìŠ¤í¬ë¥¼ íŒì •í•˜ëŠ” QA ê²Œì´íŠ¸ì…ë‹ˆë‹¤.
ì¤‘ìš”: ë‚´ìš©ì„ ìƒˆë¡œ ì‘ì„±í•˜ê±°ë‚˜ ê³ ì¹˜ì§€ ë§ê³ , ì˜¤ì§ 'ê²€ì¦ ê²°ê³¼'ë§Œ ì¶œë ¥í•˜ì„¸ìš”.

[ì…ë ¥]
section_text: {section_text}
raw_text: {raw_text}

[íŒì • ìƒíƒœ]
- PASS: ë°”ë¡œ RAG ë°˜ì˜ ê°€ëŠ¥
- NEED_FIX: ì‚¬ëŒì´ ìˆ˜ì •/ë³´ê°• í›„ ë°˜ì˜ ê¶Œì¥
- BLOCK: RAG ë°˜ì˜ ê¸ˆì§€(ë³´ì•ˆ/ì‹¬ê° ì¶©ëŒ/í˜•ì‹ ë¶•ê´´)

[ê²€ì¦ í•­ëª©]
1) PII_RISK (BLOCK ìš°ì„ )
- ì „í™”ë²ˆí˜¸/ì´ë©”ì¼/ê³„ì¢Œ/ìƒì„¸ì£¼ì†Œ/ì‹ë³„ë²ˆí˜¸ê°€ ë§ˆìŠ¤í‚¹ ì—†ì´ ë…¸ì¶œë˜ë©´ BLOCK
- ë§ˆìŠ¤í‚¹ ê·œì¹™ ì˜ˆ: 010-****-1234, ab***@domain.com, ìƒì„¸ì£¼ì†Œ ***, ê³„ì¢Œë²ˆí˜¸ ****

2) CONFLICT (BLOCK ë˜ëŠ” NEED_FIX)
- ê°™ì€ ë¬¸ì„œ/ì„¹ì…˜ ë‚´ ìƒì¶© ê·œì¹™(í™˜ë¶ˆ ê°€ëŠ¥ vs ë¶ˆê°€ ë“±) ì§•í›„
- ì„œë¡œ ë‹¤ë¥¸ ì¡°ê±´ì´ ì¶©ëŒí•˜ëŠ”ë° ìš°ì„ ìˆœìœ„/ì˜ˆì™¸ê°€ ëª…ì‹œë˜ì§€ ì•ŠìŒ

3) MISSING/AMBIGUOUS (NEED_FIX)
- í•„ìˆ˜ê°’(ê¸°í•œ/ê¸ˆì•¡/ë‹´ë‹¹/ì±„ë„/ì¡°ê±´) ëˆ„ë½
- ëª¨í˜¸ í‘œí˜„(ì ë‹¹íˆ/ê°€ëŠ¥í•˜ë©´/ìƒí™©ì— ë”°ë¼/ì‹ ì†íˆ ë“±) ê³¼ë‹¤

4) HALLUCINATION_RISK (NEED_FIX ë˜ëŠ” BLOCK)
- raw_textì— ê·¼ê±°ê°€ ë³´ì´ì§€ ì•ŠëŠ” êµ¬ì²´ ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì •ì±…ì´ ì„ì—¬ ë“¤ì–´ê°„ í”ì 
- íŠ¹íˆ "ë°˜ë“œì‹œ", "ë¬´ì¡°ê±´", "í•­ìƒ"ì²˜ëŸ¼ ë‹¨ì •ì  í‘œí˜„ì´ ê·¼ê±° ì—†ì´ ì¶”ê°€ëœ ê²½ìš°

5) FORMAT (NEED_FIX)
- "## ì„¹ì…˜" / "### í•­ëª©" / "-" bullet í˜•ì‹ ë¶•ê´´
- ì„¹ì…˜ëª…/í•­ëª©ëª…ì´ ì˜ë¯¸ ì—†ì´ ë¹„ì–´ ìˆê±°ë‚˜ ë°˜ë³µë¨

[ì¶œë ¥ í˜•ì‹]
- ì•„ë˜ RFC8259 ìœ íš¨ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. (ì½”ë“œë¸”ë¡/ì„¤ëª… ê¸ˆì§€)
- scoreëŠ” 0~100 ì •ìˆ˜

{{
  "status": "PASS|NEED_FIX|BLOCK",
  "score": 0,
  "reasons": [
    {{
      "type": "PII_RISK|CONFLICT|MISSING|AMBIGUOUS|HALLUCINATION_RISK|FORMAT",
      "severity": "LOW|MEDIUM|HIGH",
      "message": "ë¬´ì—‡ì´ ë¬¸ì œì¸ì§€",
      "location_hint": "ê°€ëŠ¥í•˜ë©´ ì„¹ì…˜/í•­ëª© ì œëª© ë˜ëŠ” ë¬¸ì œ ë¼ì¸ ì¼ë¶€",
      "fix_suggestion": "ì–´ë–»ê²Œ ê³ ì¹˜ë©´ ë˜ëŠ”ì§€(ì§§ê²Œ)"
    }}
  ],
  "required_actions": [
    "ì‚¬ìš©ìê°€ í•´ì•¼ í•  ì¡°ì¹˜ 1",
    "ì¡°ì¹˜ 2"
  ]
}}

[íŒì • ê°€ì´ë“œ]
- BLOCK: (1) PII ë¯¸ë§ˆìŠ¤í‚¹ ë…¸ì¶œ, (2) ì‹¬ê°í•œ ì¶©ëŒ, (3) ê·¼ê±° ì—†ëŠ” ìˆ˜ì¹˜/ê¸ˆì•¡/ê¸°í•œ ë‹¤ìˆ˜, (4) í˜•ì‹ ë¶•ê´´ ì‹¬ê°
- NEED_FIX: ëª¨í˜¸/ëˆ„ë½ì´ í•µì‹¬ ë‹µë³€ì— ì˜í–¥ì„ ì£¼ëŠ” ìˆ˜ì¤€, ë˜ëŠ” [í™•ì¸ í•„ìš”]ê°€ ê³¼ë„í•˜ê±°ë‚˜ í•µì‹¬ê°’ì´ ë¹„ì–´ ìˆìŒ
- PASS: ìœ„ ë¬¸ì œ ì—†ìŒ ë˜ëŠ” ê²½ë¯¸(LOW)ì´ë©° ë‹µë³€ í’ˆì§ˆì— ì˜í–¥ ì ìŒ
"""


@router.post("/doc/{doc_id}/section/{section_name}/gate")
def gate_section(doc_id: str, section_name: str):
    """Run AI gate check on a single section."""
    conn = get_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT section_id, section_text FROM manual_sections WHERE doc_id = ? AND section_name = ?",
                   (doc_id, section_name))
    sec = cursor.fetchone()
    if not sec:
        conn.close()
        raise HTTPException(status_code=404, detail="Section not found")

    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = ?", (doc_id,))
    doc = cursor.fetchone()
    raw_text = (doc["raw_text"] or "")[:4000] if doc else ""

    gate_result = {"status": "PASS", "score": 100, "reasons": [], "required_actions": []}

    if is_llm_available():
        try:
            content = call_llm(GATE_CHECK_PROMPT.format(
                section_text=sec["section_text"][:3000],
                raw_text=raw_text
            ), temperature=0.3)
            if content:
                json_match = re.search(r'\{[\s\S]*\}', content)
                if json_match:
                    gate_result = json.loads(json_match.group())
        except Exception as e:
            print(f"[GATE_SECTION] LLM error: {e}")

    # Save gate result to manual_sections
    cursor.execute(
        "UPDATE manual_sections SET gate_status = ?, gate_score = ?, gate_reasons_json = ?, gate_stale = 0, updated_at = ? WHERE section_id = ?",
        (gate_result.get("status", "PASS"), gate_result.get("score", 100),
         json.dumps(gate_result.get("reasons", []), ensure_ascii=False),
         datetime.now().isoformat(), sec["section_id"])
    )
    conn.commit()
    conn.close()

    return {"success": True, "section_name": section_name, **gate_result}


@router.post("/doc/{doc_id}/section/{section_name}/gate-stale")
def set_gate_stale(doc_id: str, section_name: str):
    """Mark section as gate_stale (saved without gate re-check)."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute(
        "UPDATE manual_sections SET gate_stale = 1, updated_at = ? WHERE doc_id = ? AND section_name = ?",
        (datetime.now().isoformat(), doc_id, section_name)
    )
    conn.commit()
    conn.close()
    return {"success": True}


# ============ STEP 2: QUALITY GATE (document-level) ============

QUALITY_GATE_PROMPT = """ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ ìš´ì˜ ë§¤ë‰´ì–¼ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ë§¤ë‰´ì–¼ ì„¹ì…˜ë“¤ì„ ê²€í† í•˜ê³  ì´ìŠˆë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”.

ì´ìŠˆ íƒ€ì…:
- MISSING (RED): í™˜ë¶ˆ/ì˜ˆì•½/ìš´ì˜ì‹œê°„/ê¶Œí•œ ì¤‘ í•µì‹¬ ì •ë³´ê°€ ì™„ì „íˆ ì—†ìŒ
- AMBIGUOUS (YELLOW): "ìƒí™©ì— ë”°ë¼", "ê°€ëŠ¥í•˜ë©´", "ì ë‹¹íˆ", "í˜‘ì˜ í›„" ë“± ëª¨í˜¸í•œ í‘œí˜„
- CONFLICT (RED): ê°™ì€ ì£¼ì œì—ì„œ ìƒë°˜ëœ ê·œì¹™ ë°œê²¬
- PII_RISK (RED): ì£¼ë¯¼ë²ˆí˜¸/ì „í™”ë²ˆí˜¸ ë“± ê°œì¸ì •ë³´ íŒ¨í„´
- API_NEEDED (YELLOW): "ì˜ˆì•½ ìƒì„±", "ë¬¸ì ë°œì†¡", "ê°•ì¢Œ ì¶”ê°€" ë“± ì‹œìŠ¤í…œ ì—°ë™ í•„ìš”

ê° ì´ìŠˆ í˜•ì‹:
{{"severity": "RED|YELLOW", "issue_type": "íƒ€ì…", "message": "ì„¤ëª…", "suggestion": "í•´ê²°ë°©ì•ˆ"}}

ë§¤ë‰´ì–¼ ë‚´ìš©:
{sections_text}

JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""


@router.post("/doc/{doc_id}/quality-gate")
def quality_gate(doc_id: str):
    """Run quality checks on manual sections."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = ?", (doc_id,))
    sections = cursor.fetchall()
    if not sections:
        conn.close()
        raise HTTPException(status_code=400, detail="Manualize first")
    
    sections_text = "\n\n".join([f"[{s['section_name']}]\n{s['section_text']}" for s in sections])
    
    issues = []
    
    # Rule-based checks first
    # PII check
    pii_patterns = [
        (r'\d{6}-\d{7}', 'ì£¼ë¯¼ë²ˆí˜¸'),
        (r'010-?\d{4}-?\d{4}', 'ì „í™”ë²ˆí˜¸'),
    ]
    for pattern, pii_type in pii_patterns:
        if re.search(pattern, sections_text):
            issues.append({
                "severity": "RED",
                "issue_type": "PII_RISK",
                "message": f"{pii_type} íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤",
                "suggestion": "ê°œì¸ì •ë³´ë¥¼ ì‚­ì œí•˜ê±°ë‚˜ ë§ˆìŠ¤í‚¹í•˜ì„¸ìš”"
            })
    
    # Ambiguous phrases check
    ambiguous_phrases = ["ìƒí™©ì— ë”°ë¼", "ê°€ëŠ¥í•˜ë©´", "ì ë‹¹íˆ", "í˜‘ì˜ í›„", "ê²½ìš°ì— ë”°ë¼", "í•„ìš”ì‹œ"]
    for phrase in ambiguous_phrases:
        if phrase in sections_text:
            issues.append({
                "severity": "YELLOW",
                "issue_type": "AMBIGUOUS",
                "message": f"ëª¨í˜¸í•œ í‘œí˜„ ë°œê²¬: '{phrase}'",
                "suggestion": "êµ¬ì²´ì ì¸ ê¸°ì¤€ì´ë‚˜ ì¡°ê±´ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”"
            })
    
    # API needed check
    api_phrases = ["ì˜ˆì•½ ìƒì„±", "ì˜ˆì•½ ì·¨ì†Œ", "ë¬¸ì ë°œì†¡", "SMS", "ê°•ì¢Œ ì¶”ê°€", "ê°•ì¢Œ ì‚­ì œ", "íšŒì› ë“±ë¡"]
    for phrase in api_phrases:
        if phrase in sections_text:
            issues.append({
                "severity": "YELLOW",
                "issue_type": "API_NEEDED",
                "message": f"ì‹œìŠ¤í…œ ì—°ë™ í•„ìš”: '{phrase}'",
                "suggestion": "í•´ë‹¹ ê¸°ëŠ¥ì˜ API ìŠ¤í™ì„ ì •ì˜í•˜ì„¸ìš”"
            })
    
    # Missing check for critical sections
    for s in sections:
        if s["section_text"] == "ì •ë³´ ì—†ìŒ" and s["section_name"] in ["í™˜ë¶ˆ/ìœ„ì•½/ì •ì‚°", "ì˜ˆì•½/ì·¨ì†Œ/ë³€ê²½", "ìš´ì˜ì‹œê°„/íœ´ë¬´"]:
            issues.append({
                "severity": "RED",
                "issue_type": "MISSING",
                "message": f"í•„ìˆ˜ ì„¹ì…˜ '{s['section_name']}'ì˜ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤",
                "suggestion": "í•´ë‹¹ ê·œì •ì„ ì¶”ê°€í•˜ì„¸ìš”"
            })
    
    # LLM-based additional checks if available
    llm_error_msg = None
    if is_llm_available() and len(issues) < 5:
        try:
            content = call_llm(QUALITY_GATE_PROMPT.format(sections_text=sections_text[:6000]), temperature=0.3)
            if content:
                json_match = re.search(r'\[[\s\S]*\]', content)
                if json_match:
                    llm_issues = json.loads(json_match.group())
                    issues.extend(llm_issues[:5])  # Limit LLM issues
            else:
                llm_error_msg = "LLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (API í• ë‹¹ëŸ‰ ì´ˆê³¼ ê°€ëŠ¥ì„±)"
        except Exception as e:
            llm_error_msg = f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {str(e)}"
            print(f"[QUALITY_GATE] LLM error: {e}")
    
    # Clear old issues and save new
    cursor.execute("DELETE FROM qa_issues WHERE doc_id = ?", (doc_id,))
    for issue in issues:
        issue_id = f"issue_{uuid.uuid4().hex[:8]}"
        cursor.execute(
            "INSERT INTO qa_issues (issue_id, doc_id, severity, issue_type, message, suggestion, status) VALUES (?, ?, ?, ?, ?, ?, 'OPEN')",
            (issue_id, doc_id, issue.get("severity", "YELLOW"), issue.get("issue_type", "OTHER"), 
             issue.get("message", ""), issue.get("suggestion", ""))
        )
    
    conn.commit()
    conn.close()
    
    red_count = len([i for i in issues if i.get("severity") == "RED"])
    yellow_count = len([i for i in issues if i.get("severity") == "YELLOW"])
    
    return {
        "success": True, 
        "doc_id": doc_id, 
        "red_count": red_count, 
        "yellow_count": yellow_count, 
        "issues": issues,
        "llm_error": llm_error_msg,
        "api_specs": extract_api_spec(doc_id).get("specs", [])
    }


# ============ STEP 2: UPDATE SECTIONS ============

class SectionsUpdate(BaseModel):
    sections: dict # {section_name: text}

@router.put("/doc/{doc_id}/sections")
def update_sections(doc_id: str, req: SectionsUpdate):
    """Update manual sections manually."""
    conn = get_connection()
    cursor = conn.cursor()

    for name, text in req.sections.items():
        cursor.execute(
            "UPDATE manual_sections SET section_text = ? WHERE doc_id = ? AND section_name = ?",
            (text, doc_id, name)
        )
    
    cursor.execute("UPDATE documents SET updated_at = ? WHERE doc_id = ?", (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id}


# ============ STEP 2: AI HELPER (Fill/Refine) ============

class RefineRequest(BaseModel):
    text: str
    task: str  # "refine", "fill", "recommend"
    context: Optional[str] = None  # Section name or issue message
    allow_qa: Optional[str] = None  # "true" or "false" (for fill task)


def _to_bool_allow_qa(val) -> bool:
    """Normalize allow_qa to bool. Accepts bool, str, int, None."""
    if val is None:
        return False
    if isinstance(val, bool):
        return val
    if isinstance(val, str):
        return val.strip().lower() in ("true", "1", "yes")
    if isinstance(val, (int, float)):
        return bool(val)
    return False


FILL_SECTION_TEXT_PROMPT_V3 = """ë‹¹ì‹ ì€ RAG ì²­í¬(ì„¹ì…˜ í…ìŠ¤íŠ¸)ë¥¼ 'ë…ë¦½ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•œ ë§¤ë‰´ì–¼'ë¡œ ë³´ê°•í•˜ëŠ” í¸ì§‘ìì…ë‹ˆë‹¤.
ì¤‘ìš”: ì´ ì‘ì—…ì€ 'ìƒˆ ì •ë³´ ì¶”ê°€'ê°€ ì•„ë‹ˆë¼, ë™ì¼ ë¬¸ì„œ(raw_text) ë‚´ë¶€ì˜ ê´€ë ¨ ë‚´ìš©ì„ ëª¨ì•„ ì¬êµ¬ì„±í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

[ì…ë ¥]
section_text: {section_text}
raw_text: {raw_text}

[ì ˆëŒ€ ê·œì¹™]
1) ì›ë¬¸ì— ì—†ëŠ” ì •ë³´(ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì •ì±…/ì˜ˆì™¸)ë¥¼ ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”.
2) ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ë‚´ìš©ì„ ì±„ìš°ì§€ ë§ê³ , í•´ë‹¹ ì§€ì ì—ë§Œ "[í™•ì¸ í•„ìš”: ë¬´ì—‡ì„ í™•ì¸?]" ë¼ë²¨ì„ ë¶™ì´ì„¸ìš”.
3) ì•”ë¬µ ì¡°ê±´/ì „ì œëŠ” raw_textì— ì•”ì‹œ/í‘œí˜„ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ëª…ì‹œì ìœ¼ë¡œ í’€ì–´ì“°ì„¸ìš”.
4) ì•½ì–´/ë‚´ë¶€ ìš©ì–´ëŠ” ì›ë¬¸ì— ë“±ì¥í•œ ê²ƒë§Œ í’€ì–´ ì„¤ëª…ì„ ì¶”ê°€í•˜ì„¸ìš”. (ì›ë¬¸ì— ì—†ìœ¼ë©´ ê¸ˆì§€)
5) ê°œì¸ì •ë³´(ì „í™”/ì´ë©”ì¼/ê³„ì¢Œ/ìƒì„¸ì£¼ì†Œ/ì‹ë³„ë²ˆí˜¸ ë“±)ëŠ” ***ë¡œ ë§ˆìŠ¤í‚¹ì„ ìœ ì§€í•˜ì„¸ìš”. ì›ë¬¸ì— ìˆì–´ë„ ê·¸ëŒ€ë¡œ ë…¸ì¶œ ê¸ˆì§€.
6) [Q&A ì •ì±…] {qa_policy_text}
7) ê¸°ì¡´ section_textì˜ ì£¼ì œ/ë²”ìœ„ë¥¼ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”. (ë‹¤ë¥¸ ì„¹ì…˜ ì£¼ì œë¥¼ ì„ì–´ ë„£ì§€ ë§ ê²ƒ)

[ê°œì„  ëª©í‘œ]
- ì•ë’¤ ì„¹ì…˜ ì—†ì´ë„ ì´ section_textë§Œ ì½ê³  ë‹µë³€ ê°€ëŠ¥í•œ ìˆ˜ì¤€ìœ¼ë¡œ,
  ì›ë¬¸ì— í©ì–´ì§„ ê´€ë ¨ ê·œì¹™/ì¡°ê±´/ì±„ë„/ì˜ˆì™¸ë¥¼ ì´ ì„¹ì…˜ ì•ˆì— í†µí•©í•˜ì„¸ìš”.
- ì¤‘ë³µ bullet ì œê±°, í‘œí˜„ ì •ëˆ, í•­ëª© ì œëª©ì„ ëª…í™•íˆ.
- ë„ˆë¬´ ê¸´ í•­ëª©ì€ ê°™ì€ ì£¼ì œ ì•ˆì—ì„œ 2ê°œë¡œ ìª¼ê°œë˜ í˜•ì‹ì€ ìœ ì§€í•˜ì„¸ìš”.

[ì¶œë ¥ í˜•ì‹(ë°˜ë“œì‹œ ìœ ì§€)]
- ì˜¤ì§ ê°œì„ ëœ section_text ì „ì²´ë¥¼ plain textë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- ì„¹ì…˜ ì‹œì‘: "## "
- í•­ëª© ì œëª©: "### "
- ë³¸ë¬¸: "-" bullet
- Q&AëŠ” allow_qa=true ì´ê±°ë‚˜ ì›ë˜ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì—ë§Œ, í•­ëª© í•˜ë‹¨ì— ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ í¬í•¨:
  - Q: ...
  - A: ...

[ì¶”ê°€ ê°€ì´ë“œ]
- raw_textì—ì„œ ê·¼ê±°ê°€ ëª…í™•í•œ ë‚´ìš©ë§Œ 'ëª¨ì•„ì„œ' ë„£ê³ , ê·¼ê±° ì—†ëŠ” ë¶€ë¶„ì€ ì±„ìš°ì§€ ì•ŠìŠµë‹ˆë‹¤.
- "[í™•ì¸ í•„ìš”]"ëŠ” ë‚¨ë°œí•˜ì§€ ë§ê³  'í•„ìˆ˜ íŒë‹¨ì— í•„ìš”í•œ í•µì‹¬ ë¹ˆì¹¸'ì—ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.
- ì„¤ëª…/í•´ì„¤/JSON ì¶œë ¥ ê¸ˆì§€. ì˜¤ì§ ìµœì¢… í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
"""


FINALIZE_SECTION_TEXT_PROMPT_V1 = """ë‹¹ì‹ ì€ RAG ê²€ìƒ‰ ì ì¤‘ë¥ ê³¼ ë‹µë³€ ì¼ê´€ì„±ì„ ë†’ì´ê¸° ìœ„í•´ section_text(plain text)ë¥¼ 'ìµœì¢… ë¬¸êµ¬'ë¡œ ë‹¤ë“¬ëŠ” í¸ì§‘ìì…ë‹ˆë‹¤.
ì¤‘ìš”: ì‚¬ì‹¤/ì •ì±…/ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡ ë“± ìƒˆë¡œìš´ ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ì˜¤ì§ í‘œí˜„ê³¼ êµ¬ì¡°ë§Œ ìµœì í™”í•©ë‹ˆë‹¤.

[ì…ë ¥]
section_text: {section_text}
raw_text: {raw_text}

[ì ˆëŒ€ ê·œì¹™]
1) ìƒˆ ì •ë³´ ì¶”ê°€ ê¸ˆì§€(ìˆ˜ì¹˜/ê¸°ê°„/ê¸ˆì•¡/ì •ì±…/ì˜ˆì™¸/ì ˆì°¨ ì°½ì‘ ê¸ˆì§€)
2) ì›ë¬¸/í˜„ section_textì™€ ë‹¤ë¥¸ ì‚¬ì‹¤ ìƒì„± ê¸ˆì§€
3) ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ ìœ ì§€(***)
4) ê·¼ê±°ê°€ ë¶ˆëª…í™•í•œ ë¬¸ì¥ ì¶”ê°€ ê¸ˆì§€. ë¶ˆëª…í™•í•˜ë©´ "[í™•ì¸ í•„ìš”: ...]"ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ ë” ëª…í™•íˆ ì‘ì„±

[ìµœì í™” ëª©í‘œ]
- ê²€ìƒ‰ í‚¤ì›Œë“œì— ì˜ ê±¸ë¦¬ë„ë¡ í•­ëª© ì œëª©(###)ì„ 'ì§ˆë¬¸í˜• ë˜ëŠ” í‚¤ì›Œë“œí˜•'ìœ¼ë¡œ ì„ ëª…í•˜ê²Œ
  ì˜ˆ: "í™˜ë¶ˆ ê·œì •" â†’ "í™˜ë¶ˆ ê·œì •/ê¸°í•œ/ìœ„ì•½ê¸ˆ"
- bulletsë¥¼ ì§§ê³  ë³‘ë ¬ êµ¬ì¡°ë¡œ ì •ë¦¬(ì¤‘ë³µ ì œê±°)
- ê°™ì€ ì˜ë¯¸ì˜ í‘œí˜„ì„ í†µì¼(ìš©ì–´ í‘œì¤€í™”)
- Q&Aê°€ ì´ë¯¸ ì¡´ì¬í•˜ë©´, ì§ˆë¬¸ì„ ë” ëª…í™•íˆ í•˜ë˜ ë‹µì€ ê·¸ëŒ€ë¡œ(ë‚´ìš© ì¶”ê°€ ê¸ˆì§€)
- ë„ˆë¬´ ê¸´ bulletì€ 2ê°œë¡œ ë¶„ë¦¬í•˜ë˜ ì˜ë¯¸ ìœ ì§€

[ì¶œë ¥]
- ì˜¤ì§ ìµœì¢… section_text ì „ì²´ë¥¼ plain textë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.
- í˜•ì‹ ìœ ì§€:
  - "## " ì„¹ì…˜
  - "### " í•­ëª©
  - "-" bullet
  - Q/AëŠ” ì¡´ì¬í•  ë•Œë§Œ ìœ ì§€
- ì„¤ëª…/í•´ì„¤/JSON ê¸ˆì§€
"""


@router.post("/doc/{doc_id}/refine-text")
def refine_text(doc_id: str, req: RefineRequest):
    """AI helper: fill (ë§¥ë½ ë³´ê°•), refine (RAG ìµœì í™”), recommend (í‘œì¤€ í…œí”Œë¦¿ ì œì•ˆ)."""
    if not is_llm_available():
        raise HTTPException(status_code=503, detail="LLM ì‚¬ìš© ë¶ˆê°€")

    # Fetch original document raw_text for reference
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = ?", (doc_id,))
    doc = cursor.fetchone()
    conn.close()

    raw_text = ""
    if doc and doc["raw_text"]:
        raw_text = doc["raw_text"][:4000]

    raw_text_safe = raw_text or "(ì›ë³¸ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ì¡´ í…ìŠ¤íŠ¸ë§Œ ì°¸ê³ í•˜ì„¸ìš”.)"

    try:
        if req.task == "fill":
            allow_qa = _to_bool_allow_qa(req.allow_qa)
            qa_policy_text = (
                "ì›ë¬¸ ê·¼ê±°ê°€ ëª…í™•í•œ ê²½ìš°ì—ë§Œ Q&Aë¥¼ 1~3ê°œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹µì´ ë¶ˆëª…í™•í•˜ë©´ Q&Aë¥¼ ë§Œë“¤ì§€ ë§ê³  [í™•ì¸ í•„ìš”]ë¡œ ì²˜ë¦¬í•˜ì„¸ìš”."
                if allow_qa else
                "Q&AëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ê¸°ì¡´ Q&Aë§Œ ìœ ì§€/ì •ë¦¬í•˜ì„¸ìš”."
            )
            prompt = FILL_SECTION_TEXT_PROMPT_V3.format(
                section_text=req.text,
                raw_text=raw_text_safe,
                qa_policy_text=qa_policy_text
            )
        elif req.task == "refine":
            prompt = FINALIZE_SECTION_TEXT_PROMPT_V1.format(
                section_text=req.text,
                raw_text=raw_text_safe
            )
        else:
            # recommend: use fill prompt with Q&A disabled
            prompt = FILL_SECTION_TEXT_PROMPT_V3.format(
                section_text=req.text,
                raw_text=raw_text_safe,
                qa_policy_text="Q&AëŠ” ìƒˆë¡œ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”. ê¸°ì¡´ Q&Aë§Œ ìœ ì§€/ì •ë¦¬í•˜ì„¸ìš”."
            )

        suggestion = call_llm(prompt, temperature=0.3)
        return {"success": True, "suggestion": suggestion}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ============ STEP 2: APPROVE ============

@router.post("/doc/{doc_id}/approve")
def approve(doc_id: str):
    """Approve document if no RED issues open."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT doc_id FROM documents WHERE doc_id = ?", (doc_id,))
    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")
    
    cursor.execute("SELECT COUNT(*) as cnt FROM qa_issues WHERE doc_id = ? AND severity = 'RED' AND status = 'OPEN'", (doc_id,))
    red_count = cursor.fetchone()["cnt"]
    
    if red_count > 0:
        conn.close()
        raise HTTPException(status_code=400, detail=f"Cannot approve: {red_count} RED issues open")
    
    cursor.execute("UPDATE documents SET status = 'APPROVED', updated_at = ? WHERE doc_id = ?",
                   (datetime.now().isoformat(), doc_id))
    conn.commit()
    conn.close()
    
    # Auto-reindex after approval
    reindex_result = reindex(doc_id)
    
    return {"success": True, "doc_id": doc_id, "status": "APPROVED", "reindex": reindex_result}


# ============ STEP 2: REINDEX ============

@router.post("/doc/{doc_id}/reindex")
def reindex(doc_id: str):
    """Chunk manual sections for RAG retrieval."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = ?", (doc_id,))
    sections = cursor.fetchall()
    if not sections:
        conn.close()
        raise HTTPException(status_code=400, detail="No sections to index")
    
    # Delete existing chunks
    cursor.execute("DELETE FROM chunks WHERE doc_id = ?", (doc_id,))
    
    chunk_size = 500
    overlap = 100
    chunk_count = 0
    
    for section in sections:
        text = section["section_text"]
        section_name = section["section_name"]
        
        if not text or text == "ì •ë³´ ì—†ìŒ":
            continue
        
        # Simple chunking with overlap
        start = 0
        chunk_index = 0
        while start < len(text):
            end = start + chunk_size
            chunk_text = text[start:end]
            
            if chunk_text.strip():
                chunk_id = f"chunk_{uuid.uuid4().hex[:8]}"
                cursor.execute(
                    "INSERT INTO chunks (chunk_id, doc_id, section_name, chunk_index, chunk_text, created_at) VALUES (?, ?, ?, ?, ?, ?)",
                    (chunk_id, doc_id, section_name, chunk_index, chunk_text, datetime.now().isoformat())
                )
                chunk_count += 1
                chunk_index += 1
            
            start = end - overlap
            if start >= len(text):
                break
    
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id, "chunk_count": chunk_count}


# ============ STEP 2: GET SECTIONS ============

@router.get("/doc/{doc_id}/sections")
def get_sections(doc_id: str):
    """Get manual sections for a document."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT section_name, section_text, gate_status, gate_reasons_json, gate_stale FROM manual_sections WHERE doc_id = ?", (doc_id,))
    sections = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return sections


def _split_raw_by_headings(raw_text: str) -> list:
    """Split raw_text into chunks by heading patterns."""
    heading_pattern = re.compile(r'^(?:#{1,4}\s+.+|(?:\d+[\.\)]\s*).+|.+[:\uff1a]\s*)$', re.MULTILINE)
    matches = list(heading_pattern.finditer(raw_text))
    if not matches:
        return []
    chunks = []
    for i, m in enumerate(matches):
        start = m.start()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(raw_text)
        heading = m.group().strip().lstrip("#").strip()
        body = raw_text[start:end].strip()
        chunks.append({"heading": heading, "body": body})
    return chunks


def _heading_match(section_name: str, headings: list) -> int:
    """Match section name to heading index by word overlap. Returns -1 if no match."""
    sec_words = set(re.findall(r'[\wê°€-í£]+', section_name.lower()))
    if not sec_words:
        return -1
    best_idx, best_score = -1, 0
    for i, h in enumerate(headings):
        h_words = set(re.findall(r'[\wê°€-í£]+', h["heading"].lower()))
        overlap = len(sec_words & h_words)
        score = overlap / max(len(sec_words | h_words), 1)
        if score > best_score and score >= 0.3:
            best_score = score
            best_idx = i
    return best_idx


@router.get("/doc/{doc_id}/source-map")
def get_source_map(doc_id: str):
    """Get per-section matched raw text using heading matching."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT raw_text FROM documents WHERE doc_id = ?", (doc_id,))
    doc = cursor.fetchone()
    if not doc:
        conn.close()
        raise HTTPException(status_code=404, detail="Document not found")

    cursor.execute("SELECT section_name FROM manual_sections WHERE doc_id = ?", (doc_id,))
    sections = [row["section_name"] for row in cursor.fetchall()]
    conn.close()

    raw_text = doc["raw_text"] or ""
    chunks = _split_raw_by_headings(raw_text)
    if not chunks:
        source_map = {name: None for name in sections}
        return {"source_map": source_map, "raw_text": raw_text, "matched": 0}

    # 1) Find matched heading index for each section (in order)
    match_indices = []
    for sec_name in sections:
        idx = _heading_match(sec_name, chunks)
        match_indices.append(idx)

    # 2) For each section, extract raw_text from matched heading to next matched heading
    source_map = {}
    for i, sec_name in enumerate(sections):
        idx = match_indices[i]
        if idx < 0:
            source_map[sec_name] = None
            continue
        # Find the next section's matched heading index that comes after this one
        next_chunk_idx = len(chunks)
        for j in range(i + 1, len(sections)):
            if match_indices[j] > idx:
                next_chunk_idx = match_indices[j]
                break
        # Collect raw_text from matched heading to next boundary
        start_pos = chunks[idx]["body"]  # not useful, use raw positions
        # Rebuild from chunk bodies between idx and next_chunk_idx
        parts = [chunks[k]["body"] for k in range(idx, min(next_chunk_idx, len(chunks)))]
        source_map[sec_name] = "\n\n".join(parts)

    return {"source_map": source_map, "raw_text": raw_text, "matched": sum(1 for v in source_map.values() if v is not None)}


@router.get("/doc/{doc_id}/issues")
def get_issues(doc_id: str):
    """Get QA issues for a document."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT issue_id, severity, issue_type, message, suggestion, status FROM qa_issues WHERE doc_id = ?", (doc_id,))
    issues = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return issues


# ============ STEP 3: CHAT WITH RAG ============

class ChatRequest(BaseModel):
    apt_id: str
    client_id: str = "default"
    conversation_id: Optional[str] = None
    message: str


CHAT_PROMPT = """ë‹¹ì‹ ì€ ì•„íŒŒíŠ¸ ì»¤ë®¤ë‹ˆí‹° ê·œì •ì— ëŒ€í•´ ë‹µë³€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

ë¬¸ì„œ ë‚´ìš©:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {question}

JSON í˜•ì‹:
{{
  "reply_text": "ë‹µë³€ ë‚´ìš© (ê°„ê²°í•˜ê²Œ)",
  "citations": [
    {{"doc_id": "ë¬¸ì„œID", "doc_title": "ë¬¸ì„œëª…", "section_name": "ì„¹ì…˜ëª…", "snippet": "ì¸ìš© ë¶€ë¶„ 120ì ì´ë‚´"}}
  ],
  "confidence": "HIGH|MED|LOW",
  "next_question": null ë˜ëŠ” "ì¶”ê°€ ì§ˆë¬¸(ê·¼ê±° ë¶€ì¡±ì‹œ)",
  "actions": []
}}

ê·œì¹™:
- citationsì´ ì—†ìœ¼ë©´ confidenceëŠ” LOWë¡œ ì„¤ì •
- LOWì¼ ê²½ìš° next_questionì— í™•ì¸í•  ì§ˆë¬¸ 1ê°œ í¬í•¨
- ì˜ˆì•½ìƒì„±/ë¬¸ìë°œì†¡ ë“± ì‹¤í–‰ ì•¡ì…˜ì€ ê¸ˆì§€ (actionsëŠ” í•­ìƒ [])
- ê·¼ê±° ì—†ì´ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”"""


def keyword_search(query: str, chunks: list, top_k: int = 5) -> list:
    """Simple keyword-based search scoring."""
    keywords = set(query.lower().replace("?", "").replace(".", "").split())
    scored = []
    for chunk in chunks:
        text = chunk["chunk_text"].lower()
        score = sum(1 for kw in keywords if kw in text)
        if score > 0:
            scored.append((score, chunk))
    scored.sort(key=lambda x: -x[0])
    return [c for _, c in scored[:top_k]]


@router.post("/chat")
def chat(req: ChatRequest):
    """Chat with RAG retrieval and citations."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get or create conversation
    if req.conversation_id:
        conv_id = req.conversation_id
        cursor.execute("UPDATE conversations SET last_at = ? WHERE conversation_id = ?",
                       (datetime.now().isoformat(), conv_id))
    else:
        conv_id = f"conv_{uuid.uuid4().hex[:12]}"
        cursor.execute(
            "INSERT INTO conversations (conversation_id, apt_id, client_id, created_at, last_at) VALUES (?, ?, ?, ?, ?)",
            (conv_id, req.apt_id, req.client_id, datetime.now().isoformat(), datetime.now().isoformat())
        )
    
    # Save user message
    user_msg_id = f"msg_{uuid.uuid4().hex[:8]}"
    cursor.execute(
        "INSERT INTO messages (msg_id, conversation_id, role, text, created_at) VALUES (?, ?, 'user', ?, ?)",
        (user_msg_id, conv_id, req.message, datetime.now().isoformat())
    )
    
    # Retrieve chunks from APPROVED documents for this apt_id
    cursor.execute("""
        SELECT c.chunk_id, c.doc_id, c.section_name, c.chunk_text, d.title as doc_title
        FROM chunks c
        JOIN documents d ON c.doc_id = d.doc_id
        WHERE d.apt_id = ? AND d.status = 'APPROVED'
    """, (req.apt_id,))
    all_chunks = [dict(row) for row in cursor.fetchall()]
    
    # Keyword search
    top_chunks = keyword_search(req.message, all_chunks, top_k=5)
    
    # Default response
    response = {
        "conversation_id": conv_id,
        "reply_text": "",
        "citations": [],
        "confidence": "LOW",
        "next_question": None,
        "actions": []
    }
    
    if not top_chunks:
        response["reply_text"] = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        response["next_question"] = "ì–´ë–¤ ë‚´ìš©ì— ëŒ€í•´ ë” ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”?"
    else:
        # Build context
        context_parts = []
        for chunk in top_chunks:
            context_parts.append(f"[ë¬¸ì„œ: {chunk['doc_title']} / ì„¹ì…˜: {chunk['section_name']}]\n{chunk['chunk_text']}")
        context = "\n\n".join(context_parts)
        
        if is_llm_available():
            try:
                content = call_llm(CHAT_PROMPT.format(context=context, question=req.message), temperature=0.3)
                if content:
                    json_match = re.search(r'\{[\s\S]*\}', content)
                    if json_match:
                        parsed = json.loads(json_match.group())
                        response["reply_text"] = parsed.get("reply_text", "")
                        response["citations"] = parsed.get("citations", [])
                        response["confidence"] = parsed.get("confidence", "MED")
                        response["next_question"] = parsed.get("next_question")
            except Exception as e:
                response["reply_text"] = f"LLM ì˜¤ë¥˜: {str(e)[:50]}"
        
        if not response["reply_text"]:
            # Mock response without LLM
            response["reply_text"] = f"ë¬¸ì„œì—ì„œ {len(top_chunks)}ê°œì˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤."
            response["citations"] = [
                {
                    "doc_id": c["doc_id"],
                    "doc_title": c["doc_title"],
                    "section_name": c["section_name"],
                    "snippet": c["chunk_text"][:150] + "..." if len(c["chunk_text"]) > 150 else c["chunk_text"]
                } for c in top_chunks[:2]
            ]
            response["confidence"] = "MED" if top_chunks else "LOW"
    
    # Save assistant message
    asst_msg_id = f"msg_{uuid.uuid4().hex[:8]}"
    meta_json = json.dumps({
        "citations": response["citations"],
        "confidence": response["confidence"],
        "retrieval_count": len(top_chunks)
    }, ensure_ascii=False)
    cursor.execute(
        "INSERT INTO messages (msg_id, conversation_id, role, text, meta_json, created_at) VALUES (?, ?, 'assistant', ?, ?, ?)",
        (asst_msg_id, conv_id, response["reply_text"], meta_json, datetime.now().isoformat())
    )
    
    conn.commit()
    conn.close()
    
    return response


# ============ STEP 3: IMPROVEMENTS GENERATOR ============

@router.post("/improvements/generate")
def generate_improvements(apt_id: str):
    """Generate improvement suggestions from chat logs."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get recent assistant messages with LOW confidence or empty citations
    cursor.execute("""
        SELECT m.msg_id, m.text, m.meta_json, m.created_at, c.apt_id
        FROM messages m
        JOIN conversations c ON m.conversation_id = c.conversation_id
        WHERE c.apt_id = ? AND m.role = 'assistant'
        ORDER BY m.created_at DESC
        LIMIT 50
    """, (apt_id,))
    messages = cursor.fetchall()
    
    suggestions = []
    seen_topics = set()
    
    for msg in messages:
        meta = json.loads(msg["meta_json"] or "{}")
        confidence = meta.get("confidence", "HIGH")
        citations = meta.get("citations", [])
        
        # Check for LOW confidence or empty citations
        if confidence == "LOW" or not citations:
            # Extract topic from message text
            text = msg["text"][:100]
            topic_key = text[:30]
            
            if topic_key not in seen_topics and len(suggestions) < 5:
                seen_topics.add(topic_key)
                
                # Determine target section
                target_section = "ì˜ˆì™¸/ë¬¸ì˜/ê¶Œí•œ"
                if "í™˜ë¶ˆ" in text or "ì •ì‚°" in text:
                    target_section = "í™˜ë¶ˆ/ìœ„ì•½/ì •ì‚°"
                elif "ì˜ˆì•½" in text or "ì·¨ì†Œ" in text:
                    target_section = "ì˜ˆì•½/ì·¨ì†Œ/ë³€ê²½"
                elif "ìš´ì˜" in text or "ì‹œê°„" in text:
                    target_section = "ìš´ì˜ì‹œê°„/íœ´ë¬´"
                
                suggestions.append({
                    "title": f"ì •ë³´ ë³´ì™„ í•„ìš”: {text[:30]}...",
                    "reason": f"confidence={confidence}, citations={len(citations)}ê°œ",
                    "target_section_name": target_section,
                    "proposed_patch": ""
                })
    
    # Get the latest APPROVED doc for this apt
    cursor.execute(
        "SELECT doc_id FROM documents WHERE apt_id = ? AND status = 'APPROVED' ORDER BY version DESC LIMIT 1",
        (apt_id,)
    )
    doc_row = cursor.fetchone()
    target_doc_id = doc_row["doc_id"] if doc_row else None
    
    # Save suggestions
    for sug in suggestions:
        sug_id = f"sug_{uuid.uuid4().hex[:8]}"
        cursor.execute("""
            INSERT INTO improve_suggestions (sug_id, apt_id, title, reason, proposed_patch, target_doc_id, target_section_name, status, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, 'PENDING', ?, ?)
        """, (sug_id, apt_id, sug["title"], sug["reason"], sug["proposed_patch"], target_doc_id, sug["target_section_name"],
              datetime.now().isoformat(), datetime.now().isoformat()))
    
    conn.commit()
    conn.close()
    
    return {"success": True, "count": len(suggestions), "suggestions": suggestions}


# ============ STEP 3: ONE-CLICK PATCH APPLY ============

PATCH_PROMPT = """ë¬¸ì„œ ì„¹ì…˜ì— ì¶”ê°€í•  FAQ í•­ëª©ì„ ìƒì„±í•˜ì„¸ìš”.

ì œì•ˆ ì œëª©: {title}
ì œì•ˆ ì´ìœ : {reason}
ëŒ€ìƒ ì„¹ì…˜: {section_name}

í˜„ì¬ ì„¹ì…˜ ë‚´ìš©:
{section_text}

ê·œì¹™:
- ì—†ëŠ” ê·œì •ì„ ë§Œë“¤ì§€ ë§ˆì„¸ìš”
- í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "í™•ì¸ í•„ìš”" í˜•íƒœë¡œ ì‘ì„±
- ê°„ê²°í•œ Q&A í˜•ì‹ìœ¼ë¡œ ì‘ì„±

ì¶œë ¥ í˜•ì‹ (ì¶”ê°€í•  í…ìŠ¤íŠ¸ë§Œ):
---FAQ---
Q: ì§ˆë¬¸
A: ë‹µë³€
---"""


@router.post("/improvements/{sug_id}/apply")
def apply_improvement(sug_id: str):
    """Apply improvement suggestion with one-click patch."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get suggestion
    cursor.execute("SELECT * FROM improve_suggestions WHERE sug_id = ?", (sug_id,))
    sug = cursor.fetchone()
    if not sug:
        conn.close()
        raise HTTPException(status_code=404, detail="Suggestion not found")
    
    if sug["status"] == "APPLIED":
        conn.close()
        raise HTTPException(status_code=400, detail="Already applied")
    
    target_doc_id = sug["target_doc_id"]
    target_section = sug["target_section_name"]
    
    # Get current section text
    cursor.execute(
        "SELECT section_id, section_text FROM manual_sections WHERE doc_id = ? AND section_name = ?",
        (target_doc_id, target_section)
    )
    section_row = cursor.fetchone()
    
    if not section_row:
        conn.close()
        raise HTTPException(status_code=404, detail="Target section not found")
    
    current_text = section_row["section_text"]
    section_id = section_row["section_id"]
    
    # Generate patch
    patch_text = ""
    if is_llm_available():
        try:
            prompt = PATCH_PROMPT.format(
                title=sug["title"],
                reason=sug["reason"],
                section_name=target_section,
                section_text=current_text[:2000]
            )
            patch_text = call_llm(prompt, temperature=0.3)
        except Exception as e:
            print(f"[PATCH] Error: {e}")
            patch_text = f"\n\n---FAQ---\nQ: {sug['title']}\nA: í™•ì¸ í•„ìš” - ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
    
    if not patch_text:
        patch_text = f"\n\n---FAQ---\nQ: {sug['title']}\nA: í™•ì¸ í•„ìš” - ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
    
    # Append patch to section
    new_text = current_text + "\n" + patch_text
    cursor.execute(
        "UPDATE manual_sections SET section_text = ? WHERE section_id = ?",
        (new_text, section_id)
    )
    
    # Update suggestion status
    cursor.execute(
        "UPDATE improve_suggestions SET status = 'APPLIED', updated_at = ? WHERE sug_id = ?",
        (datetime.now().isoformat(), sug_id)
    )
    
    conn.commit()
    conn.close()
    
    # Reindex the document
    reindex_result = reindex(target_doc_id)
    
    return {"success": True, "sug_id": sug_id, "status": "APPLIED", "reindexed": reindex_result}


# ============ STEP 3: API SPEC EXTRACTOR ============

API_SPEC_PROMPT = """ì•„ë˜ ë§¤ë‰´ì–¼ ì„¹ì…˜ì—ì„œ ì‹œìŠ¤í…œ APIê°€ í•„ìš”í•œ ì˜ë„(intent)ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ë§¤ë‰´ì–¼ ë‚´ìš©:
{sections_text}

í’ˆì§ˆ ì´ìŠˆ (API_NEEDED):
{api_issues}

ê° intentì— ëŒ€í•´ API ìŠ¤í™ì„ JSON ë°°ì—´ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
[
  {{
    "intent_name": "ì˜ˆì•½ ìƒì„±",
    "endpoint": "/api/booking/create",
    "method": "POST",
    "request_fields": ["member_id", "class_id", "date"],
    "response_fields": ["booking_id", "status"],
    "auth": "ì…ì£¼ë¯¼|ê´€ë¦¬ì|ì‹œìŠ¤í…œ",
    "notes": "ë¹„ê³ "
  }}
]

JSON ë°°ì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”."""


@router.post("/doc/{doc_id}/extract-api-spec")
def extract_api_spec(doc_id: str):
    """Extract API specifications from document."""
    conn = get_connection()
    cursor = conn.cursor()
    
    # Get sections
    cursor.execute("SELECT section_name, section_text FROM manual_sections WHERE doc_id = ?", (doc_id,))
    sections = cursor.fetchall()
    
    # Get API_NEEDED issues
    cursor.execute("SELECT message FROM qa_issues WHERE doc_id = ? AND issue_type = 'API_NEEDED'", (doc_id,))
    api_issues = [row["message"] for row in cursor.fetchall()]
    
    sections_text = "\n\n".join([f"[{s['section_name']}]\n{s['section_text']}" for s in sections])
    
    specs = []
    
    if is_llm_available():
        try:
            prompt = API_SPEC_PROMPT.format(
                sections_text=sections_text[:4000],
                api_issues=", ".join(api_issues)
            )
            content = call_llm(prompt, temperature=0.3)
            if content:
                json_match = re.search(r'\[[\s\S]*\]', content)
                if json_match:
                    specs = json.loads(json_match.group())
        except Exception as e:
            print(f"[API_SPEC] Error: {e}")
    
    # Fallback: generate from API_NEEDED issues
    if not specs:
        for issue in api_issues:
            intent = issue.replace("ì‹œìŠ¤í…œ ì—°ë™ í•„ìš”: ", "").replace("'", "")
            specs.append({
                "intent_name": intent,
                "endpoint": f"/api/{intent.replace(' ', '-').lower()}",
                "method": "POST",
                "request_fields": ["member_id"],
                "response_fields": ["status", "message"],
                "auth": "ê´€ë¦¬ì",
                "notes": "ìë™ ì¶”ì¶œë¨ - ê²€í†  í•„ìš”"
            })
    
    # Clear and save specs
    cursor.execute("DELETE FROM api_specs WHERE doc_id = ?", (doc_id,))
    for spec in specs:
        spec_id = f"spec_{uuid.uuid4().hex[:8]}"
        cursor.execute("""
            INSERT INTO api_specs (spec_id, doc_id, intent, endpoint, method, req_fields_json, resp_fields_json, auth, errors_json, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (spec_id, doc_id, spec.get("intent_name", ""), spec.get("endpoint", ""), spec.get("method", "POST"),
              json.dumps(spec.get("request_fields", []), ensure_ascii=False),
              json.dumps(spec.get("response_fields", []), ensure_ascii=False),
              spec.get("auth", ""), "[]", datetime.now().isoformat()))
    
    conn.commit()
    conn.close()
    
    return {"success": True, "doc_id": doc_id, "spec_count": len(specs), "specs": specs}


@router.get("/doc/{doc_id}/api-spec/export")
def export_api_spec(doc_id: str, format: str = "json"):
    """Export API specs as JSON or YAML."""
    conn = get_connection()
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT intent, endpoint, method, req_fields_json, resp_fields_json, auth, errors_json
        FROM api_specs WHERE doc_id = ?
    """, (doc_id,))
    rows = cursor.fetchall()
    conn.close()
    
    specs = []
    for row in rows:
        specs.append({
            "intent": row["intent"],
            "endpoint": row["endpoint"],
            "method": row["method"],
            "request_fields": json.loads(row["req_fields_json"] or "[]"),
            "response_fields": json.loads(row["resp_fields_json"] or "[]"),
            "auth": row["auth"],
            "errors": json.loads(row["errors_json"] or "[]")
        })
    
    if format == "yaml":
        # Simple YAML conversion
        yaml_lines = ["api_specs:"]
        for spec in specs:
            yaml_lines.append(f"  - intent: {spec['intent']}")
            yaml_lines.append(f"    endpoint: {spec['endpoint']}")
            yaml_lines.append(f"    method: {spec['method']}")
            yaml_lines.append(f"    auth: {spec['auth']}")
            yaml_lines.append(f"    request_fields: {spec['request_fields']}")
            yaml_lines.append(f"    response_fields: {spec['response_fields']}")
        return {"format": "yaml", "content": "\n".join(yaml_lines)}
    
    return {"format": "json", "specs": specs}


# ============ STEP 3: GET SUGGESTIONS ============

@router.get("/improvements")
def list_improvements(apt_id: Optional[str] = None):
    """List improvement suggestions."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute("""
            SELECT s.*, a.name as apt_name
            FROM improve_suggestions s
            LEFT JOIN apartments a ON s.apt_id = a.apt_id
            WHERE s.apt_id = ?
            ORDER BY s.created_at DESC
        """, (apt_id,))
    else:
        cursor.execute("""
            SELECT s.*, a.name as apt_name
            FROM improve_suggestions s
            LEFT JOIN apartments a ON s.apt_id = a.apt_id
            ORDER BY s.created_at DESC
        """)
    
    suggestions = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return suggestions


@router.get("/conversations")
def list_conversations(apt_id: Optional[str] = None):
    """List conversations."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute("SELECT * FROM conversations WHERE apt_id = ? ORDER BY last_at DESC", (apt_id,))
    else:
        cursor.execute("SELECT * FROM conversations ORDER BY last_at DESC")
    
    convs = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return convs


@router.get("/conversations/{conversation_id}/messages")
def get_conversation_messages(conversation_id: str):
    """Get messages for a conversation."""
    conn = get_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM messages WHERE conversation_id = ? ORDER BY created_at", (conversation_id,))
    messages = [dict(row) for row in cursor.fetchall()]
    conn.close()
    return messages


# ============ STEP 4-1: BRANCH CLASS SYNC ============

@router.post("/branch/{branch_id}/classes/sync")
def sync_branch_classes(
    branch_id: str,
    apt_id: str = Form(...),
    classes_json: str = Form(...),
    asof: Optional[str] = Form(None)
):
    """Sync branch classes from form-urlencoded data.
    
    Accepts application/x-www-form-urlencoded with:
    - apt_id: apartment ID (required)
    - classes_json: JSON string array of class objects (required)
    - asof: timestamp string (optional)
    """
    # Parse classes_json
    try:
        classes = json.loads(classes_json)
        if not isinstance(classes, list):
            raise ValueError("classes_json must be a JSON array")
    except json.JSONDecodeError as e:
        return {
            "success": False,
            "error": str(e),
            "preview": classes_json[:100] if classes_json else "",
            "hint": "classes_json must be valid JSON array"
        }
    except ValueError as e:
        return {
            "success": False,
            "error": str(e),
            "preview": classes_json[:100] if classes_json else "",
            "hint": "classes_json must be valid JSON array"
        }
    
    conn = get_connection()
    cursor = conn.cursor()
    
    now = datetime.now().isoformat()
    asof_value = asof or now
    upserted = 0
    
    for cls in classes:
        class_id = cls.get("class_id") or cls.get("id") or f"cls_{uuid.uuid4().hex[:8]}"
        name = cls.get("name", "")
        start = cls.get("start", "")
        end = cls.get("end", "")
        capacity = cls.get("capacity", 0)
        reserved = cls.get("reserved", 0)
        
        # Upsert using INSERT OR REPLACE
        cursor.execute("""
            INSERT OR REPLACE INTO branch_class_cache 
            (apt_id, branch_id, class_id, name, start, end, capacity, reserved, asof, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (apt_id, branch_id, class_id, name, start, end, capacity, reserved, asof_value, now))
        upserted += 1
    
    conn.commit()
    conn.close()
    
    return {"success": True, "upserted": upserted, "branch_id": branch_id, "apt_id": apt_id}


@router.get("/branch/{branch_id}/classes")
def get_branch_classes(branch_id: str, apt_id: str, date: Optional[str] = None):
    """Get cached classes for a branch."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if date:
        # Filter by date (classes where start contains the date string)
        cursor.execute("""
            SELECT * FROM branch_class_cache 
            WHERE branch_id = ? AND apt_id = ? AND start LIKE ?
            ORDER BY start
        """, (branch_id, apt_id, f"{date}%"))
    else:
        cursor.execute("""
            SELECT * FROM branch_class_cache 
            WHERE branch_id = ? AND apt_id = ?
            ORDER BY start
        """, (branch_id, apt_id))
    
    classes = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return {"branch_id": branch_id, "apt_id": apt_id, "classes": classes, "count": len(classes)}


@router.get("/classes")
def list_all_classes(apt_id: Optional[str] = None):
    """List all cached classes, optionally filtered by apt_id."""
    conn = get_connection()
    cursor = conn.cursor()
    
    if apt_id:
        cursor.execute("SELECT * FROM branch_class_cache WHERE apt_id = ? ORDER BY start", (apt_id,))
    else:
        cursor.execute("SELECT * FROM branch_class_cache ORDER BY start")
    
    classes = [dict(row) for row in cursor.fetchall()]
    conn.close()
    
    return {"classes": classes, "count": len(classes)}
